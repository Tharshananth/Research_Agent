{
  "paper_id": "10.3390_machines11070677",
  "metadata": {
    "title": "YOLO-v1 to YOLO-v8, the Rise of YOLO and Its Complementary Nature toward Digital Manufacturing and Industrial Defect Detection",
    "authors": [
      "Muhammad Hussain"
    ],
    "doi": "10.3390/machines11070677",
    "publication_year": "2015",
    "journal": null,
    "keywords": []
  },
  "sections": {
    "abstract": "rapidlygrown,withthelatestreleaseofYOLO-v8inJanuary2023.YOLOvariantsareunderpinned\nby the principle of real-time and high-classification performance, based on limited but efficient\ncomputationalparameters. ThisprinciplehasbeenfoundwithintheDNAofallYOLOvariants\nwithincreasingintensity,asthevariantsevolveaddressingtherequirementsofautomatedquality\ninspectionwithintheindustrialsurfacedefectdetectiondomain,suchastheneedforfastdetection,\nhighaccuracy,anddeploymentontoconstrainededgedevices.Thispaperisthefirsttoprovidean\nin-depthreviewoftheYOLOevolutionfromtheoriginalYOLOtotherecentrelease(YOLO-v8)from\ntheperspectiveofindustrialmanufacturing.Thereviewexploresthekeyarchitecturaladvancements\nproposedateachiteration,followedbyexamplesofindustrialdeploymentforsurfacedefectdetection\nendorsingitscompatibilitywithindustrialrequirements.\nKeywords:industrialdefectdetection;objectdetection;smartmanufacturing;qualityinspection",
    "introduction": "detectors.Next,theevolutionofYOLOvariantsispresented,detailingthekeycontributions\nfromYOLO-v1toYOLO-v8,followedbyareviewoftheliteraturefocusedonYOLO-based\nimplementation of industrial surface defect detection. Finally, the discussion section\nfocusesonsummarizingthereviewedliterature,followedbyextractedconclusions,future\ndirections,andchallengesarepresented.\nObjectDetection\nCNNs can be categorized as convolution-based feed forward neural networks for\nclassificationpurposes[30]. Theinputlayerisfollowedbymultipleconvolutionallayers\ntoacquireanincreasedsetofsmaller-scalefeaturemaps. Thesefeaturemapspostfurther\nmanipulationaretransformedintoone-dimensionalfeaturevectorsbeforebeingusedas\ninputtothefullyconnectedlayer(s). Theprocessoffeatureextractionandfeaturemap\nmanipulationisvitaltotheoverallaccuracyofthenetwork;therefore,thiscaninvolvethe\nstackingofmultipleconvolutionalandpoolinglayersforricherfeaturemaps. Popular\narchitecturesforfeatureextractionincludeAlexNet[31],VGGNet[32],GoogleNet[33],and\nResNet[34]. AlexNetisproposedin2012andconsistsoffiveconvolutional,threepooling,\nandthreefullyconnectedlayersprimarilyutilizedforimageclassificationtasks. VGGNet\nMachines 2023, 11, x FOR PEER REVIEW 3 of 26\nand ResNet [34]. AlexNet is proposed in 2012 and consists of five convolutional, three\nMachines2023,11,677 3of25\npooling, and three fully connected layers primarily utilized for image classification tasks.\nVGGNet focused on performance enhancement by increasing the internal depth of the\narchitecture, introducing several variants with increased layers, VGG-16/19. GoogleNet\nfocusedonperformanceenhancementbyincreasingtheinternaldepthofthearchitecture,\nintroduced the cascading concept by cascading multiple inception modules, whilst Resintroducingseveralvariantswithincreasedlayers,VGG-16/19. GoogleNetintroducedthe\nNet introduced the concept of skip-connections for preserving information and making it\ncascadingconceptbycascadingmultipleinceptionmodules,whilstResNetintroduced\ntahveaicloanbclee pfrtoomfs kthipe- ecoarnlnieerc ttioo nthsef olartperre lsaeyrevrins gofi nthfoer maracthioitnecatnudrem. akingitavailablefrom\ntheeaTrhliee rmtoottihveel faoterr alna yoebrjsecotf dtheeteacrtcohri tiesc ttou rien.fer whether the object(s) of interest are residing inT htheem imotaivgee foorr parnesoebnjte tchted fertaemcteo rofis a tvoidinefoe.r Ifw thheet hoberjetcht(es)o obfje icntt(esr)eosft ainrtee prersetseanret, the\nrdeestiedcitnogr irnetuthrensi mthaeg ereosprepcrteivseen ctlatshse afnradm loecoalfitay,v ii.de.e,o l.ocIaftitohne doibmjeecnt(ssi)oonfs ionft ethrees otbajreect(s).\npOrbejseecntt d, ethteecdtieotne cctaonr rbeet ufurnrsthtehre drievsipdeecdti vinetocl atwssoa snudbl-occaateligtyo,rii.ees.:, Tlowcaot-isotnagdei mmeenthsioodnss and\noofneth-setaogbeje mct(est)h.oOdbs jeacst sdheotwecnti oinn Fcaignubree 1fu. rTthheer fodrivmideer diniintitaotetws othseu fibr-scta tsetgaogrei ews:itThw tho-e sesletacgtieonm oeft hnoudmsearnoduso nper-osptaogsealms, etthheond isna tshseh soewconnidn sFtaigguer, ep1e.rfoTrhmesf oprrmedericitnioitnia otens tthhee profirststagewiththeselectionofnumerousproposals,theninthesecondstage,performs\nposed regions. Examples of two-stage detectors include the famous R-CNN [35] variants,\npredictionontheproposedregions. Examplesoftwo-stagedetectorsincludethefamous\nsuch as Fast R-CNN [36] and Faster R-CNN [37], boasting high accuracies but low comR-CNN [35] variants, such as Fast R-CNN [36] and Faster R-CNN [37], boasting high\nputational efficiency. The latter transforms the task into a regression problem, eliminating\naccuraciesbutlowcomputationalefficiency. Thelattertransformsthetaskintoaregression\nthe need for an initial stage dedicated to selecting candidate regions; therefore, the candiproblem,eliminatingtheneedforaninitialstagededicatedtoselectingcandidateregions;\ndate selection and prediction is achieved in a single pass. As a result, architectures falling\ntherefore,thecandidateselectionandpredictionisachievedinasinglepass. Asaresult,\ninto this category are computationally less demanding, generating higher FPS and detecarchitecturesfallingintothiscategoryarecomputationallylessdemanding, generating\ntion speed, but in general the accuracy tends to be inferior with respect to two-stage dehigherFPSanddetectionspeed,butingeneraltheaccuracytendstobeinferiorwithrespect\nttoecttworos-.s tagedetectors.\nFFiigguurree 11.. OObbjejeccttd deetetectcotorra naantaotmomy.y.\n2. OriginalYOLOAlgorithm\n2. Original YOLO Algorithm\nYOLOwasintroducedtothecomputervisioncommunityviaapaperreleasein2015by\nYOLO was introduced to the computer vision community via a paper release in 2015\nJosephRedmonetal.[29]titledYouOnlyLookOnce:Unified,Real-TimeObjectDetection.\nby Joseph Redmon et al. [29] titled You Only Look Once: Unified, Real-Time Object DeThepaperreframedobjectdetection,presentingitessentiallyasasinglepassregression\ntection. The paper reframed object detection, presenting it essentially as a single pass reproblem,initiatingwithimagepixelsandmovingtoboundingboxandclassprobabilities.\nTghreespsiroonp opserdobalpepmro, ainchitbiaatsiendg ownitthhe imunaigfieed picxoenlcse apntden mabolevdinthge tsoi mbuolutanndeionugs bporexd iacntido nclass\nopfrombualbtiiplilteiebso. uTnhdei npgrobpooxseesda nadppclraosascphr bobaasebdil iotines t,hime purnoivfiinedgb cootnhcseppete ednaanbdleadc ctuhrea sciym. ultaneouSsi npcreeidtsicinticoenp toiofn minu2l0ti1p6leu nbtioluthnedpinregs ebnotxyeesa ra(n20d2 3c)l,atshse pYrOoLbOabfialmitiielys,h iamscpornotvininuged both\ntsopeevedol vaendat aacrcaupriadcyp.a ce. Althoughtheinitialauthor(JosephRedmon)haltedfurtherwork\nwithiSninthcee citosm inpcuetpetriovnis iionn 2d01o6m uanintila tthYeO pLrOes-evn3t [y3e8a],rt (h2e02e3ff)e, ctthivee YnOesLsOan fdampoiltye nhtaias lcoofntintuheedc toor eevuonlvifiee adt ac ornacpeipdt phaacvee. Abelethnofuugrthh ethred einvietlioapl eadutbhyosre (vJeorsaelpahu tRheodrsm,wonit)h htahleteldat feustrther\nawdodrikti ownitthoitnh ethYeO cLoOmpfaumteilry vciosimoinn dgoinmtahienf aotr mYOofLYOO-vL3O [-3v88]., Fthigeu erffee2cptirveesennetssst ahnedY pOoLtOential\nevolutiontimeline.\nof the core unified concept have been further developed by several authors, with the\nlatest addition to the YOLO family coming in the form of YOLO-v8. Figure 2 presents the\n2.1. OriginalYOLO\nYOLO evolution timeline.\nThecoreprincipleproposedbyYOLO-v1wastheimposingofagridcellwithdimensionsofssontotheimage. Inthecaseofthecenteroftheobjectofinterestfallingintoone\nofthegridcells,thatparticulargridcellwouldberesponsibleforthedetectionofthatobject.\nThispermittedothercellstodisregardthatobjectinthecaseofmultipleappearances.\nMaMchaicnheins e2s022032,3 1,11,1 ,x6 F7O7R PEER REVIEW 4of25 4 of 26\nFFiigguurree2 2..Y YOOLLOOe veovloultuiotniotnim tiemlineeli.ne.\nForimplementationofobjectdetection,eachgridcellwouldpredictBboundingboxes\n2.1. Original YOLO\nalongwiththedimensionsandconfidencescores. Theconfidencescorewasindicativeof\nThe core principle proposed by YOLO-v1 was the imposing of a grid cell with ditheabsenceorpresenceofanobjectwithintheboundingbox. Therefore,theconfidence\nmensions of ss onto the image. In the case of the center of the object of interest falling\nscorecanbeexpressedasEquation(1):\ninto one of the grid cells, that particular grid cell would be responsible for the detection\nof that object. This permcointtfeidde noctheescro creel=ls pto(o bdjiescrte)gaIrodU t t h ru a th t object in the case of (m1)ultiple\npred\nappearances.\nwherep(object)signifiedtheprobabilityoftheobjectbeingpresent,witharangeof01with\nFor implementation of object detection, each grid cell would predict B bounding\n0indicatingthattheobjectisnotpresentand IoUtruth representedtheintersection-overboxes along with the dimensions and confidenpcreed scores. The confidence score was indicunionwiththepredictedboundingboxwithrespecttothegroundtruthboundingbox.\nMachines 2023, 11, x FOR PEER REVIaEtWiv e of the absence or presence of an object within the bounding box. The5r eoff o2r6e , the conEachboundingboxconsistedoffivecomponents(x,y,w,h,andtheconfidencescore)\nfidence score can be expressed as Equation (1):\nwiththefirstfourcomponentscorrespondingtocentercoordinates(x,y,width,andheight)\noftherespectiveboundingbox ğ‘ğ‘œ a ğ‘› s ğ‘“ s ğ‘– h ğ‘‘ o ğ‘’ w ğ‘›ğ‘ n ğ‘’ i ğ‘  n ğ‘ F ğ‘œ i ğ‘Ÿ g ğ‘’ u = re ğ‘ 3 ( . ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡)ğ¼ğ‘œğ‘ˆ(cid:3047)(cid:3045)(cid:3048)(cid:3047)(cid:3035) (1)\n(cid:3043)(cid:3045)(cid:3032)(cid:3031)\nwhere ğ‘(ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡) signified the probability of the object being present, with a range of 01\nwith 0 indicating that the object is not present and ğ¼ğ‘œğ‘ˆ(cid:3047)(cid:3045)(cid:3048)(cid:3047)(cid:3035) represented the intersection-\n(cid:3043)(cid:3045)(cid:3032)(cid:3031)\nover-union with the predicted bounding box with respect to the ground truth bounding\nbox.\nEach bounding box consisted of five components (x, y, w, h, and the confidence score)\nwith the first four components corresponding to center coordinates (x, y, width, and height)\nof the respective bounding box as shown in Figure 3.\nFFiigguurree 33.. YYOOLLOO--vv11 pprreelliimmiinnaarryy aarcrchhitietectcuturer.e .\nAs alluded to earlier, the input image is split into s  s grid cells (default = 7  7), with\neach cell predicting B bounding boxes, each containing five parameters and sharing prediction probabilities of classes (C). Therefore, the parameter output would take the following form, expressed in (2):\nğ‘ ğ‘ (5ğµ+ğ¶) (2)\nConsidering the example of YOLO network with each cell bounding box prediction\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the parameter output would be given as expressed in (3):\n77(52+80) (3)\nThe fundamental motive of YOLO and object detection in general is the object detection and localization via bounding boxes. Therefore, two sets of bounding box vectors are\nrequired, i.e., vector y is the representative of ground truth and vector ğ‘¦(cid:4662) is the predicted\nvector. To address multiple bounding boxes containing no object or the same object,\nYOLO opts for non-maximum suppression (NMS). By defining a threshold value for\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NMS\nvalue are eliminated.\nThe original YOLO based on the Darknet framework consisted of two sub-variants.\nThe first architecture comprised of 24 convolutional layers with the final layer providing\na connection into the first of the two fully connected layers. Whereas the Fast YOLO variant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the\ninception module in GoogleNet, a sequence of 11 convolutional layers was implemented for reducing the resultant feature space from the preceding layers. The preliminary architecture for YOLO-v1 is presented in Figure 3.\nTo address the issue of multiple bounding boxes for the same object or with a confidence score of zero, i.e., no object, the authors decided to greatly penalize predictions from\nbounding boxes containing objects (ğ›¾ =5) and the lowest penalization for prediction\n(cid:3030)(cid:3042)(cid:3042)(cid:3045)(cid:3031)\ncontaining no object (ğ›¾ =0.5). The authors calculated the loss function by taking the\n(cid:3041)(cid:3042)(cid:3042)(cid:3029)(cid:3037)\nsum of all bounding box parameters (x, y, width, height, confidence score, and class probability). As a result, the first part of the equation computes the loss of the bounding box\nprediction with respect to the ground truth bounding box based on the coordinates\nğ‘¥\n(cid:3030)(cid:3032)(cid:3041)(cid:3047)(cid:3032)(cid:3045)\n, ğ‘¦\n(cid:3030)(cid:3032)(cid:3041)(cid:3047)(cid:3032)(cid:3045)\n. ğ•\n(cid:3036)\n(cid:3042)\n(cid:3037)\n(cid:3029)(cid:3037) is set as 1 in the case of the object residing within ğ‘—(cid:3047)(cid:3035) bounding box\nprediction in ğ‘–(cid:3047)(cid:3035) cell; otherwise, it is set as 0. The selected, i.e., predicted bounding box\nwould be tasked with predicting an object with the greatest IoU, as expressed in (4):\nğ›¾ (cid:3020)(cid:3118) (cid:3003) ğ•(cid:3042)(cid:3029)(cid:3037)[(ğ‘¥ ğ‘¥(cid:3549))(cid:2870)+(ğ‘¦ ğ‘¦(cid:3549))(cid:2870)] (4)\n(cid:3030)(cid:3042)(cid:3042)(cid:3045)(cid:3031) (cid:3036)(cid:2880)(cid:2868) (cid:3037)(cid:2880)(cid:2868) (cid:3036)(cid:3037) (cid:3036) (cid:3114) (cid:3036) (cid:3114)\nThe next component of the loss function computes the prediction error in width and\nheight of the bounding box, similar to the preceding component. However, the scale of\nerror in the large boxes has lesser impact compared to the small boxes. The normalization\nof width and height between the range 0 and 1 indicates that their square roots increase\nMachines 2023, 11, x FOR PEER REVIEW 5 of 26\nMachines 2023, 11, x FOR PEER REVIEW 5 of 26\nFigure 3. YOLO-v1 preliminary architecture.\nMachines 2023, 11, x FOR PEER REVIEW 5 of 26\nAs alluded to earlier, the input image is split into s  s grid cells (default = 7  7), with\neach cell predicting B bounding boxes, each containing five parameters and sharing prediction probabilities of classes (C). Therefore, the parameter output would take the following form, expressed in (2):\nFigure 3. YOLO-v1 preliminary architecture.\nğ‘   ğ‘   (5 ğµ +ğ¶) (2)\nAs aClluodnseidd etori enagr ltiheer, etxhaem inpplue to ifm YaOgeL Ois snpeltiwt ionrtko ws it hs geraicdh c ceelllsl (bdoeufnaudlitn =g 7b o x7 )p, rweditihc tion\neachs ceet ltlo p 2r eadnidct einvga lBu abtoinugn dthine gb ebnocxhems, aerakc hC OcoCnOta idnaintag s efitv ceo pnasirsatminegt eorfs 8 a0n cdla sshsaesri, nthge p prea-ramFigured 3i. cYOtiLeoOtn-ev 1rp porreuolimtbpianuabryti lawirtchioeituesc tluodref . b cel agsisveesn ( aCs) .e Txphreersesfoedre i,n t h(3e) :p arameter output would take the folMAlaosc hawilnleusidn20e2gd3 , t1fo1o ,e6ar7r7mlie,r , ethxep inrpeust simeadg ei ins s (p2lit) :in to s  s grid cells (default = 7  7), with 5of25\n7 7 (5  2+80) (3)\neach cell predicting B bounding boxes, each containing five parameters and sharing prediction probabilities of classes (C). Therefore, the parameter oğ‘ utpuğ‘ t wo(u5ld tağµke+ thğ¶e f)o l- (2)\nlowing form, expreTssehde i nf u(2)n: dameAnstaalll umdeodttioveear loiefr ,YthOeiLnpOut iamnadge oisbspjelicttin dtoestecstgiroidnc eilnls g(deefnauelrta=l7 is 7t)h,e object detecwitheachcellpredictingBboundingboxes,eachcontainingfiveparametersandsharing\ntCioonn asnidde rloincgal itzhaet iğ‘ eoxnğ‘ a mv(i5pa lğµbe+o oğ¶uf) n YdOinLgO b onxeetws. oTrhke wreiftohr( 2ee), a tcwh oc eslelt sb oouf nbdouinngd binogx bporxe dviecctitoonrs are\nprediction probabilities of classes (C). Therefore, the parameter output would take the\nCsoents itdorere i2nqg ua thnired ee xdea,mv ipa.leleu. o, afv YtfeiOonclLlgotOow tnrinhe gytewf ooibrsrmke t ,nwhecixetphh r remeeascspaehd rrcekeinlsl (Ceb2o)nO:utnaCdtiOnivg ebdo oax tfpa rgesdreioctt uiconno nds tirsutitnhg a onfd 8 v0 eccltaosrs eğ‘¦s , itsh teh pea prraemd-icted\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the parameter voeucttpourt. wToo ualddd bree sgsi vmenu latsip elxep breosusneddi inng\ns\n( 3b\ns\n)o: x(5 es\nB\nc+o\nC\nn)taining no object or t\n(2\nh\n)\ne same object,\neter output would be given as expressed in (3):\nYOLO opts for non-maximum suppression (NMS). By defining a threshold value for\n7Co7nsi(d5er2in+g8t0h)e e xamp\nle\nof\nY\nOL\n(\nO\nn\net\nwo\n+\nrk\nw\nit )(h3 )e achcellboundingboxprediction\n(3)\nNMS, all overlaspetptoi2nagnd pervaelduaitcintgetdhe bbeonuchnmdariknCgO CbOodxaetass ewtcoitnhsis atinng Ioof8U0c llaosswes,etrh etphaaranm etthere defined NMS\nThe fundamental motive of YOouLtOpu atnwd ooubljdecbt edgeitvecetnioans ienx pgreensesreadl iisn t(h3e) :object detecvalue are eliminated.\ntion and locaTlizhateio nfu vina dboaumndienng tbaoxl ems. Tohteirvefeor eo, ftw Yo OsetLs oOf b aounnddi nogb bjoexc vte cdtoerst earcet ion in general is the object detecrequired, i.e., vectoTr yh ies t hoer riegprienseanlt aYtivOe oLf Ogro bunads terudth o annd tvheceto 7Dr ğ‘¦a 7irs k thn (e5e p tr2 efd+ rica8t0em )d ework consisted of tw(3)o sub-variants.\ntion and localization via bounding boxes. Therefore, two sets of bounding box vectors are\nvector. To address multiple bounding boxes containing no object or the same object,\nYOLOr eoqptus T ifro h re e dn o, fi n i- r .m s ea t .x , a ivm r eu c mc h t i os t u e rp c py t r u eTiss rhsi e eot fnhuc ne o (Nd marMmeS pep)n. r rti Bae slysme ed doent fiivo tneai f not gf2 iY v 4 aOe tLc hoOo ref nas nhg vdor o lodobl ujuveca t ntl i udd o ee nt fetoca rrtuil o nt la hiny ageen rnsedr a w vlise ittchh teo otbr hj eecğ‘¦t  fi dei ntse acttliho lne a y p e r r e d p i r c o te v d id ing\nNMS,\nv\na\ne\nll\nc\no\nt\nv\no\near\nr\nlac\n.\np op\nT\nnin\no\nng ep\na\ncre\nd\ntdi\nd\nioct\nr\nne\ne\nd\ns\ni bn\ns\no a tun o\nm\nndd ti\nu\nlnh og\nl\nc e\nt\nab\ni\nloi fi\np\nzxa r\nl\net\ne\nss i o wt n\nb\niotvh\no\nf i aa\nu\ntnb h\nn\nIo eo\nd\nuU n t\ni\ndlw\nn\noinw\ng\no ge rb\nb\nf to uh\no\nxal e\nx\nnl s y\ne\n.th\ns\nTec h od\nc\neen r\no\nfie n fn\nn\noee r\nt\nde\na\nc ,N\ni\nt t\nn\nw eMd\ni\noS\nn\ns l\ng\ne a t s y\nn\no e f\no\nr b s o\no\n. u W n\nb\nd\nje\ni h n\nc\ng e.t b r o e\no\nx a\nr\nv s e\nt\nc t\nh\nt h o\ne\nr e s\ns\na F r\na\ne a\nm\nst\ne\nY\no\nO\nb\nL\nje\nO\nct\n,\nvarrequired,i.e.,vectoryistherepresentativeofgroundtruthandvectoryisthepredicted\nvalue are eliminated.\niant consisted ovfe cotonr.lTyo andidnreess cmounltivploelbuoutniodinngabl olxaesyceornsta ihniongstnionogbj efcetowrtehre sfiamlteeorbsje ceta,YcOhL.O Inspired by the\nTYheO oLrigOin alo YpOtLsO fboasre dn oon nth-em Daarxkniemt furamme wsourkp cponrseisstesdi oofn tw (oN suMb-vSar)i.a nBts.y defining a threshold value for\noptsfornon-maximumsuppression(NMS).BydefiningathresholdvalueforNMS,all\nThe first arcihniteccetupret icoomnp rmiseod dofu 2l4e c oinnvo lGutioonoagl llaeyNerse wt,it ha t hse efiqnaul leanyecr ep roovfid i1ng 1 convolutional layers was impleNMS, all overlappinovge rplarpepdingicptreeddi cbteodubonudndiningg bbooxexsewsit hwainthIo Uanlo wIoerUth alnotwheedre fithneadnN tMhSev dalueefined NMS\na connection into the first of the two fully connected layers. Whereas the Fast YOLO varmented for redaureceilnimgin tahtede. resultant feature space from the preceding layers. The prelimiiant covnasilsuteed oafr oen ley lniimne icnonavtoeludti.o nal layers hosting fewer filters each. Inspired by the\nTheoriginalYOLObasedontheDarknetframeworkconsistedoftwosub-variants.\ninception mnodaurlye ian rGcohoigtleeNcettu, ar ese fqouern cYe OofL 1O-1v c1o nivso lputrioensael nlatyeerds winas Fimigpule-re 3.\nThe original YTOheLfiOrs tbarachsietedct uorenc otmhper iDsedaorfk2n4ceotn vforlaumtioneawllaoyrekrs wcoithntshiesfitneadll aoyefr tpwrovoi dsinugb-variants.\nmented for reducing the resultant feature space from the preceding layers. The prelimiTo addressa ctohnene icstisonuein tooft hme fiurslttoipf tlhee btwooufunlldyicnongn ebctoedxelasy efros.rW thheere sasatmheeF oasbtjYeOcLtO or with a confinary aTrchheite cfiturrset f oar rYcOhLiOte-vc1t ius prree scenotemd ipn rFiigsuerde 3 o. f 24 convolutional layers with the final layer providing\nvariant consisted of only nine convolutional layers hosting fewer filters each. Inspired\nTo adddreessn thcee i sssuceo orfe m ouflt izpeler boo,u ni.dein.,g nboox eos bfojre tchte, stahmee oabujetcth oor rwsit dh ae ccoindfie-d to greatly penalize predictions from\na connection into thbey fithresitn coepf ttiohnem towduole fiunlGlyoo cgloeNnent,eactseeqdue lnacye eofrs1. W1 chonevroeluatsio tnhalel ayFerasswt aYsOLO vardence score of zero, i.e., no object, the authors decided to greatly penalize predictions from\nbounding boxeism cpolenmteantiendifnorgr eodbucjeincgtsth e(ğ›¾resultant=fea5tu)r easnpdac ethfroem lothwe persetc epdienngalalyiezras.tiTohne for prediction\nboundiainng tb ocxoesn csoinstatienidng o ofb joecntsl (yğ›¾\nğ‘ ğ‘œ\nn\nğ‘œğ‘Ÿ\ni\nğ‘‘\nn=e5 c) oanndv thoe lluowtieost npeanla llğ‘iazğ‘œağ‘œytğ‘Ÿieoğ‘‘rn sfo hr porsedtiicntigon fewer filters each. Inspired by the\npreliminaryarchitectureforYOLO-v1ispresentedinFigure3.\ncontai i n n in c g e n p coo t o i nb o jte n act i ( m nğ›¾ ğ‘›iğ‘œn oğ‘œğ‘d gğ‘— u =n l 0o e .5 )o i . n bTh jee GT ca o u o t a t o h( d ğ›¾o dg r ğ‘›r s le ğ‘œe c sğ‘œ a sN l ğ‘t c hğ‘— u ee l t a= i, t s e s d ua 0 e t.h o 5 s e fe )l m .o q s u T u s l t fh ei u pn en le c c ta b i e o o u n u t on bh d y f o i t n a 1 r g ksi b n o cg x a 1 t e hl s ce f co u ro l t a nh t ev e soa d ml u t e h toi e boj e l nc o tao s lr s w l f ai u thy n ea c rc t os i n o fiw n - a b s y i t m ak p i l n e- g the\nsum of all bounding box parameters (x, y, width, height, confidence score, and class probsum of all bounddenicnegsc obreooxfz peroa,ri.ae.m,noeotbejrecst ,(thxe, ayu,th worsiddtehci,d ehdetiogghreta, tlcyopnenfialdizeenprcedei csticonosrfero,m and class probabilitym). Aesn at eredsu lfto, trh er fierdst upacritn ogf t hteh eequ raetiosnu clotmanputt efse tahet ulorsse o sf pthae cboeu nfdrionmg b otxh e preceding layers. The prelimiboundingboxescontainingobjects(Î³ =5)andthelowestpenalizationforprediction\ncoord\np ğ‘¥\nğ‘\nr\nğ‘’\ne\nğ‘›\nd\nğ‘¡ğ‘’\nic\nğ‘Ÿ\nn t , i a o ğ‘¦ n\nğ‘\nr\nğ‘’\ny\nğ‘›\nw\nğ‘¡\na\nğ‘’\na i\nğ‘Ÿ\nbt . h r i ğ• c l\nğ‘–\nğ‘œ r\nğ‘—\ni ğ‘ h et ğ‘— s y i p i t s e) e s c. e t c A t t t a o u s s r t 1 ha e e i n f rg o t er h c\nt\no r s\nh\no e u\ne\nu nY c n t\ns\na ld a\nu\ns O t i e n\nm\n,t r i o Ln tu\no\nf h g t O\nf\nh th e n\na\n- e\nl\nb o\nl\nv fio o\nb\no u 1 b r\no\nb n j s\nu\nj e e d i c\nn\nt cs i t n t\nd\np r g p\ni\n( e\nn\nÎ³ a s r\ng\nb n id r oe o\nb\no i tx bs n\no\nj e g\nx\nob = n a\np\nw fs\na\nt0 e i t\nr\nt e d . h h\na\n5 d\nm\ni ) e n o . n\ne\niT ğ‘—\nt\ne ğ‘¡ n\ne\nh â„ tq\nr\nh e\ns\nb e F u a o\n(x\niu u ca g\n,\no t n h t\ny\no d u i\n,\no r i od r\nw\nn rs in g e\ni\nn\nd\nc a b\nt\na3\nh\ntc o l e\n,\nc. o x s u\nh\nm\ne\nla\nig\nte\nh\np d\nt,\nu t\nc\nh\no\nt e e\nnfi\ns lo\nd\ns t\ne\ns h\nn\nf\nc\nu e\ne\nn\ns\nc l\nc\no t\no\nio s\nr\nn\ne\ns\n,\nb\na\no y\nn\nf\nd\nt a t\nc\nk\nl\nh i\na\nn\ns\ne g\ns\nbounding box\nprediction in\np\nT\nr ğ‘–oğ‘¡eâ„ d\nace\ni\ndl\nc\nl;d\nt\no\ni\nr\no\nthe\nn\nesr ws\nw\nitseh\ni\n,\nt\nei\nh\nt ii ss\nr\nss\ne\neut\ns\nae\np\ns\ne\no0.\nc\nfT\nt\nhm e\nt o\nuse llet\nt\nci\nh\ntpe\ne\ndl ,e i\ng\n.eb\nr\n.,o\no\npu\nu\nren\nn\ndid\nd\nctie nd\nt r\ngb\nu\no bu\nt\nn\nh\nod xin\nb\neg\no\ns b\nu\nofxo\nn\nr\nd\nt\ni\nh\nn\ne\ng\ns\nb\na\no\nm\nx\ne\nb\no\na\nb\ns\nje\ne\nc\nd\nt o\no\nr\nn\nw\nth\nit\ne\nh a\nco\nc\no\no\nr\nn\nd\nfi\nin\n-\nates\nprobability).Asaresult,thefirstpartoftheequationcomputesthelossoftheboundingbox\nwould d b e e n ta cğ‘¥s e k\nğ‘\ne\nğ‘’\ns d\nğ‘›\nc w\nğ‘¡\no\nğ‘’\nit r\nğ‘Ÿ\nh e, p oğ‘¦re f\nğ‘\nd\nğ‘’\ni z c\nğ‘›\nt e i\nğ‘¡\nn r\nğ‘’\ng o\nğ‘Ÿ\na ., p n ir ğ• . o ee\nğ‘–\nğ‘œ d b\nğ‘—\nğ‘ . j i e ,c ğ‘— c t ni t oi w osn i w t os h ieb t t ht h j e erae g css r pt e ,e a 1 ct t t e ht s io t ne I t o hat U euh , g t a erh s o u e con x ar p dss r t e e r s du s to e eh d fcb i io n tdu h ( n 4 ee ) dd : ino tgbobj oegxcrbtea sareetdlsyoin dpthienencgoa olwridziinetah tpeisnrxe cdeğ‘—n ğ‘¡ tiâ„ ecr ,tbioonusn fdrionmg box\nbounpdreindgic btiooxne ğ›¾ ğ‘siğ‘œnğ‘œ cğ‘Ÿğ‘‘ o ğ‘– n ğ‘† ğ‘– ğ‘¡y = 2â„c0tea ntğµ ğ‘—cie=rne . 0 ğ• ili o ğ‘– ğ‘œ nlğ‘—j b ğ‘ ; j ğ‘— g [i(o sğ‘¥ s otğ‘– ehb tğ‘¥e a jğ‘– s er)21 cw+i t n si(ğ‘¦s t h ğ‘–(e e ğ›¾, c ğ‘¦a iğ‘– )s t 2e ]i o s f th s= e e o t5 b a j ) e c sa t r n0 es .d i d T i t n hh g ee w(4 i l )s t o h e i w n le j ec th st b et o d u p n ,e d in i . n ea g .l b ,i o zp x a p rt r ei e do di ni c c t i t o feo n dr in pbroeudnicdtiinogn box\nith cell;otherwise,itisseta ğ‘ s ğ‘œ 0 ğ‘œ . ğ‘Ÿ T ğ‘‘ heselected,i.e.,predictedboundingboxwouldbetasked\nTchoe nnetxwat icnoomuinplgdon enbnoet o tof atbhsejke loecsdts f (uwğ›¾ncittiohn pcor=mepd0uit.ec5st )tih.n eTg ph raeedn iac touiobnthj eerocrortr s wi nc wiatilhdct uht hlaanedt e gdre tahtee slto Isos Ufu, nacs teioxnp rbeys steadk iinng ( 4th):e\nwithpğ‘›rğ‘œeğ‘œdğ‘icğ‘—tinganobjectwiththegreatestIoU,asexpressedin(4):\nheight of the bounding box, similar to the preceding component. However, the scale of\nerror\ns\nin\nu\nth\nm\ne l a\no\nrg\nf\ne\na\nb\nl\no\nl\nx e\nb\ns\no\nha\nu\ns\nn\nle\nd\nsse\ni\nr\nn\ni\ng\nm p\nb\nac\no\nt\nx\nco m\np\np\na\na\nr\nr\na\ned\nm\nt ğ›¾ o\ne\nt\nt\nh\ne\ne Î³\nr\nsm\ns\na\n(\nl\nx\nlğ‘† b\n,\n2S o2\ny\nx  e\n,\nsğµ\nw\n. BT\ni\nh\nd\ne ğ•\nt\noğ‘œn\nh\nbğ‘o j\n(cid:104),\nrğ‘— ( m [\nh\nx( a\ne\nğ‘¥ l\ni\niz\ng\nx a Ë†\nh\nt ) i\nt\no 2ğ‘¥\n,\nn +\nc\n)(\no\n2 y\nn\n+\nfi\ny(Ë†\nd\nğ‘¦)2\ne(cid:105)n\nc\nğ‘¦\ne\n)\ns\nc\n]\no r e, and\n(4 )\nclass prob-\n(4)\nof widatbh ialnidty h)e.ig Aht sb eatw reeens tuhel tr,a ntghee 0 fianrds 1t ipndaicrağ‘ ttğ‘œ eo ğ‘œs ğ‘Ÿtf cğ‘‘ho oat rth d tğ‘– eh=ei =0ier0 qsqu ğ‘—u=j= aa00rtei iğ‘– orjğ‘—onot sc iionğ‘–cmreiapseu ğ‘– te i s th i e ğ‘– los ğ‘– s of the bounding box\npredictionT hwe inthex tr ecsopmeTcp hto e ntnoe e x n ttc tho m oep f ogt n hreon e tu l o onft sdh s e l fto urssun ftu chn t c i t o ibo n no cuc o onm mdpuip tne u sgt t h e ebs po r t exh d i e cbt i paonsree erdd ro i r coi t nni w o i ntdh th eer a r ncd ooro irnd winiadttehs and\nheightoftheboundingbox,similartotheprecedingcomponent. However,thescaleof\nğ‘¥ he,i gğ‘¦ht of t.h ğ•e ğ‘œ ğ‘ b ğ‘— e oriurso nrsidnetithn eaglsa rb1geo ibxno,x etsshihmea siclleaasssree rt iomo fpt ahtchtece op morpbearcjeeedcdtto irtnhegess imcdoailmlnbgopx eowsn.iTtehhneitnn.o rHmğ‘—ğ‘¡aâ„ol iwzbatoeiovunenrd, itnhge bsocaxl e of\nğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ ğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ ğ‘–ğ‘—\npredeircrtoiorn i nin t hğ‘–eğ‘¡ â„la crge o le f l w ;b i o d ot t x h he a e n sr d wh h a e i i ss g e h l t ,e b si e ts t w eis e r e i n sme th tp e a r a a sc n g t0 e c. 0 oT a m n h d ep 1 as in re d el i e c d a c t tt e eo st d h t a ,h t ie t . h es e . i m, r s p q a u rl a el r db er io o cx o t t ee s sd in . c Tb re oh as ue e nndoirnmga bliozxa tion\nthe differences for smaller values to a higher degree compared to that of larger values,\nwouoldf wbei dtathsk aendd w hieetxhipgr ephssrte edbdaesitc(w5t)i:enegn a tnh eo brjaencgt ew 0it han tdhe 1 g irnedaitceastte Iso Uth,a at st hexeiprr seqssueadr ei nr o(4o)t:s increase\nğ›¾ Î³ ğ‘†2S2 ğµB ğ•oğ‘œbğ‘j (cid:34) ğ‘—(cid:16) [(  ğ‘¥w  (cid:112) ğ‘¥w)Ë† 2 (cid:17)2 ++( (cid:18) ğ‘¦ (cid:112) h ğ‘¦ (cid:113) )2 hË†] (cid:19) 2 (cid:35) (5) (4)\nğ‘ğ‘œğ‘œğ‘Ÿcğ‘‘oord ğ‘–=i=00 ğ‘—=j=00 iğ‘–jğ‘— ğ‘–i ğ‘– i ğ‘– i ğ‘– i\nThe next componeNnextt ,othf ethloses loofsthse fcuonnficdteinocens ccooremispcoumtpeust etdhbea spedreondwichteitohenr tehreroobrje citnis width and\npresentorabsentwithrespecttotheboundingbox. Penalizationoftheobjectconfidence\nheight of the bounding box, similar to the preceding component. However, the scale of\nerrorisonlyexecutedbythelossfunctionifthatpredictorwasresponsiblefortheground\nerror in the large boxes has lesser impact compared to the small boxes. The normalization\nof width and height between the range 0 and 1 indicates that their square roots increase\nMachines 2023, 11, x FOR PEMEaRch RinEeVs I2E0W23 , 11, x FOR PEER REVIEW 5 of 26 5 of 26\nMachines 2023, 11, x FOR PEER REVIEW 5 of 26\nMachines 2023, 11, x FOR PEER REVIEW 5 of 26\nMachines 2023, 11, x FOR PEER REVIEW 5 of 26\nMachines 2023, 11, x FOR PEER REVIEW 5 of 26\nFigure 3. YOLO-v1 preliFmiginuarery 3 a. rYcOhiLteOc-tvu1r ep. reliminary architecture.\nAs alluded to earlier, tAhse ainllpuudte idm toag eea risli sepr,l itth ien itnop su t sim graigde c iesl lssp (ldite ifnatuol ts = 7s g r7i)d, wceiltlhs (default = 7  7), with\nFigure 3. YOLO-v1 preliminary architecture.\neach cell predicting Be baochu ncdeliln pg rbeodxicetsi,n ega Bch b coounntdaiinnign gb ofixvees ,p eaarcahm ceotnertas iannindg s fihvaeri npga rparme-eters and sharing preFigure 3. YOFLigOu-vre1 p3r. eYliOmLinOar-yv a1r cphrietelicmtuirne.a ry architecture.\ndiction probabilities odfi cctliaosns eps r(oCb)a. bTilhiteireesf oorfe c, ltahses epsa (rCa)m. Tetheerr oefuotrpeu, tt hweo pualdra tmaketee rth oeu ftoplu-t would take the folAs alluded to earlier, the input image is split into s  s grid cells (default = 7  7), with\nlowing form, expresselodw inin (g2 )f:o rm, expressed in (2):\nAse aalclhu dceeldl ptor eedaircliteinr,g t hBe b ionupnudt iinmga gbeo xise ss,p eliatc ihn tcoo sn ta sin ginrigd ficevlels p (adreafmaueltte =rs 7 a n 7d) ,s whaitrhin g preAs alluded to earlier, the input image is split into s  s grid cells (default = 7  7), with\neach cedlli cptrioedni cptrinobga Bb ibliotiuens doifn gcl absosxeess (, Ce)a.c Thh ceornetfaoirnei,n tgh efi vpea rpaamraemteert eorust panudt wshoaurlidn gt apkree -the foleach cell predicting B boundinğ‘ g bğ‘ ox(e5s, eğµac+hğ¶ c)o n tainğ‘ ingğ‘  fiv(5e pğµar+amğ¶)e t ers and sh(2a)r ing pre- (2)\ndictionl opwroibnagb fiolirtmies, eoxfp crleasssseeds i(nC )(.2 )T: herefore, the parameter output would take the folFigure 3. YOLO-v1 preliminary architecture.\ndiction probabilities of classes (C). Therefore, the parameter output would take the following form, expressed in (2):\nConsidering the examCpolen soifd YerOinLgO t hnee tewxaomrkp wlei tohf eYaOchL Oce nlle btwouonrkd iwngit hb oexa cphr ecdeilcl tbioonu nding box prediction\nğ‘ ğ‘ (5ğµ+ğ¶) (2)\nlowing form, expressed in (2):\nAs alluded to earlier, the input image is split into s  s grid cells (default = 7  7), with\nset to 2 and evaluatingse tth teo b2e anncdhğ‘ m evağ‘ arklu( 5CatOiğµnC+gO ğ¶th )d e a btaesnecth cmonasriks tCinOgC oOf 8d0a ctalassest(e 2c)so , nthsies tpinarga omf -80 classes, the paramConsidering the exaemapchle c oefll YpOreLdOic tnientgw Bo rbko wunitdhi nega cbho cxeelsl, beoauchn dcionngt abinoxin pgr fiedviec tpioarna meters and sharing preeter output would be egtievre onu atsp euxt pwreosusledd b ien g(3iv)ğ‘ :e n ağ‘ s e(x5preğµs+seğ¶d) i n (3): (2)\nCosnest itdoe 2ri nangd t heev aelxuaamtipngle tohdfe i YcbtOeionLncOh p mnreoatbrwka boCirlOkit CiweOsit hodf ae ctaalcashse stc ecesol ln( Cbso)is.u tTinnhdgei nroegff o8br0oe xc, l taphsrese edpsi,ac trthiaoemn pe aterra mou-tput would take the folset to 2e atnerd o euvtapluuta twinogu ltdh eb be egnicvhleomnw aaisrn keg x CfpoOrreCmsOs, ee ddx apitnrae s(s3es)te: cdo inns i(s2t)i:n g of 80 classes, the paramConsidering the exampl7e of7 YO(L5O 2n+et8w0o) r k w7ith7 eac(h5 ce2ll+ b8o0u)n ding box (p3)r ediction (3)\neter output would be given as expressed in (3):\nset to 2 and evaluating the ben7ch7ma(r5k 2C+O8C0)O datasğ‘ et cğ‘ on(s5istğµin+gğ¶ o) f 80 cla(3s)s es, the param- (2)\nThe fundamental motTivhee offu YndOaLmOe anntadl ombojeticvt ed oeft eYcOtioLnO i na ngde noebrjaelc ti sd tehtee cotbiojenc tin d geteence-ral is the object deteceter output would be gi7ven7 as( 5ex2p+re8s0s)e d in (3): (3)\ntion and lTohcae lfiuznadtiaomne vntiitoaal n bm oaountindvd el Coioncofg anY lsbOiizdoLaexOtreii onsa.ngn T d tvh hoieaeb r ejbeexcofatomu drnepetdl,ee tic wnotifgoo Y nb s OioenLxt gsOe eso nn.f eeT brthawole uoirsrne ktdfh oweirn ioetghb, jtbeewacotcxo hd vseceteeetlcclst - booorfus b naodruien ngd bionxg pbroedxi cvteiocnto rs are\nrequ T i h r tei e o f d nu , n a i nd .e da . m , l o v ec e na c tlai t zl o am r t i y oo r t ni ei v sq ve i tu ao hi fb er s Ye e o r t d uO e tn ,p oL d i O r. 2i ee n a . a s g , nn e v bdd ne o t eo c x a vb t e t a o js i el v . r u cT e ta y h dt o ie i en fs rt ge 7 e gt fc h t r o  th o r e ieoe 7u n r ,b ne t eiw pd nn ( o r c 5 g te h r se s me u n e t 2 e t sa nh r r + oa t k a fl a 8 t Cbi n s 0i oO v d) tu e h C n v e o Od e o f i c b nd tg jga o e r t c ro bat os u dğ‘¦ ex net vt di ce s eo c c t n t -t rh so u ir e s t s t h p ian r ra ge en d o d f i c 8 vt 0 ee c dc l t a o s r se s ğ‘¦ ,  t (i h 3s e ) t p h a e r a p m re - dicted\ntion andre lqouciarleizda, tii.oe.n, vveiac tboor uyn ids itenhtgee r br ooepuxertpes.su eTtn hwteaorteuivfloedr oeb,fe tg wgrioovu esnnedt as stor efu xbtphor uaennssddei dnveg icn bt oo(3rx ) :ğ‘¦v  eicst othrse aprree dicted\nvector. To address mvuelcttioprle. Tboo uandddirnegs sb moxuelst ipcolen tbaoinuinndgi nngo boobxjeecst coorn ttahien isnagm ne oo bobjejcetc,t or the same object,\nrequirevde, cit.eo.r,. vTeToct hoaerd ydfu risen stdsh aem mrueepltnripetsaleel n mbtaootuitvniedv ieonf og g fr boYouOxneLds Otcro uantnhta dain noidnb gvje encctot o droe bğ‘¦tjee cicstt itoohrne tpihnree dgsieacnmteeedr aolb ijesc tth, e object detecYOLO opts for non-mYOaxLiOm uompt ss ufporp rneossni-omn a(xNimMuSm). Bsuyp dper7efisns7iinogn(5 a(N 2tMh+r8Se0s)).h oBlyd dveafilunein fgo ra threshold v(a3)l ue for\nvector. YTOoL aOd dorpetsss fmoru lntoipnl-em baoxuimnduimng sbuopxperse scsoionnta i(nNinMgS )n. oB yo bdjeecfit noinr gt hae tsharmeseh oolbdje vcta,l ue for\ntion and localization via bounding boxes. Therefore, two sets of bounding box vectors are\nNMS, all overlapping NprMedSi, catleld o vbeorulanpdpiningg b porxeeds iwctietdh banou IonUdi nlogw beorx tehsa wn itthhe a dne IfionUe dlo NwMerS t han the defined NMS\nYOLO NopMtsS ,f aolrl onvoenr-lmapapxiinmgu pmre dsuicptepdr ebsosuionnd i(nNgM bSo)x.e Bs yw idthe fianni nIogU a l otwhreers thhoalnd tvhael dueefi fnoerd NMS\nrequired, i.e., vector y i\nT\ns\nh\nt\ne\nh\nf\ne\nu n\nre\nd\np\nam\nre\ne\ns\nn\ne\nt\nn\nal\nt a\nm\nti\no\nv\nti\ne\nv e\no f\no f\ng\nY\nro\nO\nu\nL\nn\nO\nd\na n\ntr\nd\nu\no\nth\nbj e\na\nc\nn\nt\nd\nde\nv\nte\ne\nc\nc\nt\nt\nio\no\nn\nr\niğ‘¦n g\ni\ne\ns\nn\nt\ne\nh\nra\ne\nl\np\nis\nr\nt\ne\nh\nd\ne\ni\no\nct\nb\ne\nje\nd\nc t detecNvMalSu, eav lala rolueve ee ralliramep epilniinmagti enpdaret.e dvdiac. lteude t bi a oor nue n a edn lid inm glo ibc n aoa lxi t zeesa d twi . o inth v aian bIooUun lodwinegr bthoaxnes t.h Teh deerefifnoerde, NtwMoS s ets of bounding box vectors are\nvector. To address multiple bounding boxes containing no object or the same object,\nvalue aTrhe ee lioTmrhiiegn aiontreiagdli. n YaOl YLOOL ObaT r bs e hae q sed u e do i o r r e oni d ng , t i th i nh .e eae . , l D D v Y e aaO c rr t kk o Lnn r Oe e y t t f i b r s faa r t msa h em e ed w r e e oow p rn r ko e t s rch e ko n e nc t s a Doi t sn i at v esr e dik s o nto f e ef g dtt r w o for u ofa n smt d uw be t o r -w u v sa t o h urri ba a kn n - cvt d soa . v nr e is c ai t ns o tt r es d.ğ‘¦  o is f t t h w e o pr s e u di b c - te v d a riants.\nYOLO opts for non-maximum suppression (NMS). By defining a threshold value for\nTheT fihT reh s ot e r a fiigr ric snt ha a ilt rYe ch cOt itLu eOr c e t ub Tc rae ohs em c edo p fi mo vrrnp eis sc r tti et h s oadee rr d . Dc o hT o fa o f ir2 tk 2 e4a n4 c d ect c d tuo o r fn rnr ee va sv m s o co l oe ml u uwm t u i t ooilpo n t r irka npi l slac e l eol a dn b y las e ooiy r u ss ften ew r2d ds4i i n to w h cg f o i ttt h bnwho e vo x fi to e sh n ls uue a b c l tfi- io lvo a nn ay ntar a e aila i r ln ln p ailt na r yso gy.e v ern i r d ops i n rwoo g bv ijteihdct i tnhogre tfihne asla lmaye eorb pjercot,v iding\nT a h e c o fi n rs n at e ca c orN t nc i hn o Mie n tceS tc i i, n to ua t nrl o lei n o t c h tvoo e me t rh fia ple r ar c s ifip o s t rep n s o dit nYf noo eOt gff hc L t2 t p e h iO4 o er t ce wn t oodw pn oi i n ovt c sf t otf uo eulfu l d o l tl ltry y h ib o en c c ono oofi uan nn l r nn -l snm dae te cy i ao t c nex e ft rgd i e smt dh lbwa u e oy l ima txe t h yw r e s st e s.huo r Wweps f .pfi u h i W t rne l h el rasy h ela s a ie l c n oas ro yn t e I n he o a r( e n U sN p e  F t Mr c l h aoo t s See vwt ) d i .dY F e Bi l Or a n ay s Lgt y h t O ed a Yre n sfiO v . n a t LW rh in - O e gh  d e av e r ae fi thar n -rs e e ts d hh e N o ld M F av S sa tl uYeO foLrO  vara conneicatniotv ncao ilnnusteois attehrdee fioefrl siomtn olifyn t nhaietne\nN\ntedw\nM\nc.o o\nS\nnf\n,\nuv\na\nlol\nl\ny\nl\nl u\no\nct\nv\noio\ne\nnn\nr\nn\nl\na\na\nel\np\nc lt\np\naey\ni\nd\nn\ne\ng\nlras\np\ny h\nr\ne\ne\nor\nd\nss.t\ni\ni\nc\nWn\nte\ngh\nd\nfe e\nb\nrwe\no\na\nu\nesr\nn\ntfi\nd\nhl\ni\net\nn\ne\ng\nrF s\nb\na es\no\nta\nx\ncY\ne\nh\ns\nO.\nw\nLInO\nit\ns\nh\np vi\na\nra\nn\ner dIo\nb\nU\ny\nl\nt\no\nh\nw\ne\ner than the defined NMS\niant consisted of only inainnte c coonnsvisoteludt ioofn oanl llya yneirnse h coosntvinoglu fteiwonear lfi llatyeersr se hacohst. iInngs pfeirweder b fiyl ttehres each. Inspired by the\niant coninsicsetpedti oonfT om hne oly d o nu ril ine g ei i n nco a Gnl vo Yo v o Ol a gu l l Lu teiN eOo n a e a rb t, e l a la esa ley i s mdeerq i son u h an e t on etsc dhtei . en o gD f fea 1wr  kenr1 efic tlot ef n rrv as o melau ecthw io. n oIna rsl k p lia cry oeedn r s sbi ys w tte ahs de i o m f p t l w e- o sub-variants.\ninception module in iGncoeopgtlieoNn emt, oad suelqe uienn Gceo oogf le1Ne1t, ac osnevqouleunticoen aolf l1ayer1s cwoansv oimluptiloen- al layers was impleinceptiomne nm T t h eod e d uf fi ole r r s ri t ne a d G r u c co h ion i g t gl e et c Nh t e u e t r r,e e as c u s oT lte mh aqn eu p t oe r fnr e iic sg ae e t i u nd oa r fe lo Yf 1s p O2 a 4L c1 O e c o cf b ro n o an v m sveo o dl tl u h uo e t tni ipo o tnr nh eaea cl l e D ld l aaa iyr n y ek g e rn s r l e a stwy wf ear r as i s mt .i h mT e wt hp h eloe e r p- kfi r e cn l o i a m nls i li - astyeedr o pf rtwovoi dsuinb-gv ariants.\nmented for reducing mtheen rteesdu lftoarn rte fdeuatcuinreg stphae cree sfruoltman tth fee aptruercee dspinagc el afyroerms. tThhee p prerecleidmini-g layers. The prelimimentedn aforry raerdchuictiencgtu trhee f orers YuOltaLnOt -fve1a tius rper espseanctee dfr oinm F tihgeu rper 3e.c eding layers. The prelimiThe first architecture comprised of 24 convolutional layers with the final layer providing\na connection into the first of the two fully connected layers. Whereas the Fast YOLO varnnaaryr ya racrh M ci a thc e h Tii c ntt e oes u 2car 0t2 ed 3u , d f 1ro 1 re, r e x sY Ffs OoO R tr h P L EYenO E ROai- s R vrsLEy1u VO I ei Ea as W-or cpv cf o r1hm ne isi n tusee e l nt c cpitt tper i u odele n rs ie ebni n no fFt out oierng d t duY h ire inOen fi g 3L r F.b s Oio t g x- o uve f sr1 t h e fio e s3 r t . p w thr o ee f ss u ea l nm ly te e c do o b n ij n ne e c c tF t oi e gr d uw l r a ie y th e 3 r a. s . c W on h fi e - 6 r e of a 2 s 6 the Fast YOLO variant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the\nToTd oaed nadcdere dsscrsoe trshes eo tfihs zseeu rieos ,os ifu. eme., uonTl i oft ao i nmop bat l euj cde olbcdtn toir, spu et i h s nlset edse aibdtnuh og o teuf h b no oiosn rdxss l ei yudns ee ngfc i oo n ibrdfe o et mh c dxo e etun ossvla tgfo miorpl e u erla t eottio l hbybn je eop a csluet an l na oma ydrl e iweiz rn e s iog tp h hb r ob jaeesod tcc ixito n ce n g tosifi or ffe - nwow sr fie rttrhoh fi m ealt secraosm neafiec -oh.b Ijnescpt iorerd w byit hth ea confid d e e n n ce c s e bc s oo c ur o i enn r odc e fie nz o pge f r t b z o ioo e , n ix r .ee o s. m , , c i n . oo o ed nd . o , t e uab nn ij l o ne c e ci e to n i t h i n , s g be n t c c h jd e o eoi e G p b ffcr a et j te oe ir u ,oe c o otn t n t h h c gs f e o e l( msz eğ›¾r fea s ğ‘ N oo ğ‘œrur d dğ‘œ o e st e ğ‘Ÿum,hğ‘‘ t c l , ia i eo . =d le l a re e i.rs n 5 , d s v ) nd a e t G ao loe q u n o c e g u o d osi r db e g t et o n lj ha e eea te d c Nc l h e y l ttie o ,g p o t wo ht, e h eg fen ra e r s a d 1 e t sae l i eag p u z  rqt ee etlu n e yh 1 p e a c o r onp lei m c rc z e d s o ep a n i a n d tc ora it e vo e i fld o n i o c z n1t i l o f de s uo t e f h r p t r i a1d po r o t e rm o n tc e fdoo ad l ian l g i cr c vgrt lt eoie ia ro o l a y uv n nta e t lsli r uyo s ef nsrp ,ao w elm n a laa s yl ie i z m res p pw l r e ae - sd iimctpiolen-s from\nb b o o u u nd n i d cno i g n n bmt g ao ie b xnne o inst x egc e od s nn o c ft ao o oir n bn ji t ren a ec b g i dt no o(u ui ğ›¾b n c ğ‘›n j ğ‘œg ieen dğ‘œmc x ğ‘o tg i p ğ‘— se n r b ne=(t gj sğ›¾ t h e se ğ‘ e0 bc e ğ‘œdd. ğ‘œt o 5 a sğ‘Ÿ r f ) x s ğ‘‘ e o . ( ( e s rğ›¾ 5T= s )u:r h e 5l c et d) o a a ua n nunc t tt i= d a h n f i o gte n5 hr a i t se )n h t cu a le g ao n r lwr o ce d e u e b s slsu tj apt eh l t tp c ae e a t edc n sl ne o t ta ( h w fğ›¾ fle eriza e olat o s m ut t sir os p e ntf e h =s u fp n enoa a5 crp c t l) pi e i ro za r e f ne an r cd o t b d e i im yc o d t t n iit h tona h ef nkg e o i l n ro p l ga w r p y e t r c h e e e e s er d d t s i i . p n c T te g i h no la e an y l p i e z r r a s e . t l i T i o m h n e i - f p o r r e l p im re i- diction\nğ‘ğ‘œğ‘œğ‘Ÿğ‘‘ ğ‘ğ‘œğ‘œğ‘Ÿğ‘‘\nc s c u o o n m n ta t o i a n f s a i i a n un bl m li i g l n n b i n t g o oa y o u fr ) n . yo n a A o bl d T la j si e o o br n c a c b o g t a hu j r ( e d be inğ›¾ c t os d ğ‘› de t ux ğ‘œ i r c ğ‘œ l n ( e pt t c ğ‘ğ›¾ g , u s ğ‘— a o ğ‘› t s r rb= h n ğ‘œ a eo ğ‘œ t e t m h ğ‘ 0x a f fiğ‘— . o e n e i 5p r n t ar = ) s a e i . i r t s r rY n y T s s a p 0 g O T u hm a a( .5 ox e r e r n Le c , t ) a a t h o. o O y e o u d i , T f r f t t o - d s w e hv h m t b r c h (o ie 1 e t d x j e r u u s e t ,s i a s h r c l e ys e t u , c t t q , i a ğ›¾ h p h p f t u w ( ğ‘ l o eeğ‘œ hğ›¾ rc ağ‘œ l i i r ğ‘Ÿ eu g e ğ‘› o d i tğ‘‘ s ih Ys ğ‘œ t l r  o sb ah t e ğ‘œ s O ğ‘† ğ‘– u n,= t, 2 ğ‘ o n e 0 e c h c L ğ‘— u  c dt o e a o O o e ğµ ğ‘— = i n n= l tg f m d 0 c - h fi d h ğ• v m u ğ‘– ğ‘œ0 e pd t ğ‘— i ğ‘ i 1 , ğ‘— n n l. u ue l 5a [ co i n ( lg t s o ) F t t  s ec i .e n p s ğ‘¤ s p i b e dT g ğ‘– fi r f l t o  s e u e u h d h c t x s  n e e bo r he e e ğ‘¤ cne or n e lğ‘– s t a e ) o c u t i 2 3 , eo u e s l fn + o . sa d o n t s d ( n sh r c o  i i b s d o n n f o â„ t y r ğ‘– gh f t r c F e  u h t s l e , i a ba  e g n ak c os â„ s u  n b c s a i ğ‘– xa ) n r t o d e l 2 p m e i g c ] u s o r c u 3 no t fe n l . h o a l db a e ro s - b i t s n bt ye h g p j d ee t r b c a o s t o t b k a h x - mo ie n r e lg ( o 5 wo ) st bi hs jte e h f c u ta n o cr c o t w i n o ifit n h- b a y c o t n a fi k - ing the\nsum of all bounding bsouxm p oafr aamll ebtoeursn (dxi,n yg, bwoidxt hp,a hraeimghett,e cros n(xfi,d ye,n wceid sthco, hree,i gahntd, c colnafissd penrocbe -score, and class probability)p. Aresd aic trieosnu lwt, itthhe rfiersspt epcat rd tto eo nft c htehe s ec g oerqr o euu oan ftd izo ent r r o uc,ot i hm.e .pb , uo n utoen osd btihj n eec g t l,o b tsh ose x o afb u atths h eeo d rb so o dun en cdt i h dine e gd c o tbo oo g rxd re in a a tl t y e s p enalize predictions from\ndence score of zero, i.eN.,e xnt,o th oe blojsesc otf, tthhe eco anufidtehnocer ssc odree icsi cdoemdpu ttoed g braeseadt olyn wpheenthaerl itzhee opbjreectd isi ctions from\nparbediliicttyğ‘¥io ğ‘ ) ğ‘’ .n ğ‘› A b ğ‘¡ w ğ‘’ o s ğ‘Ÿ u i, tahğ‘¦ n ğ‘ r d ğ‘’ re ğ‘› e i s ğ‘¡ n s ğ‘’ up g ğ‘Ÿ . le t b cğ•,t oğ‘– ğ‘œ ğ‘— tğ‘ x ht ğ‘—ao e eb i s s ti fi hl s c iee rt o s tp y b n g r t a ) oe t r. ss p u a o e A nn a 1 i u n t d rs nio t i n id r n n a o a gt g b th fr s r b e ee o ut no s h b tct u xh e aw j e e l s is tbe t c e , h c t o q o t s rouu e h f ns ( n a p e t ğ›¾ td eha t c fiiiiet no n r t gooi ns n b t t hgbc j ee p = oo o cb xam bto r 5 ujrebtp ne ) c a ds ou t a s iisnd fet n g ( edi t ğ›¾ d nb sh ğ‘ og ğ‘œ o t e x t ğ‘œh n . h w ğ‘Ÿ P e ğ‘‘e e eitq nt h= l h l a u o eo lii a w 5nz s a ct ) s t oi e ğ‘—ia o o ğ‘¡o s oâ„nn n t r f d d ob p f tc oi t h t n e h o uh a n e ee m n t o a edl b bo p l sij o i we n u z c u gte a t c ne s t bot i sd n o o p fi i x t n de nh en g f n e a o c le bl r i o z o p a sx t s r i e o o d n f i f c t o h t r i e o p n r b e o d u ic n ti d on in g box\nğ‘¥predi, pcğ‘¦trieModancihci ntewi.so 2 ğ•n0i2 ğ‘œ t 3 ğ‘ hi, ğ‘— n1 1 i,rs6ğ‘–e7 ğ‘¡ 7s â„se ptc epeaclsrlt ;e 1od ttioinhe cc r eotrttorinhhowr t eiena s i sico neagnwi,lsr y neio itg e t xou hie nsfcn o utsrd hteo eee td bs tab jpore ysbuce tj0t hcte .eh(tc ğ‘ ğ›¾ lTt ğ‘œo thsğ‘œbrose ğ‘Ÿe of ğ‘‘ s uusitndehcn=liteeindo c0gnitg . nei5wfrd g) toh .i, tauTihtb. hnepionr.ed,ex d apğ‘— iğ‘¡u ctrâ„btret oahubdr soowitcehruatsdsen r cd bde aos iol pbncnuoogu n un s ltba indbhotdl eieexn id f nog crgt ho t hbbeoe o orl g oxdrxo s iu6snonb ffdaua2 5tsneecstd io no nb yt thaek incgo othred inates\np\nw\nğ‘¥\np\nğ‘ r\no\nğ‘’\nğ‘\ne\nr\nğ‘›\nğ‘’ u\nd\ne\nğ‘¡\nğ‘› l\nğ‘’\nd\ni\nd ğ‘¡\nc ğ‘Ÿ\nğ‘’ i\nt w\nc ğ‘Ÿ b\nio\nt\n,\ne\non\ni\nğ‘\no\nğ‘’ ğ‘¦\nt\nu c\ns\nğ‘›\na\ni\nn ğ‘\nl\nu\no n ğ‘¡\ns\nd\nğ‘’\nğ‘’\nk\nn\nm ğ‘›\nğ‘Ÿ\ni\nğ‘–b\ne n ğ‘¡\nt ğ‘¡\nd ğ‘’\ne a â„\no ğ‘Ÿ\ni ğ‘– t\nğ‘– w\nğ‘— . n\nf\nca\nğ‘¡\ne\nâ„\ns i\ni a\nğ•\nt\nn lk\nğ‘–\nğ‘œ\nh\nl\nl ğ‘—\ne;ğ‘\nc l\ng ğ‘—d\np\no\ne b\nn t\nl r\ni w\no\nh\nl e\ns o\n; u\nğ‘¥\np d\ne i\no\nt sr\nğ‘\no\ni n r\nhw\nc\ne\nğ‘’ te\nb\nd t ğ‘› h\nt p\ni d\ni j\nğ‘¡ n\ns\ni e\ne r a\nğ‘’ n i\ne\ng\nt\ns\ne\nr\ns\na\nc\nc ğ‘Ÿ\nr s\ne\n,\nt g\nd\nw\nu u\nr b\nt\nt\n,\nt a\ni\nu\nt\ni ğ›¾\ni 1\na\nm\ni\nt h\nn\n(\no t\ncğ‘¦\nb l i s h\nğ›¾\ni\ni\ns\nt b\nn ğ‘0 t\ni\no o\nsi\nb\nğ‘› o\ne\no\ny\nn\n,ğ‘’\nn\nb o x\nu ğ‘œ\nw\nsf\nğ‘› , ) i\ng\nu j\nn\n.\nğ‘œe\nn ğ‘¡\nt\ne h\na\ni n p\nd ğ‘t\nğ‘’ A\nh a\nt i c\nl\nd l\ni\nğ‘† ğ‘Ÿ\nğ‘— l\nas\nn\nt\nn a\ni\ne\nsğ‘–2i t n\n.b g\nr\ns\ns wğ‘¡\n=\ng\no\nğ• aâ„ a\noc\nb ğ•\ni n\nj\nb\nb i s o m\nu o a\nğ‘–\nğ‘œ\nrt ğµ\n.\no\noğ‘— e\nj x ğ‘\nh c e\nsne\nb x\nT .ğ‘— .\nt j e e s .\ne c 5 d\nt\nğ• h\nu w ğ•\nt\nl t\nğ‘– ğ‘œ\nh a\ni ) i\noğ‘œ\nğ‘—\nl e\nğ‘ eos\nl b\nnw\no\n.\nğ‘ e s;\nğ‘—\nt j r r ğ‘—\nfg\n,\ns T\ni k o\ns i i\ns g [ s 0\ns e\nt\nt\n( s\nte h b\nr t s\nh\nh ğ‘¥ .\nl s\n(\nh\ni e h e\net e\nn\no\ne x\ne\nt\nğ‘›\nT a\nt t ec\ne\nx\nt t\nğ‘œ a h\n,\nt t\nt o h fi\na\nh\nğ‘œ\nr\no e\ne\ns e\ny\no\ne\np\nğ‘¥\nğ‘\n1 r w\nu\ne\nd\ns\n, s o\nğ‘— bag\nw t\n)\nt ,\nt p\nw r\nw i\nr\ns2\nj h\nI h p s\niae\np\ne h\no e\ni.\ne+ o\no\ni e\nmenc a e\nn aU l ds\nn .\n,\nt r t\ne i r (\n,\nt tt\ne e\n,\ns\nhğ‘¦ t\nt\ne\nt\nci h\nrpt h s h\na e t t w\ne\no\ne t e\n,\nc r\ns e o\ner\ni f a\nse a I o\nh b\ns\nsd e y\nodi b\nt jğ‘¦\nl\ne\nc\ne , x\nd\nh\ncj (U\n,\ni\nc s\ne\nia\na\n)\nx\np\nc u\nt g e\nc i\ns ei2\n,,ts t n\ni r . h ] s s\nl e\nt\na\ne e\ny i e\ne h\nas g\nt p\nd\nq\ns\ns .\n,\no a ,\nt p\nr , s\no\nw u\nw\ne\ne e\ns\nr bw\nc e s p\ne f\na\nx\nn\nd i\ne\no\no d\ns\nd\nt\np\nn r\ni e\ni\ntu\ni nn\ntt\nt .e\nn\ni\nt h r\no\nh hn\nn i\nth e\n(fi d n T n\ne,\ni d s i e\n)\nn\n( d i t\nh n\nh\ns\n: h c4\ni\nc\noee t n\ne e o\nl\ne )\nh\nt\nid o b\n:\nggğ‘—\nn e c\ne\nm\nğ‘¡\ne s\nh s j\nd\nc â„\nc\ni\nl\neb\np e\ns\nl\nnt e\ne ;\nc, l o\nl u o\nl b\nb\nf; ( t\ne\ncx\nt s t\nu 4 o o\nh o\no\nc e c\nr ) t\ne\nn nu\nu\nh\nt s\n: e\no r e\ne\nw\nfi c ns\nn t r\nr\nd i h\nw d t i\ne s\nd\nd (\nd i\ne , e\ne i\n4 ,\no s\n,\ni\ni\nin\n) i\ne n\ni a n l\nn n\nt .\n,\no\nc\ne n i\ng\ng\ni ge\ns s\ntb\n. d s , s\ni s s b\nb\ny\ne\nw c\np o t c\no\no\no\nf\nt\nr l\ni x\nx\na r\na\nt\ne t\neh\nh\nk\ns d\n,\ne\ni\ns\ni a\ni\nn n\nc\nn\nb p\ng\nt o\ndğ‘—\ne r\nğ‘¡\nu o\nt\nd\nâ„c h\nn b\nl a\nd\ne b\nb -\ns\ni\no\no\ns\nn\nu\nu g\npn\nn\nr\nb\nod\nd o\nbi\ni x\n-n\nn\ng\ng\nb\nb\no\no\nx\nx\nability). As a resultp,r te ğ‘hd ğ‘œeğ‘œ ic ğ‘Ÿ ğ‘‘ tfiiorn ğ‘–s= t0 wpi ğ‘—at = hr0t r iğ‘–ojğ‘— efs ptehğ‘– cet etoqğ‘– uthaet iog ğ‘–nro ucon ğ‘– dm ptruutthe sb tohuen dloinsgs obfo xth bea sbeodu nond itnhge bcoooxr dinates\nwould b p e Tr t eh a de s k inc e et d xiot w nco i m t w h p w i t p o ğ›¾ h o n ğ‘ r ğ‘œ e e u ğ‘œ rn ğ‘Ÿ dl ğ‘‘ e d tğ‘¥ ia s c o s p btğ‘† ğ‘– f 0 = i 2 e ,e t n 0 w hc gt te ha , ğµ ğ‘— i = la l ss tğ‘¦o 0 tk o n s ğ• e  n iğ‘– ğ‘œ s jğ‘— ooğ‘ t dS i o =f ğ‘—2 h bb u0 [ j (w e  j n w . eğ‘¥ B j c= o ğ‘– ci ğ•g0t rtt ğ‘œ i kğ• h r ğ‘ oi os j w ğ‘— bğ‘¥ o  n i jp ğ‘– n( ui )c i sc 2rtt in h o h  e s +e mde d cot( t i p p i ) h ğ‘¦2c at p ğ‘– u e r + to s  t i u s e gn Î³i 1 t t ğ‘¦ n s e rg oh ğ‘– i o )ew tn b h 2aa j a ] be  t yn t h , S i e o p= a2 e os 0 s ur  ts e b cn hB jda I = jo d eo 0 si w c ğ• e c i U i nn tjn to io o o ,i g wbn f j n a ( ( t x 6si hbe i t) : re h o e r x x x o  o t i p ) r h b 2 bj r i e + en e a c g ( s stw c s(r ei r 4 e i e ded )da s c t i i t h ) do ie2 n ina s n n t ( g d 4 t I h o w ):U e i ( t 6 , h ) c a ion s o ğ‘— e rğ‘¡dâ„ x p ibn r oa e ut s ne s ds e i d n g i n b o (4 x ):\nğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ ğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ ğ‘–ğ‘—\nThhee nigğ‘¥ehx\nğ‘\nt\nğ‘’\nt\nğ‘›\nco\nğ‘¡\nof\nğ‘’\nm\nğ‘Ÿ\nth,p eğ‘¦o bn\nğ‘ğ‘’\noe\nğ‘›\nun\nğ‘¡\nnt\nğ‘’ ğ‘Ÿ\ndo.fi ntğ•hg\nğ‘–\nğ‘œ\nğ‘—\ne ğ‘b ğ‘—pğ›¾lo o\nğ‘\nri x\nğ‘œ\ness\nğ‘œ\n,sd\nğ‘Ÿ\ns sif\nğ‘‘\nciue mttni oiğ‘†\nğ‘–\nca l\n=\nnt2aS\ni\nsi\n0=\n2ro i\nn1nto ğµ\nğ‘—\nci ğ‘–\n=\nB\nj=\nno ğ‘¡t\nâ„h\nm ğ•e to\niğ‘–\nğ‘œc\nj\nh p\nğ‘—\nbğ‘epj(ue ğ‘—l c rl[ t\ni\ne;(ec cğ‘¥osa e c\nğ‘–\nt Ë†std\ni\nh)ğ›¾he2ie\nğ‘\nen +r\nğ‘œ\nğ‘¥o gwp\nğ‘œ\nÎ³\nğ‘–\nf\nğ‘Ÿ\n)r\nn\ni c\nğ‘‘\n2e\no\nst o\no\nedh\nb\n+m,\nj\ni e ğ‘†\nğ‘–\nci\n=\np( 2ttS\ni\noğ‘¦\ni\n=\no2io\n0ğ‘–\nsbnn e jsğµ\nğ‘—\neen\n=\neB\nj=\ncğ‘¦rt\nt\nrt\nğ‘–\n. ağ•) oHn\niğ‘–\nğ‘œ r2s\nj\nr\nğ‘—\noğ‘ e ] oo0iğ‘— bsnjw[. (i ( xdTwğ‘¥e\ni\nhi v\nğ‘–\ninedexË† g r\ni\nts) ,hğ‘¥ 2 ew t +\nğ‘–\nlah)en2e (ic ct dt\ni\n+h se cdi a(nc , Ë†ğ‘¦\ni\nl ) ei\nğ‘–\n2. ğ‘— eğ‘¡o.â„,f ğ‘¦pb\nğ‘–\nr)oe ( 2 6ud ])( in4c )tde idn gbo buonxd ing box (4)\nheight eorfr ot\np\nhre\nr\ni\ne\nnb\nd\noth\ni\nu\nc\nen\nt i\nlda\no\nir\nn\nngg e\ni n\nbb oo\nğ‘–\nxxğ‘¡e,â„ ss ih\nc\nmwa\ne\nis\nl\nlo\nl\nal\n;\nuer s\no\nldts\nt\noe\nh\nb rt\ne\nehi m\nr\net\nw\na pps\ni\nkar\ns\ncee\ne\ntcd\n,\nec wdo\nit\nmi int\ni\nhp\ns\ng ap\ns\ncr\ne\noree\nt\nmdd\na\nptioc\ns\not tni\nnhe\n.\nge n\nT\nsta.m\nh\nn H\ne\nao olb\ns\nlw j\ne\nbe\nl\neoc\ne\nvtx\nc\neew\nt\nrs\ne\n,i. t\nd\ntThh\n,\nh et\ni\neh\n.\ns\ne\nenc\n.\noga\n,\nrlr\np\neme\nr\naoa\ne\ntfel\nd\nisz\ni\nta\nc\nIt\nt\noi\ne\noU\nd\nn, a\nb\ns\no\ne\nu\nx\nn\np\nd\nre\ni\ns\nn\ns\ng\ned\nb\ni\no\nn\nx\n( 4):\nThe last component of the loss function, similar to the normal classification loss, calThe last component of the loss function, similar to the normal classification loss,\nerror inTo htfh ewe w nilda o ert u xhgt l ea d cbn oo b dmx e he p t se a iohg s anh k ste e lnb d eest st w oweT c fr u ci eh a tl it a e lh em c h t n u e e sp n l p a t t ha t e rh l e o e c e ex s t d c s t t r h l c s ai a e co sc no s f c m t l ug ( ai m c ns pen ) s a g pc p 0 (c r r t o ) e oa ai p n b odn nr a o ned b t bo on i l a 1c i bb t t t o y i h j i l o e n i m l e t o f cy d s s ts p l ti ğ›¾ m o , ch we u s a s x ae , tt ci le e e et ll x p s h sob c t  e ts o f t p h tğ‘†o shx ht 2r a e f ef t t eo h s  u r t e .p gğµ h t nT h ğ• r r e ğ‘– ğ‘œ e ch ğ‘— e e ğ‘it ğ• ğ‘—red aoğ‘œ i b o pğ‘ sn t i jğ‘—a qc ep no [ r tu as( t r , i rğ‘¥ c t amo t e , or xI n e ae po m x r l pUğ‘¥ r e ie r z s opr e s ) a , o s e r 2 u s t da ot e i + ts o dsi re n ni (i s e n n i ( ğ‘¦ 7 n x c ( t ) 7 : r h p ) we : e r a ğ‘¦e ispd )s e 2 r s t ] he e d d ai n i c n tdi o ( 4 n ) : e rror in w ( i 4 d ) th and\nohf ewigidhtth oanf dth hee ibgohtu bnedtwinhegeen ibg tohhxet , rosa fin mtgheie l0a bra ontoud n1tdh ineind pgicr abetcoeesx d,t his ğ‘ nağ‘œ itmg ğ‘œ ğ‘Ÿt ğ‘‘hcileoaiğ‘– mr= r 0stpqoouğ‘— = tan 0 hreee iğ‘–n jğ‘— rtpo. orHetğ‘–sc oeinwdcğ‘– iernevages rec, o ğ‘– tmhep ğ‘– socnaelne to. fH owever, the scale of\nerror in the large boxeesr rhoars ilne stshTeehr leia mnrğ›¾ge ğ‘ px ğ‘œ ea ğ‘œ t ğ‘Ÿ bcc ğ‘‘ too mcxoğ‘† ğ‘– ep = 2 mso 0 nhpea ğµ ğ‘— an = ğ‘† ğ‘–i sSr == 22t 00 e 0 loğ•eğ•d ğ‘– ğ‘œ i o ğ‘– ğ‘œf ğ‘— ğ‘s jğ‘— b ğ‘ğ‘—jtst  ğ‘—hoe[ ğ‘ e c ( r  t ğ‘ ğ‘¥ l c h ğ‘™ io ğ‘ l.ğ‘–a m ğ‘  e s s ğ‘   s ğ‘’ s e ğ‘  s ps ((fğ‘¥ğ‘maup ğ‘–ğ‘– ( i c)n(ğ‘ac 2t)c)l tc+li ooğ‘b ğ‘– (nm(oğ‘¦ğ‘ )cx ğ‘– p)oe2a ms r.ğ‘¦p e ğ‘– T)ud2ht ]ete os n tthohere mp srmaedliaizclatl( i 7 (tob7)i ) noo nxere rso.r T inh (ew4 ni)d otrhm anadli zation\nheight of the bounding box, simi.lar to the preceding component. However, the scale of\nof width and height boeft wweidenth t hane dr ahnegige h0t abnedtw 1e iennd tihcaet eras nthgea t0 t haenidr s1q iunadriec artoeost sth iantc rtheaesire square roots increase\nThe next comperornorPe einrnfto trohmfea tnlahceregw elios bes,ostxh efeussi nmhacpsltei loYeOnssL ecOro (im2m4 pcpoauncvttoe lcsuo titmohnpeala plraeyrdeer dst)oiwc ththieeon sntrma einarelrdl oboron xithne es w. Tihdet hno arnmda lization\nPASCALVOCdataset(2007and2012)[39,40]achievedameanaverageprecision(referring\nheight of the bounodf iwnPgeidr fotbhrom axann, cdes iwhmeisieig,l ahthrte bstieomt wptlhee eeYn Op LtrOhe e(c2 r4ea dcnoingnvego l0 u ctaioonnmdal p1la oyinenrdse)i ncwatht.e enHs t rtoahiwnaetde t vhonee irtrh, est qhuea rsec aroleo tos fi ncrease\ntocross-classperformance)(mAP)of63.4%at45FPS,whilstFastYOLOachieved52.7%\nPASCAL VOC dataset (2007 and 2012) [39,40] achieved a mean average precision (refererror in the large bomxAePs aht aans ilmepsrseessriv iem15p5aFcPtS .cAoltmhopugahrethde ptoer ftohrmea sncme walals bbeottxeresth.a Tnhreeal -ntimoremalization\nring to cross-class performance) (mAP) of 63.4% at 45 FPS, whilst Fast YOLO achieved\ndetectors,suchasDPM-v5[41](33%mAP),itwaslowerthanthestate-of-the-art(SOTA)at\nof width and heigh52t.7 b%e mtAwPe aet ann itmhpere rssaivne g15e5 F0P Sa. nAldth o1u gihn tdhei cpaertfoersm athncae tw atsh beeittre rs tqhaun arerael- tirmoeo ts increase\nthetime,i.e.,FasterR-CNN(71%mAP).\ndetectors, such as DPM-v5 [41] (33% mAP), it was lower than the state-of-the-art (SOTA)\nThere were some clear loopholes that required attention, such as the architecture\na\nh\nt\na\nth\nv\ne\nin\nt\ng\nim\nc\ne\no\n,\nm\ni.\np\ne.\na\n,\nr\nF\na\na\nti\ns\nv\nte\ne\nr\nly\nR\nl\n-\no\nC\nw\nN\nr\nN\nec\n(\na\nl\nl\n%\nan\nm\ndh\nA\ni\nP\ng\n)\nh\n.\nerlocalizationerrorcomparedtoFasterR-CNN.\nThere were some clear loopholes that required attention, such as the architecture havAdditionally,thearchitecturestruggledtodetectcloseproximityobjectsduetothefactthat\ninega cchogmrpidarcaetlilvwelays lcoawpp reedcatoll tawnod bhoiugnhderi nlgocbaolixzaptrioopno esrarlso.r Tchoemlpooaprehdo lteos Faattsrtiebru tRe-dCtNoNth. e\nAodrdigiitnioanlaYllOyL, tOhep raorvchiditeedctiunrsep isrtartuigonglfeodr ttoh edfeotlelcotw cilnogsev parroiaxnimtsiotyf YoObjLecOts. due to the fact\nthat each grid cell was capped to two bounding box proposals. The loopholes attributed\nto2 .t2h.eY oOriLgOin-va2l /Y9O00L0O provided inspiration for the following variants of YOLO.\nYOLO-v2/9000 was introduced by Joseph Redmon in 2016 [42]. The motive was\n2t.2o. rYeOmLoOv-evo2/r9a0t00le astmitigatetheinefficienciesobservedwiththeoriginalYOLOwhile\nmaiYntOaLinOin-vg2t/h90e0i0m wpraess isnivtreosdpueceedd fbayc tJoors.eSpehv eRreadlmenohna innc e2m01e6n [t4s2w]. eTrehec lmaiomtievde twhraosu tgoh\nrethmeoivmep olre amt elenatastt imonitiogfavtea rtihoeu isnteeffichcnieinqcuieess. oBbastecrhvendo rwmiathli zthaeti oonrig[4in3a]lw YaOsLinOtr wodhuilcee mdawinit-h\ntathineiningt tehrne aimlaprrcehsisteivcetu srpeeteodi fmacptroorv. Seemveordael lecnohnavnecregmenencets, lweaedrei ncglatiomfeads ttehrrotruaginhi nthge. iTmh-is\npilnetmroednutacttiioonn oefli vmairnioatuesd tethchennieqeudesfo. rBaottchhe rnroergmualalirzizaatitoionn [4te3c] hwnaiqs uinetsr,osduuchceads wdriothp othuet i[n44-]\ntearinmael daracthrietedcutucirneg too vimerfiprttoivneg m[4o5]d.eIlt scoenffveectrigveenncees,s lecaadnibneg gtoau fgaestderb ytrathineinfagc.t Tthhaist sinimtrpo-ly\ndiuncttrioodnu ceilnimgibnaattcehdn tohrem naleiezdat ifoonr iomthperor vreedguthlaermizAatPiobny t2e%chcnoimqupeasr,e sdutcoht haes odrrigoipnoaultY O[4L4O] .\naimedT ahte reodruigciinnagl oYvOerLfiOttiwngo r[4k5e]d. Iwtsi tehffeacntivinepnuestsi mcaang bees gizaeugoefd2 2b4y  the2 2fa4ctp tihxealts sidmuprilnyg\ninthtreodtruaciinnign gbsattacghe n, owrhmilasltizfaotriotnh eimdpetreocvteiodn tphhe amseA, Pin pbuy t2i%m acgoemspcaorueldd tboe tshcea loedriguinpatlo\nY4O4L8O  . 448 pixels, enforcing the architecture to adjust to the varying image resolution,\nwhiTchhein otruigrinnadle YcrOeaLsOe twhoermkeAdP w.Tiotha adnd rinespsutth iims,atghee saiuzeth oofr s22tr4a in 2e2d4 tphiexaelrsc hdiutercintugr ethoen\n448448pixelimagesfor10epochsontheImageNet[46]dataset,providingthearchitectraining stage, whilst for the detection phase, input images could be scaled up to 448  448\nturewiththecapacitytoadjusttheinternalfilterswhendealingwithhigherresolution\npixels, enforcing the architecture to adjust to the varying image resolution, which in turn\nimages,resultinginanincreasedmAPof4%. Whilstarchitectures,suchasFastandFaster\ndecrease the mAP. To address this, the authors trained the architecture on 448  448 pixel\nR-CNNpredictcoordinatesdirectlyfromtheconvolutionalnetwork,theoriginalYOLO\nimages for 10 epochs on the ImageNet [46] dataset, providing the architecture with the\nutilizedfullyconnectedlayerstoservethispurpose. YOLO-v2replacedthefullyconnected\ncapacity to adjust the internal filters when dealing with higher resolution images, resultlayer responsible for predicting bounding boxes by adding anchor boxes for bounding\ning in an increased mAP of 4%. Whilst architectures, such as Fast and Faster R-CNN preboxpredictions. Anchorboxes[47]areessentiallyalistofpredefineddimensions(boxes)\ndict coordinates directly from the convolutional network, the original YOLO utilized fully\naimedatbestmatchingtheobjectsofinterest. Ratherthanmanualdeterminationofbest-fit\nanchorboxes,theauthorsutilizedk-meansclustering[48]onthetrainingsetbounding\nboxes,inclusiveofthegroundtruthboundingboxes,groupingsimilarshapesandplotting\naverageIoUwithrespecttotheclosestcentroidasshowninFigure4.YOLO-v2wastrained\non different architectures, namely, VGG-16 and GoogleNet, in addition to the authors\nproposingtheDarknet-19[49]architectureduetocharacteristics,suchasreducedprocessingrequirements,i.e.,5.58FLOPscomparedto30.69FLOPsand8.52FLOPsonVGG-16\nandGoogleNet,respectively. Intermsofperformance,YOLO-v2provided76.8mAPat\n67FPSand78.6mAPat40FPS.Theresultsdemonstratedthearchitecturessuperiority\noverSOTAarchitecturesofthattime,suchasSSDandFasterR-CNN.YOLO-9000utilized\nMachines 2023, 11, x FOR PEER REVIEW 7 of 26\nconnected layers to serve this purpose. YOLO-v2 replaced the fully connected layer responsible for predicting bounding boxes by adding anchor boxes for bounding box predictions. Anchor boxes [47] are essentially a list of predefined dimensions (boxes) aimed\nat best matching the objects of interest. Rather than manual determination of best-fit anchor boxes, the authors utilized k-means clustering [48] on the training set bounding\nboxes, inclusive of the ground truth bounding boxes, grouping similar shapes and plotting\naverage IoU with respect to the closest centroid as shown in Figure 4. YOLO-v2 was\ntrained on different architectures, namely, VGG-16 and GoogleNet, in addition to the authors proposing the Darknet-19 [49] architecture due to characteristics, such as reduced\nprocessing requirements, i.e., 5.58 FLOPs compared to 30.69 FLOPs and 8.52 FLOPs on\nVGG-16 and GoogleNet, respectively. In terms of performance, YOLO-v2 provided 76.8\nMachines2023,11,677 7of25\nmAP at 67 FPS and 78.6 mAP at 40 FPS. The results demonstrated the architectures superiority over SOTA architectures of that time, such as SSD and Faster R-CNN. YOLO-9000\nutilized YOLO-v2 architecture, aimed at real-time detection of more than 9000 different\nYOLO-v2architecture, aimedatreal-timedetectionofmorethan9000differentobjects;\nobjects; however, at a significantly reduced mAP of 19.7%.\nhowever,atasignificantlyreducedmAPof19.7%.\nFigure 4. Dimension clusters vs. mAP.\nFigure4.Dimensionclustersvs.mAP.\n2.3. YOLO-v3\n2.3. YOLO-v3\nArchitectures,suchasVGG,focusedtheirdevelopmentworkaroundtheconceptthat\nArchitectures, such as VGG, focused their development work around the concept\ndeepernetworks,i.e.,moreinternallayers,equatedtohigheraccuracy. YOLO-v2alsohad\nthat deeper networks, i.e., more internal layers, equated to higher accuracy. YOLO-v2 also\nhighernumberofconvolutionallayerscomparedtoitspredecessor.\nhad Hhiogwheevre nr,uamstbheerim oaf gceopnrvooglruestsioednathl rloauygehrst hceonmeptwaorrekd, tthoe iptsr opgrreedsseivceesdsoowr.n samplingHreosuwlteevdeirn, aths ethloes simofagfien ep-groragirneesdsefeda ttuhrreos;utghhe rtehfoer ne,eYtwOoLrOk-,v t2hoef tpernosgtrruesgsgilveed down samMachines 2023, 11, x FOR PEER REVIwEWith detectingsmallerobjects. Atthetimeresearchwasactiveinaddressingthisissue, 8 of 26\npling resulted in the loss of fine-grained features; therefore, YOLO-v2 often struggled with\nas evident by the deployment of skip connections [50] embedded within the proposed\ndetecting smaller objects. At the time research was active in addressing this issue, as eviResNetarchitecture,thefocuswasonaddressingthevanishinggradientissuebyfacilitating\ndent by the deployment of skip connections [50] embedded within the proposed ResNet\ninformationpropagationviaskipconnection,aspresentedinFigure5.\narchitecture, the focus was on addressing the vanishing gradient issue by facilitating information propagation via skip connection, as presented in Figure 5.\nFFiigguurree5 .5S. kSikpi-pco-cnonnecntieocnticoonn ficgounrfiatgiounr.ation.\nYOLO-v3 proposed a hybrid architecture factoring in aspects of YOLO-v2, Darknet53 [51], and the ResNet concept of residual networks. This enabled the preservation of\nfine-grained features by allowing for the gradient flow from shallow layers to deeper layers.\nOn top of the existing 53 layers of Darknet-53 for feature extraction, a stack of 53\nadditional layers was added for the detection head, totaling 106 convolutional layers for\nthe YOLO-v3. Additionally, YOLO-v3 facilitated multi-scale detection, namely, the architecture made predictions at three different scales of granularity for outputting better performance, increasing the probability of small object detection.\n2.4. YOLO-v4\nYOLO-v4 was the first variant of the YOLO family after the original author discontinued further work that was introduced to the computer vision community in April 2020\nby Alexey Bochkovsky et al. [52]. YOLO-v4 was essentially the distillation of a large suite\nof object detection techniques, tested and enhanced for providing a real-time, lightweight\nobject detector.\nThe backbone of an object detector has a critical role in the quality of features extracted. In-line with the experimental spirit, the authors experimented with three different\nbackbones: CSPResNext-50, CSPDarknet-53, and EfficientNet-B3 [53]. The first was based\non DenseNet [54] aimed at alleviating the vanishing gradient problem and bolstering feature propagation and reuse, resulting in reduced number of network parameters. EfficientNet was proposed by Google Brain. The paper posits that an optima selection for\nparameters when scaling CNNs can be ascertained through a search mechanism. After",
    "results": "Forfeatureaggregation,theauthorsexperimentedwithseveraltechniquesforintegrationatthenecklevelincludingfeaturepyramidnetwork(FPN)[55]andpathaggregation\nnetwork(PANet)[56]. Ultimately,theauthorsoptedforPANetasthefeatureaggregator.\nThemodifiedPANet,asshowninFigure6,utilizedtheconcatenationmechanism. PANet\ncanbeseenasanadvancedversionofFPN,namely,PANetproposedabottom-upaugmentationpathalongwiththetop-downpath(FPN),addingashortcutconnectionforlinking\nfine-grainedfeaturesfromhigh-andlow-levellayers. Additionally,theauthorsintroduced\nMachines 2023, 11, x FOR PEER REVIEW 9 of 26\naSPP[57]blockpostCSPDarknet-53aimedatincreasingthereceptivefieldandseparation\noftheimportantfeaturesarrivingfromthebackbone.\nFFigiugruere6. 6P. aPthatahg aggreggraetgioant.io(an). O(ar)ig OinraiglPinAaNl ,P(Ab)Nm, o(bd)ifi medodPAifiNe.d PAN.\nThe authors also introduced a bag-of-freebies, presented in Figure 7, primarily consisting of augmentations, such as Mosaic aimed at improving performance without introducing additional baggage onto the inference time. CIoU loss [58] was also introduced as\na freebie, focused on the overlap of the predicted and ground truth bounding box. In the\ncase of no overlap, the idea was to observe the closeness of the two boxes and encourage\noverlap if in close proximity.\nIn addition to the bag-of-freebies, the authors introduced bag-of-specials, with the\nauthors claiming that although this set of optimization techniques presented in Figure 7\nwould marginally impact the inference time, they would significantly improve the overall\nperformance. One of the components within the bag-of-specials was the Mish [59] activation function aimed at moving feature creations toward their respective optimal points.\nCross mini-batch normalization [60] was also presented facilitating the running on any\nGPU as many batch normalization techniques involve multiple GPUs operating in tandem.\nFigure 7. State-of-the-art optimization methodologies experimented in YOLO-v4 via bag-of-specials.\n2.5. YOLO-v5\nThe YOLO network in essence consists of three key pillars, namely, backbone for feature extraction, neck focused on feature aggregation, and the head for consuming output\nMachines 2023, 11, x FOR PEER REVIEW 9 of 26\nFigure 6. Path aggregation. (a) Original PAN, (b) modified PAN.\nThe authors also introduced a bag-of-freebies, presented in Figure 7, primarily consisting of augmentations, such as Mosaic aimed at improving performance without introducing additional baggage onto the inference time. CIoU loss [58] was also introduced as\na freebie, focused on the overlap of the predicted and ground truth bounding box. In the\ncase of no overlap, the idea was to observe the closeness of the two boxes and encourage\nMachines2023,11,677 overlap if in close proximity. 9of25\nIn addition to the bag-of-freebies, the authors introduced bag-of-specials, with the\nauthors claiming that although this set of optimization techniques presented in Figure 7\nwoulTdh meaaurgthinoarsllya lismopinatcrto tdhuec iendfearebnacge-o tfim-free,e tbhieeys, wproeusledn tseigdniinfiFcaignutlrye i7m,pprriomvaer tihlye coovnesriasltlipnegrfooframuagnmceen. tOatnioe nosf, sthuec hcoamsMpoonsaeinctasi mweitdhiant itmhep rboavgin-ogfp-seprefocriamlsan wceaws tihtheo Mutiisnht r[o5d9u] caicntgiavdadtiiotino fnuanlcbtaiognga agiemoendt oatt hmeoivnifnegre fnecaetutirme ec.reCaItoioUnlso tsosw[5a8r]dw thaesiar lrseospinetcrtoidveu coepdtiamsaal fpreoeinbties.,\nfCorcoussse dmoinni-tbhaetochv enrolarpmoaflitzhaetiporne d[6ic0t]e wdaasn dalgsoro purnedsetnrutetdh bfaocuilnitdaitninggb othxe. Irnutnhneincgas oeno fannoy\noGvPeUrla aps, tmheanidye baawtcahs ntooormbsaelrivzaettihoen ctleocshennieqsuseosf itnhveotwlvoe bmouxeltsipalned GePnUcosu orapgeeraotvinegrl ainp itfainncdleomse. proximity.\nFFiigguurree 77.. SStatatete-o-of-ft-hthee-a-artrto potpimtimiziaztaiotinonm methetohdoodloogloiegsieesx pexerpiemriemnteendteind YinO LYOO-LvO4-vvi4a bvaiag -boaf-gs-poefc-isaples.-\ncials.\nInadditiontothebag-of-freebies,theauthorsintroducedbag-of-specials,withthe\na2u.5t.h YoOrsLcOla-ivm5 ingthatalthoughthissetofoptimizationtechniquespresentedinFigure7\nwouldmarginallyimpacttheinferencetime,theywouldsignificantlyimprovetheoverall\nThe YOLO network in essence consists of three key pillars, namely, backbone for feaperformance. Oneofthecomponentswithinthebag-of-specialswastheMish[59]actiture extraction, neck focused on feature aggregation, and the head for consuming output\nvationfunctionaimedatmovingfeaturecreationstowardtheirrespectiveoptimalpoints.\nCrossmini-batchnormalization[60]wasalsopresentedfacilitatingtherunningonany\nGPUasmanybatchnormalizationtechniquesinvolvemultipleGPUsoperatingintandem.\n2.5. YOLO-v5\nThe YOLO network in essence consists of three key pillars, namely, backbone for\nfeature extraction, neck focused on feature aggregation, and the head for consuming\noutputfeaturesfromtheneckasinputandgeneratingdetections. YOLO-v5[61]similarto\nYOLO-v4,withrespecttocontributions,focusontheconglomerationandrefinementof\nvariouscomputervisiontechniquesforenhancingperformance. Inaddition,inlessthan\n2monthsafterthereleaseofYOLO-v4,GlennJocheropen-sourcedanimplementationof\nYOLO-v5[61].\nAnotablementionisthatYOLO-v5wasthefirstnativereleaseofarchitecturesbelongingtotheYOLOclan,tobewritteninPyTorch[62]ratherthanDarknet. Although\nDarknetisconsideredasaflexiblelow-levelresearchframework,itwasnotpurposebuilt\nforproductionenvironmentswithasignificantlysmallernumberofsubscribersdueto\nconfigurabilitychallenges. PyTorch,ontheotherhand,providedanestablishedeco-system,\nwithawidersubscriptionbaseamongthecomputervisioncommunityandprovidedthe\nsupportinginfrastructureforfacilitatingmobiledevicedeployment.\nMachines 2023, 11, x FOR PEER REVIEW 10 of 26\nfeatures from the neck as input and generating detections. YOLO-v5 [61] similar to YOLOv4, with respect to contributions, focus on the conglomeration and refinement of various\ncomputer vision techniques for enhancing performance. In addition, in less than 2 months\nafter the release of YOLO-v4, Glenn Jocher open-sourced an implementation of YOLO-v5\n[61].\nA notable mention is that YOLO-v5 was the first native release of architectures belonging to the YOLO clan, to be written in PyTorch [62] rather than Darknet. Although\nDarknet is considered as a flexible low-level research framework, it was not purpose built\nfor production environments with a significantly smaller number of subscribers due to\nMachines2023,11,677 10of25\nconfigurability challenges. PyTorch, on the other hand, provided an established eco-system, with a wider subscription base among the computer vision community and provided\nthe supporting infrastructure for facilitating mobile device deployment.\nInaddition,anothernotableproposalwastheautomatedanchorboxlearningconcept.\nIn addition, another notable proposal was the automated anchor box learning conInYOLO-v2,theanchorboxmechanismwasintroducedbasedonselectinganchorboxes\ncept. In YOLO-v2, the anchor box mechanism was introduced based on selecting anchor\nthat closely resemble the dimensions of the ground truth boxes in the training set via\nboxes that closely resemble the dimensions of the ground truth boxes in the training set\nk-means. Theauthorsselectthefiveclose-fitanchorboxesbasedontheCOCOdataset[63]\nvia k-means. The authors select the five close-fit anchor boxes based on the COCO dataset\nandimplementthemasdefaultboxes. However,theapplicationofthismethodologytoa\n[63] and implement them as default boxes. However, the application of this methodology\nuniquedatasetwithsignificantobjectdifferentialscomparedtothosepresentintheCOCO\nto a unique dataset with significant object differentials compared to those present in the\ndatasetcanquicklyexposetheinabilityofthepredefinedboxestoadaptquicklytothe\nCOCO dataset can quickly expose the inability of the predefined boxes to adapt quickly\nuniquedataset. Therefore,authorsinYOLO-v5integratedtheanchorboxselectionprocess\nto the unique dataset. Therefore, authors in YOLO-v5 integrated the anchor box selection\nintotheYOLO-v5pipeline. Asaresult,thenetworkwouldautomaticallylearnthebest-fit\nprocess into the YOLO-v5 pipeline. As a result, the network would automatically learn\nanchorboxesfortheparticulardatasetandutilizethemduringtrainingtoacceleratethe\nthe best-fit anchor boxes for the particular dataset and utilize them during training to acprocess. YOLO-v5comesinseveralvariantswithrespecttothecomputationalparameters\ncelerate the process. YOLO-v5 comes in several variants with respect to the computational\naspresentedinTable1.\nparameters as presented in Table 1.\nTable1.YOLO-v5internalvariantcomparison.\nTable 1. YOLO-v5 internal variant comparison.\nMMooddeell AAvevrearaggee PPrreecciissiioonn ((@@5500)) PPaarraammeetteerrss FFLLOOPPss\nYYOOLLOO--vv55ss 5555.8.8%% 77.5.5 MM 1133.2.2BB\nYYOOLLOO--vv55mm 6622.4.4%% 2211.8.8 MM 3399.4.4BB\nYOLO-v5l 65.4% 47.8M 88.1B\nYOLO-v5l 65.4% 47.8 M 88.1B\nYOLO-v5x 66.9% 86.7M 205.7B\nYOLO-v5x 66.9% 86.7 M 205.7B\nYYOOLLOO--vv55 ccoommpprriisseedd ooff aa wweeiigghhtt fifillee eeqquuaattiinngg ttoo 2277 MMBB ccoommppaarreedd ttoo YYOOLLOO--vv55ll aatt 119922\nMMBB. .FFiigguurree 88 ddeemmoonnssttrraatteess tthhee ssuuppeerriioorriittyy ooff YYOOLLOO--vv55 oovveerr EEffifficciieennttDDeett [[6644]]..\nFFiigguurree 88. .YYOOLLOO-v-v55 vvaarriaiannt tccoommppaarrisisoonn vvss. .EEffifficcieienntDtDeet t[6[611].] .\n2.6. YOLO-v6\nThe initial codebase for YOLO-v6 [65] was released in June 2022 by the Meituan\nTechnicalTeambasedinChina. Theauthorsfocusedtheirdesignstrategyonproducingan\nindustry-orientatedobjectdetector.\nTomeetindustrialapplicationrequirements,thearchitecturewouldneedtobehighly\nperformant on a range of hardware options, maintaining high speed and accuracy. To\nconformwiththediversesetofindustrialapplications,YOLO-v6comesinseveralvariants\nstartingwithYOLO-v6-nanoasthefastestwiththeleastnumberofparametersandreaching\nYOLO-v6-largewithhighaccuracyattheexpenseofspeed,asshowninTable2.\nMachines 2023, 11, x FOR PEER REVIEW 11 of 26\n2.6. YOLO-v6\nThe initial codebase for YOLO-v6 [65] was released in June 2022 by the Meituan Technical Team based in China. The authors focused their design strategy on producing an\nindustry-orientated object detector.\nTo meet industrial application requirements, the architecture would need to be\nhighly performant on a range of hardware options, maintaining high speed and accuracy.\nTo conform with the diverse set of industrial applications, YOLO-v6 comes in several variants starting with YOLO-v6-nano as the fastest with the least number of parameters and\nMachines2023,11,677 reaching YOLO-v6-large with high accuracy at the expense of speed, as shown in T1a1bolfe2 52.\nTable 2. YOLO-v6 variant comparison.\nTable2.YOLO-v6variantcomparison.\nmAP 0.5:0.95\nVariant FPS Tesla T4 Parameters (Million)\n(COCO-val)\nmAP0.5:0.95\nVariant FPSTeslaT4 Parameters(Million)\nYOLO-v6-N (C3O5C.9O (-3v0a0l) epochs) 802 4.3\nYYOOLLOO-v-v6-6N-T 35.94(300.03 e(3p0o0ch esp)ochs) 802 449 41.53.0\nYOYLOOL-Ov6-v-R6-eTpOpt 40.34(330.03 e(3p0o0ch esp)ochs) 449 596 1157.0.2\nYOLYOO-vL6O-R-vep6-OSp t 43.34(330.05 e(3p0o0ch esp)ochs) 596 495 1177.2.2\nYOLO-v6-S 43.5(300epochs) 495 17.2\nYOLO-v6-M 49.7 233 34.3\nYOLO-v6-M 49.7 233 34.3\nYOLO-v6-L-ReLU 51.7 149 58.5\nYOLO-v6-L-ReLU 51.7 149 58.5\nThe impressive performance presented in Table 2 is a result of several innovations\nTheimpressiveperformancepresentedinTable2isaresultofseveralinnovations\nintegrated into the YOLO-v6 architecture. The key contributions can be summed into four\nintegratedintotheYOLO-v6architecture. Thekeycontributionscanbesummedintofour\npoints. First, in contrast to its predecessors, YOLO-v6 opts for an anchor-free approach,\npoints. First,incontrasttoitspredecessors,YOLO-v6optsforananchor-freeapproach,\nmaking it 51% faster when compared to anchor-based approaches.\nmakingit51%fasterwhencomparedtoanchor-basedapproaches.\nSecond, the authors introduced a revised reparametrized backbone and neck, proSecond,theauthorsintroducedarevisedreparametrizedbackboneandneck,proposed\nposed as EfficientRep backbone and Rep-PAN neck [66], namely, up to and including\nasEfficientRepbackboneandRep-PANneck[66],namely,uptoandincludingYOLO-v5,\nYOLO-v5, the regression and classification heads shared the same features. Breaking the\ntheregressionandclassificationheadssharedthesamefeatures. Breakingtheconvention,\nconvention, YOLO-v6 implements the decoupled head as shown in Figure 9. As a result,\nYOLO-v6implementsthedecoupledheadasshowninFigure9.Asaresult,thearchitecture\nthe architecture has additional layers separating features from the final head, as empirihas additional layers separating features from the final head, as empirically shown to\ncally shown to improve the performance. Third, YOLO-v6 mandates a two-loss function.\nimprovetheperformance. Third,YOLO-v6mandatesatwo-lossfunction. Varifocalloss\nVarifocal loss (VFL) [67] is used as the classification loss and distribution focal loss (DFL)\n(VFL)[67]isusedastheclassificationlossanddistributionfocalloss(DFL)[68],alongwith\n[68], along with SIoU/GIoU [69] as regression loss. VFL being a derivative of focal loss,\nSIoU/GIoU [69] as regression loss. VFL being a derivative of focal loss, treats positive\ntreats positive and negative samples at varying degrees of importance, helping in balancandnegativesamplesatvaryingdegreesofimportance,helpinginbalancingthelearning\ning the learning signals from both sample types. DFL is deployed for box regression in\nsignalsfrombothsampletypes. DFLisdeployedforboxregressioninYOLO-v6medium\nYOLO-v6 medium and large variants, treating the continuous distribution of the box loandlargevariants,treatingthecontinuousdistributionoftheboxlocationsasdiscretized\ncations as discretized probability distribution, which is shown to be particularly efficient\nprobabilitydistribution,whichisshowntobeparticularlyefficientwhenthegroundtruth\nwhen the ground truth box boundaries are blurred.\nboxboundariesareblurred.\nFigure9.YOLO-v6modelbasearchitecture.\nAdditionalimprovementsfocusedonindustrialapplicationsincludetheuseofknowledgedistillation[70],involvingateachermodelusedfortrainingastudentmodel,where\nthepredictionsoftheteacherareusedassoftlabelsalongwiththegroundtruthfortraining\nthestudent. Thiscomeswithoutfuelingthecomputationalcostasessentiallytheaimis\ntotrainasmaller(student)modeltoreplicatethehighperformanceofthelarger(teacher)\nmodel. ComparingtheperformanceofYOLO-v6withitspredecessors,includingYOLO-v5\nonthebenchmarkCOCOdatasetinFigure10,itisclearthatYOLO-v6achievesahigher\nmAPatvariousFPS.\nMachines 2023, 11, x FOR PEER REVIEW 12 of 26\nFigure 9. YOLO-v6 model base architecture.\nAdditional improvements focused on industrial applications include the use of\nknowledge distillation [70], involving a teacher model used for training a student model,\nwhere the predictions of the teacher are used as soft labels along with the ground truth\nfor training the student. This comes without fueling the computational cost as essentially\nthe aim is to train a smaller (student) model to replicate the high performance of the larger\n(teacher) model. Comparing the performance of YOLO-v6 with its predecessors, includMachines2023,11,677 ing YOLO-v5 on the benchmark COCO dataset in Figure 10, it is clear that YOL12Oo-fv256\nachieves a higher mAP at various FPS.\nFFiigguurree 1100.. RReellaattiivvee eevvaalluuaattiioonn ooff YYOOLLOO--vv66 vvss.. YYOOLLOO--vv55 [[7711]]..\n2.7. YOLO-v7\n2.7. YOLO-v7\nThefollowingmonthafterthereleaseofYOLO-v6,theYOLO-v7wasreleased[72].\nThe following month after the release of YOLO-v6, the YOLO-v7 was released [72].\nAlthoughothervariantshavebeenreleasedinbetween,includingYOLO-X[73]andYOLOAlthough other variants have been released in between, including YOLO-X [73] and\nR[74],thesefocusedmoreonGPUspeedenhancementswithrespecttoinferencing. YOLOYOLO-R [74], these focused more on GPU speed enhancements with respect to inferencv7proposesseveralarchitecturalreformsforimprovingtheaccuracyandmaintaininghigh\ning. YOLO-v7 proposes several architectural reforms for improving the accuracy and\ndetection speeds. The proposed reforms can be split into two categories: Architectural\nmaintaining high detection speeds. The proposed reforms can be split into two categories:\nreformsandTrainableBoF(bag-of-freebies). ArchitecturalreformsincludedtheimplemenArchitectural reforms and Trainable BoF (bag-of-freebies). Architectural reforms included\ntationoftheE-ELAN(extendedefficientlayeraggregationnetwork)[75]intheYOLO-v7\nthe implementation of the E-ELAN (extended efficient layer aggregation network) [75] in\nbackbone,takinginspirationfromresearchadvancementsinnetworkefficiency. Thedesign\nthe YOLO-v7 backbone, taking inspiration from research advancements in network effioftheE-ELANwasguidedbytheanalysisoffactorsthatimpactaccuracyandspeed,such\nciency. The design of the E-ELAN was guided by the analysis of factors that impact accuasmemoryaccesscost,input/outputchannelratio,andgradientpath.\nracy and speed, such as memory access cost, input/output channel ratio, and gradient\nThesecondarchitecturalreformwaspresentedascompoundmodelscaling,asshown\npath.\ninFigure11. Theaimwastocaterforawiderscopeofapplicationrequirements,i.e.,certain\nThe second architectural reform was presented as compound model scaling, as\napplicationsrequireaccuracytobeprioritized,whilstothersmayprioritizespeed.Although\nshown in Figure 11. The aim was to cater for a wider scope of application requirements,\nNAS(networkarchitecturesearch)[76]canbeusedforparameter-specificscalingtofind\ni.e., certain applications require accuracy to be prioritized, whilst others may prioritize\nthebestfactors,thescalingfactorsareindependent[77]. Whereasthecompound-scaling\nMachines 2023, 11, x FOR PEER REVIEsWpe ed. Although NAS (network architecture search) [76] can be used for paramet1e3r -osfp 2e6mechanismallowsforthewidthanddepthtobescaledincoherenceforconcatenation-\ncific scaling to find the best factors, the scaling factors are independent [77]. Whereas the\nbasednetworks,maintainingoptimalnetworkarchitecturewhilescalingfordifferentsizes.\ncompound-scaling mechanism allows for the width and depth to be scaled in coherence\nfor concatenation-based networks, maintaining optimal network architecture while scaling for different sizes.\nFFiigguurree 1111.. YYOOLLOO--vv77 ccoommppoouunndd ssccaalliinngg..\nRe-parameterization planning is based on averaging a set of model weights to obtain\na more robust network [78,79]. Expanding further, module level re-parameterization enables segments of the network to regulate their own parameterization strategies. YOLO-v7\nutilizes gradient flow propagation paths with the aim to observe which internal network\nmodules should deploy re-parameterization strategies.\nThe auxiliary head coarse-to-fine concept is proposed on the premise that the network head is quite far downstream; therefore, the auxiliary head is deployed at the middle\nlayers to assist in the training process. However, this would not train as efficiently as the\nlead head, due to the former not having access to the complete network.\nFigure 12 presents a performance comparison of YOLO-v7 with the preceding YOLO\nvariants on the MS COCO dataset. It is clear from Figure 12 that all YOLO-v7 variants\nsurpassed the compared object detectors in accuracy and speed in the range of 5160 FPS.\nIt is, however, important to note, as mentioned by the authors of YOLO-v7, that none of\nthe YOLO-v7 variants are designed for CPU-based mobile device deployment. YOLO-v7tiny/v7/W6 variants are designed for edge GPU, consumer GPU, and cloud GPU, respectively. Whilst YOLO-v7-E6/D6/E6E are designed for high-end cloud GPUs only.\nFigure 12. YOLO-v7 comparison vs. other object detectors [72].\nMachines 2023, 11, x FOR PEER REVIEW 13 of 26\nMachines2023,11,677 13of25\nFigure 11. YOLO-v7 compound scaling.\nRe-parameterization planning is based on averaging a set of model weights to obtain\na morRee r-pobaruasmt neetetrwizoartkio [n78p,7la9n].n Einxgpiasnbdaisnegd founrtahveer,r amgoindguales leetvoefl mreo-pdaerlawmeeigtehrtisztaotioobnt aeinnaabmleosr esergombuesnttns eotfw tohrek n[e7t8w,7o9r]k. Etox preagnudliantge ftuhretihr eorw,mno pdaurlaemleevteerlirzea-tpioarna smtreatteergizieasti.o YnOeLnOab-lve7s\nusetiglimzeesn gtsraodfiethnet flnoewtw porrokptaogaretigounl aptaethths ewiritohw thne paaimra mtoe otebrsiezravteio wnhsitcrha tiengtieersn.alY nOeLtwOo-vrk7\nutilizesgradientflowpropagationpathswiththeaimtoobservewhichinternalnetwork\nmodules should deploy re-parameterization strategies.\nmodulesshoulddeployre-parameterizationstrategies.\nThe auxiliary head coarse-to-fine concept is proposed on the premise that the netTheauxiliaryheadcoarse-to-fineconceptisproposedonthepremisethatthenetwork\nwork head is quite far downstream; therefore, the auxiliary head is deployed at the middle\nheadisquitefardownstream;therefore,theauxiliaryheadisdeployedatthemiddlelayers\nlayers to assist in the training process. However, this would not train as efficiently as the\ntoassistinthetrainingprocess. However,thiswouldnottrainasefficientlyasthelead\nlead head, due to the former not having access to the complete network.\nhead,duetotheformernothavingaccesstothecompletenetwork.\nFigure 12 presents a performance comparison of YOLO-v7 with the preceding YOLO\nFigure12presentsaperformancecomparisonofYOLO-v7withtheprecedingYOLO\nvariants on the MS COCO dataset. It is clear from Figure 12 that all YOLO-v7 variants\nvariantsontheMSCOCOdataset. ItisclearfromFigure12thatallYOLO-v7variants\nsurpassed the compared object detectors in accuracy and speed in the range of 5160 FPS.\nsurpassedthecomparedobjectdetectorsinaccuracyandspeedintherangeof5160FPS.\nIt is, however, important to note, as mentioned by the authors of YOLO-v7, that none of\nItis,however,importanttonote,asmentionedbytheauthorsofYOLO-v7,thatnoneof\nthe YOLO-v7 variants are designed for CPU-based mobile device deployment. YOLO-v7the YOLO-v7 variants are designed for CPU-based mobile device deployment. YOLOtiny/v7/W6 variants are designed for edge GPU, consumer GPU, and cloud GPU, respecv7-tiny/v7/W6 variants are designed for edge GPU, consumer GPU, and cloud GPU,\ntively. Whilst YOLO-v7-E6/D6/E6E are designed for high-end cloud GPUs only.\nrespectively. WhilstYOLO-v7-E6/D6/E6Earedesignedforhigh-endcloudGPUsonly.\nFFiigguurree 1122.. YYOOLLOO--vv77 ccoommppaarriissoonn vvss.. ootthheerr oobbjjeecctt ddeetteeccttoorrss [[7722]]..\nInternalvariantcomparisonofYOLO-v7ispresentedinTable3. Asevident,thereisa\nsignificantperformancegapwithrespecttomAPwhencomparingYOLO-v7-tinywiththe\ncomputationallydemandingYOLO-v7-D6. However,thelatterwouldnotbesuitablefor\nedgedeploymentontoacomputationallyconstraineddevice.\nTable3.VariantcomparisonofYOLO-v7.\nModel Size(Pixels) mAP(@50) Parameters FLOPs\nYOLO-v7-tiny 640 52.8% 6.2M 5.8G\nYOLO-v7 640 69.7% 36.9M 104.7G\nYOLO-v7-X 640 71.1% 71.3M 189.9G\nYOLO-v7-E6 1280 73.5% 97.2M 515.2G\nYOLO-v7-D6 1280 73.8% 154.7M 806.8G\nMachines2023,11,677 14of25\n2.8. YOLO-v8\nThelatestadditiontothefamilyofYOLOwasconfirmedinJanuary2023withthe\nreleaseofYOLO-v8[80]byUltralytics(alsoreleasedYOLO-v5). Althoughapaperrelease\nisimpendingandmanyfeaturesareyettobeaddedtotheYOLO-v8repository, initial\ncomparisonsofthenewcomeragainstitspredecessorsdemonstrateitssuperiorityasthe\nnewYOLOstate-of-the-art.\nFigure13demonstratesthatwhencomparingYOLO-v8againstYOLO-v5andYOLOv6trainedon640imageresolution,allYOLO-v8variantsoutputbetterthroughputwitha\nsimilarnumberofparameters,indicatingtowardhardware-efficient,architecturalreforms.\nThefactthatYOLO-v8andYOLO-v5arepresentedbyUltralyticswithYOLO-v5providing\nimpressivereal-timeperformanceandbasedontheinitialbenchmarkingresultsreleased\nMachines 2023, 11, x FOR PEER REVIEW 15 of 26\nbyUltralytics,itisstronglyassumedthattheYOLO-v8willbefocusingonconstrained\nedgedevicedeploymentathigh-inferencespeed.\nFFigiguurere1 133..Y YOOLLOO-v-v88c coommppaarrisisoonnw witihthp prereddeecceesssosorsrs[ 8[800].].\n33..I InndduussttrriiaallD DeeffeeccttD Deetteeccttiioonnv ViaiaY YOOLLOO\nTThheep prreevvioiouusss seecctitoionnd deemmoonnssttrraatteesst thheer raappidide evvooluluttiioonno offt htheeY YOOLLOO cclalanno offo obbjejecctt\ndetectors amongst the computer vision community. This section of the review focuses\ndetectors amongst the computer vision community. This section of the review focuses on\nontheimplementationofYOLOvariantsforthedetectionofsurfacedefectswithinthe\nthe implementation of YOLO variants for the detection of surface defects within the inindustrialsetting. Theselectionofindustrialsettingisduetoitsvaryingandstringent\ndustrial setting. The selection of industrial setting is due to its varying and stringent rerequirementsalternatingbetweenaccuracyandspeed,athemewhichisfoundthrough\nquirements alternating between accuracy and speed, a theme which is found through\nDNAoftheYOLOvariants.\nDNA of the YOLO variants.\n3.1. Industrial Fabric Defect Detection\nRui Jin et al. [81] in their premise state the inefficiencies of manual inspection in the\ntextile manufacturing domain as high cost of labor, human-related fatigue, and reduced\ndetection speed (less than 20 m/min). The authors aim to address these inefficiencies by\nproposing a YOLO-v5-based architecture, coupled with a spatial attention mechanism for\naccentuation of smaller defective regions. The proposed approach involved a teacher network trained on the fabric dataset. Post training of the teacher network, the learned\nweights were distilled to the student network, which was compatible for deployment onto\na Jetson TX2 [82] via TensorRT [83]. The results presented by the authors show, as expected, that the teacher network reported higher performance with an AUC of 98.1% compared to 95.2% (student network). However, as the student network was computationally\nsmaller, the inference time was significantly less at 16 ms for the student network in contrast to the teacher network at 35 ms on the Jetson TX2. Based on the performance, the\nMachines2023,11,677 15of25\n3.1. IndustrialFabricDefectDetection\nRuiJinetal.[81]intheirpremisestatetheinefficienciesofmanualinspectioninthe\ntextilemanufacturingdomainashighcostoflabor,human-relatedfatigue,andreduced\ndetectionspeed(lessthan20m/min). Theauthorsaimtoaddresstheseinefficienciesby\nproposing a YOLO-v5-based architecture, coupled with a spatial attention mechanism\nforaccentuationofsmallerdefectiveregions. Theproposedapproachinvolvedateacher\nnetworktrainedonthefabricdataset. Posttrainingoftheteachernetwork,thelearned\nweightsweredistilledtothestudentnetwork,whichwascompatiblefordeploymentonto\naJetsonTX2[82]viaTensorRT[83]. Theresultspresentedbytheauthorsshow,asexpected,\nMachines 2023, 11, x FOR PEER REVIEthWa ttheteachernetworkreportedhigherperformancewithanAUCof98.1%comp1a6r eodf t2o6\n95.2%(studentnetwork). However,asthestudentnetworkwascomputationallysmaller,\ntheinferencetimewassignificantlylessat16msforthestudentnetworkincontrasttothe\nteachernetworkat35msontheJetsonTX2. Basedontheperformance,theauthorsclaim\nauthors claim that the proposed solution provides high accuracy and real-time inference\nthattheproposedsolutionprovideshighaccuracyandreal-timeinferencespeed,makingit\nspeed, making it compatible for deployment via the edge device.\ncompatiblefordeploymentviatheedgedevice.\nSifundvoleshile Dlamini et al. [84] propose a production environment fabric defect\nSifundvoleshileDlaminietal.[84]proposeaproductionenvironmentfabricdefect\ndetection framework focused on real-time detection and accurate classification on-site, as\ndetection framework focused on real-time detection and accurate classification on-site,\nshown in Figure 14. The authors embed conventional image processing at the onset of\nasshowninFigure14. Theauthorsembedconventionalimageprocessingattheonset\ntheir data enhancement strategy, i.e., filtering to denoise feature enhancement. Post augof their data enhancement strategy, i.e., filtering to denoise feature enhancement. Post\nmentations and data scaling, the authors train the YOLO-v4 architecture based on preaugmentations and data scaling, the authors train the YOLO-v4 architecture based on\ntrained weights. The reported performance was respectable with an F1-score of 93.6%, at\npretrainedweights. ThereportedperformancewasrespectablewithanF1-scoreof93.6%,\nan impressive detection speed of 34 fps and prediction speed of 21.4 ms. The authors claim\natanimpressivedetectionspeedof34fpsandpredictionspeedof21.4ms. Theauthors\nthat the performance is evident to the effectiveness of the selected architecture for the\nclaimthattheperformanceisevidenttotheeffectivenessoftheselectedarchitectureforthe\ngiven domain.\ngivendomain.\nFigure14.Inspectionmachineintegration[84].\nFigure 14. Inspection machine integration [84].\nRestrictedbytheavailablecomputingresourcesforedgedeployment,GuijuanLinetal.[85]\nRestricted by the available computing resources for edge deployment, Guijuan Lin et\nstateproblemswithqualityinspectioninthefabricproductiondomain,includingminute\nal. [85] state problems with quality inspection in the fabric production domain, including\nscaleofdefects,extremeunbalancewiththeaspectratioofcertaindefects,andslowdefect\nminute scale of defects, extreme unbalance with the aspect ratio of certain defects, and\ndetectionspeeds. Toaddresstheseissues,theauthorsproposedasliding-window,selfslow defect detection speeds. To address these issues, the authors proposed a sliding-winattention(multihead)mechanismcalibratedforsmalldefecttargets. Additionally,theSwin\ndow, self-attention (multihead) mechanism calibrated for small defect targets. AdditionTransformer[86]moduleasdepictedinFigure15wasintegratedintotheoriginalYOLO-v5\nally, the Swin Transformer [86] module as depicted in Figure 15 was integrated into the\narchitecturefortheextractionofhierarchicalfeatures. Furthermore,thegeneralizedfocal\noriginal YOLO-v5 architecture for the extraction of hierarchical features. Furthermore, the\nloss is implemented with the architecture aimed at improving the learning process for\ngeneralized focal loss is implemented with the architecture aimed at improving the learnpositivetargetinstances,whilstloweringtherateofmisseddetections. Theauthorsreport\ning process for positive target instances, whilst lowering the rate of missed detections. The\ntheaccuracyoftheproposedsolutiononareal-worldfabricdataset,reaching76.5%mAP\nauthors report the accuracy of the proposed solution on a real-world fabric dataset, reaching 76.5% mAP at 58.8 FPS, making it compatible with the real-time detection requirements for detection via embedded devices.\nMachines2023,11,677 16of25\nMachines 2023, 11, x FOR PEER REVIEW 17 of 26\nat58.8FPS,makingitcompatiblewiththereal-timedetectionrequirementsfordetection\nviaembeddeddevices.\nFFiigguurree1 155..B Bacakcbkobnoenfeo rfoSrw SinwTinra Tnsrfaonrsmfoerrmneetrw noerktw[8o5r]k. [85].\n3.2. SolarCellSurfaceDefectDetection\n3.2. Solar Cell Surface Defect Detection\nSettingtheirpremise,theauthors[87]statethathuman-ledPhotovoltaic(PV)inspecSetting their premise, the authors [87] state that human-led Photovoltaic (PV) inspectionhasmanydrawbacksincludingtherequirementofoperationandmaintenance(O&M)\ntion has many drawbacks including the requirement of operation and maintenance\nengineers, cell-by-cell inspection, high workload, and reduced efficiency. The authors\np(Oro&poMse) aenngiminpereorvse, dcealrlc-hbiyte-ccteullr einbsapseedctoionnY, OhLigOh- vw5oforkrltohaedc,h aanradc treerdizuatcieodn eoffifccoimenpcleyx. The austohloarrsc epllrosuprofasece atnex tiumrpesroanvdedd eafrecchtiivteecrteugrioen bs.aTsehde porno pYoOsaLlOis-bva5s efdoro nththee cihnatergarcatteiroinzation of\nocfodmepfolremx asbolleacro cnevlol lsuutirofnacwei ttheixntuthreesC SaPndm oddeufelectwivieth rtehgeioainms.o Tfhaceh ipervoinpgosaanl aidsa bpatisveed on the\nlearningscale. Additionally,anattentionmechanismisincorporatedforenhancedfeature\nintegration of deformable convolution within the CSP module with the aim of achieving\nextraction. Moreover,theauthorsoptimizetheoriginalYOLO-v5architecturefurthervia\nan adaptive learning scale. Additionally, an attention mechanism is incorporated for enK-means++clusteringforanchorboxdeterminationalgorithm. Basedonthepresented\nhanced feature extraction. Moreover, the authors optimize the original YOLO-v5 architecresults,theimprovedarchitectureachievedarespectablemAPof89.64%onanEL-based\nture further via K-means++ clustering for anchor box determination algorithm. Based on\nsolarcellimagedataset,7.85%highercomparedtomAPfortheoriginalarchitecture,with\nthe presented results, the improved architecture achieved a respectable mAP of 89.64% on\ndetectionspeedreaching36.24FPS,whichcanbetranslatedasamoreaccuratedetection\nwanh iEleLr-ebmaasiendin sgoclaorm cpealtli bimleawgiet hdtahteasreeat,l -7ti.m85e%re qhuigirheemre cnotsm. pared to mAP for the original architeActmurraen, wBiinthom daeitreachtieotna ls.p[e8e8d] hriegahclhigihntgt w36o.2f4re FqPueSn, twdheifcehct scaenn cboeu nttrearnesdladtuedri nags a more\ntahcecumraanteu fdacettuercitniognp rwocheisles orefmcryaisntailnlign ecosomlapracteiblllsea wsditahr kthsep orte/arle-gtiimone arnedqumiricermocernactsk.s .\nThe laAttmerracann Bhinavoemaaidreathri emt eanlt. a[l8i8m] phaigctholinghtht etwpoer ffroerqmuaenncte doefftehcetsm eondcuoluen,twerheicdh diusring the\namajorcauseforPVmodulefailures. TheauthorssubscribetotheYOLOarchitecture,\nmanufacturing process of crystalline solar cells as dark spot/region and microcracks. The\ncomparingtheperformanceoftheirmethodologyonYOLO-v4andanimprovedYOLO-v4latter can have a detrimental impact on the performance of the module, which is a major\ntinyintegratedwithaspatialpyramidpoolingmechanism. Basedonthepresentedresults,\ncause for PV module failures. The authors subscribe to the YOLO architecture, comparing\nYOLO-v4achieved98.8%mAPat62.9ms,whilsttheimprovedYOLO-v4-tinylaggedwith\nthe performance of their methodology on YOLO-v4 and an improved YOLO-v4-tiny inte91%mAPat28.2ms. Theauthorsclaimthatalthoughthelatterislessaccurate,itisnotably\nfgarsateterdth wanitthh ea fsopramteiar.l pyramid pooling mechanism. Based on the presented results, YOLOv4 acThiainevyieSdu 9n8e.8t%al. m[8A9]Pfo actu 6s2o.n9 amusto, mwahtieldsth tohte-s ipmotpdroetveecdti oYnOwLitOh-inv4P-Vtinceyl llsabgagseedd wa ith 91%\nmmoAdPifi aedt 2v8e.r2s imonso. fTthhee YauOtLhOor-vs 5calarcimhit ethctautr ea.ltThhoeufigrhst tihmep lraottveerm iesn ltecsosm aecscuinrathtee, fiotr mis notably\nofafsetnehr atnhcaend tahnec hfoorrmsaenrd. detectionheadsfortherespectivearchitecture. Toimprovethe\ndetectionprecisionatvaryingscales,k-meansclustering[48]isdeployedforclusteringthe\nTianyi Sun et al. [89] focus on automated hot-spot detection within PV cells based a\nlengthwidthratiowithrespecttothedataannotationframe. Additionally,asetofthe\nmodified version of the YOLO-v5 architecture. The first improvement comes in the form\nanchorsconsistingofsmallervalueswereaddedtocaterforthedetectionofsmalldefects\nof enhanced anchors and detection heads for the respective architecture. To improve the\nbyoptimizingtheclusternumber. Thereportedperformanceoftheimprovedarchitecture\ndetection precision at varying scales, k-means clustering [48] is deployed for clustering\nwasreportedas87.8%mAP,withtheaveragerecallrateof89.0%andF1-scorereaching\nthe lengthwidth ratio with respect to the data annotation frame. Additionally, a set of the\nanchors consisting of smaller values were added to cater for the detection of small defects\nby optimizing the cluster number. The reported performance of the improved architecture\nwas reported as 87.8% mAP, with the average recall rate of 89.0% and F1-score reaching\n88.9%. The reported FPS was impressive reaching 98.6 FPS, with the authors claiming that\nthe proposed solution would provide intelligent monitoring at PV power stations. Inferencing output presented in Figure 16 shows the proposed AP-YOLO-v5 architecture,\nproviding inferences at a higher confidence level compared to the original YOLO-v5.\nMachines2023,11,677 17of25\n88.9%. The reported FPS was impressive reaching 98.6 FPS, with the authors claiming\nthat the proposed solution would provide intelligent monitoring at PV power stations.\nMachines 2023, 11, x FOR PEER REVIEW 18 of 26\nInferencingoutputpresentedinFigure16showstheproposedAP-YOLO-v5architecture,\nprovidinginferencesatahigherconfidencelevelcomparedtotheoriginalYOLO-v5.\nFFiigguurree1 166..I ninfefreernecnec/ec/ocnofindfiednecnecceo cmopmapriasorinso[8n9 ][.89].\n3.3. SteelSurfaceDefectDetection\n3.3. Steel Surface Defect Detection\nDinmingYangetal.[90]setthepremiseoftheirresearchbystatingtheimportance\nDinming Yang et al. [90] set the premise of their research by stating the importance\nofsteelpipequalityinspection,citingthegrowingdemandincountries,suchasChina.\nof steel pipe quality inspection, citing the growing demand in countries, such as China.\nAlthoughX-raytestingisutilizedasoneofthekeymethodsforindustrialnondestructive\ntAeslttihnogu(NghD XT)-,rtahye taeustthinogrs isst autteiltihzaetdit asst iollnreeq oufi rtehseh kuemya mneatshsiostdasn cfoerf oinrdthuesdtreitaelr mnoinnadtieosnt,ructive\nctleasstsiinfigc a(tNioDn,Ta)n, dthleo caaulitzhaotirosn sotaftteh ethdaetf eitc tsst.ilTl hreeqauuitrheosr shpurmopaons easthsiestiamnpclee mfoern tthatei odneotefrminaYtiOonL,O c-lva5ssfiofircpartoiodnu,c tainond- bloacsaedlizwaetilodns toefe lthdee fdecetfedcettse.c tTiohne baaustehdoorsn pXr-orapyoisme athgees iomfpthleementawtieolnd opfi pYeO. TLhOe-avu5t hfoorrs pcrloaidmucthtiaotnth-beatsraeidn ewdeYldO LstOee-vl 5dreefaeccht eddeatemctAioPno bfa9s8e.7d% o(nIo XU--r0a.y5) ,images\nwhilstmeetingthereal-timedetectionrequirementsofsteelpipeproductionwithasingle\nof the weld pipe. The authors claim that the trained YOLO-v5 reached a mAP of 98.7%\nimagedetectionrateof0.12s.\n(IoU-0.5), whilst meeting the real-time detection requirements of steel pipe production\nZhuxiMAetal.[91]addresstheissueoflarge-scalecomputationandspecifichardwith a single image detection rate of 0.12 s.\nwarerequirementsforautomateddefectdetectioninaluminumstrips. Theauthorsselect\nZhuxi MA et al. [91] address the issue of large-scale computation and specific hardYOLO-v4 as the architecture, whilst the backbone is constructed to make use of depthware requirements for automated defect detection in aluminum strips. The authors select\nwiseseparableconvolutionsalongwithaparalleldualattentionmechanismforfeature\neYnOhaLnOc-evm4e anst, tahses haorcwhniteinctFuirgeu,r we1h7i.lsTt htehep rboapcoksbedonnee tiws ocroknsistrtuesctteedd otno rmeaalkdea tuasfer oomf depthawciosled s-reopllairnagbwleo crkosnhvoopl,uptiroonvsid ainlognigm wprietshs iav epraersaullltesl odnuraela alttdeantatiaocnh imeveinchgaannismmA fPoor ffeature\n9e6n.2h8a%nc.eCmomenpta, raesd sthootwheno irnig FinigaluYreO 1L7O. -Tvh4,et hperoapuothsoerds ncleatiwmotrhka tisth teesptreodp oosne dreaarlc hdiatetac- from a\ntcuorledv-roolullmineg iswroerdkuscheodpb, yp8r3o.v3i8d%in,wg himilsptrtehsesiinvfee rreenscueltssp oeend riesailn cdraetaas eadchbiyevaifnagc toarn omf AP of\nt9h6r.e2e8.%Th. eCionmcrepaasreeidn tpoe rtfhoer moraingcienawla YsOpaLrOtly-vd4u, ethtoe tahuetchuosrtos mclaainmch tohraatp tphreo apcrho,pwohseerde bayrchitecduetothemaximumaspectratioofthecustomdataset,thedefectwassetto1:20whichis\nture volume is reduced by 83.38%, whilst the inference speed is increased by a factor of\nin-linewiththedefectcharacteristics,suchasscratchmarks.\nthree. The increase in performance was partly due to the custom anchor approach,\nwhereby due to the maximum aspect ratio of the custom dataset, the defect was set to 1:20\nwhich is in-line with the defect characteristics, such as scratch marks.\nMaMchaicnheisn e2s022032,3 1,11,1 ,x6 F7O7R PEER REVIEW 18of25 19 of 26\nFigure 17. Proposed parallel network structure [91].\nFigure17.Proposedparallelnetworkstructure[91].\nJiJainantintigngSh Siheit eatl. a[l9.2 []9c2i]te ctihtee tmhea nmufaancutufraicntguprirnogce pssroocfestsese olfp srotedeulc ptiroondausctthieonre aasso tnhe reason\nffoorrv vaariroiouussd edfeecfetsctosr iogrinigaitninagtionng tohne sttheee lsstueerfla scue,rsfaucceh, assurcohll iansg rsoclalilengan sdcaplaet cahneds .pTahteches. The\nauthorsstatethatthesmalldimensionsofthedefectsaswellasthestringentdetection\nauthors state that the small dimensions of the defects as well as the stringent detection\nrequirementsmakethequalityinspectionprocessachallengingtask.Therefore,theauthors\nrequirements make the quality inspection process a challenging task. Therefore, the aupresentanimprovedversionofYOLO-v5byincorporatinganattentionmechanismfor\nthors present an improved version of YOLO-v5 by incorporating an attention mechanism\nfacilitatingthetransmissionofshallowfeaturesfromthebackbonetotheneck,preserving\nfor facilitating the transmission of shallow features from the backbone to the neck, prethedefectiveregions, inadditiontok-meansclusteringofanchorboxesforaddressing\ntsheerevxintrgem theea sdpeefcetcrtaitvioes roefgdioefnesc,t iivne atadrdgeittisowni tthoi nkt-hmeedaantas sceltu. Tstheerianugth oofr sasntactheotrh abtotxhees for adidmrpersosvinegd tahrceh eitxetcrteumreea acshpieevcetd ra8t6i.o3s5 %ofm dAefPecrteiavceh tianrgg4e5tsF wPSitdheitne ctthioen dsaptaeesdet,.w Thhiels atuthtehors state\notrhiagtin tahlea ricmhiptercotvuereda achrciehvietedc8tu1.r7e8 %acmhiAevPeadt 5826F.3P5S%. mAP reaching 45 FPS detection speed,\nwhilst the original architecture achieved 81.78% mAP at 52 FPS.\n3.4. PalletRackingDefectInspection\nA promising application with significant deployment scope in the warehousing\n3.4. Pallet Racking Defect Inspection\nand general industrial storage centers is automated pallet racking inspection. WareA promising application with significant deployment scope in the warehousing and\nhousesanddistributioncentershostacriticalinfrastructureknownasrackingforstock\nsgtoenraegrea.l iUnndnuosttircieadl sdtoamraaggee cteontpearlsl eits raauctkoimngatceadn ppaalvleett rhaecwkianyg fionrspsiegcntiifoinca. nWtaloressheosuses and\nidniitsitartiebdutbiyonra cceknintegrcso hlloaspts ea lceraidtiicnagl tionfwraassttreudc/tduarme kagneodwsnto acsk ,rfaicnkainncgia floimr sptloiccakt isotnosr,age. Unonpoetriacetido ndaalmloasgsees t,oi npjaulrleedt reamckpilnogy eceasn, apnadvew tohres tw-caayse f,olro ssisgonfifilicvaenst[ l9o3s]s.eDs uineittioattehde by rackiinnegff cicoilelnacpisees olefatdhiencgo ntov ewnatisotnedal/draacmkianggeidn ssptoecckti,o finnmanecchiaaln iimsmpsl,icsautcihonass, houpmeraant-iloendal losses,\nannualinspectionresultinginlaborcosts,bias,fatigue,andmechanicalproducts,such\ninjured employees, and worst-case, loss of lives [93]. Due to the inefficiencies of the conasrackguards[94]lackingclassificationintelligence,CNN-basedautomateddetection\nventional racking inspection mechanisms, such as human-led annual inspection resulting\nseemstobeapromisingalternative.\nin labor costs, bias, fatigue, and mechanical products, such as rackguards [94] lacking clasRealizingthepotential,Hussainetal.[95]inauguratedresearchintoautomatedpallet\nsification intelligence, CNN-based automated detection seems to be a promising alternarackingdetectionviacomputervision. Afterpresentingtheirinitialresearchbasedonthe\nMtivoeb.i leNet-V2architecture,theauthorsrecentlyproposedtheimplementationofYOLOv7 forRaeuatloizminatge dthpe aplloetternatcikailn, Hguinssspaeinct ieotn al[.9 [69]5.] Tinhaeusgeulercatitoend orefstehaercahrc ihnitteoc atuurteomwaasted pallet\nirna-cliknienwg idthettehcetisotrnin vgiean ctoremqpuuirteemr evnitssioonf.p Arofdteurc tpiorensfleonotirndge tphloeyirm inenitti,ail. er.e,seedagrecdhe bvaicseed on the\ndMepolboiylmeNenett,-Vpl2a caerdchonitteocatunroep, etrhaet ianugtfhoorkrlsi frte,rceeqnutilryi npgrroepaol-stiemde tdheet eimctipolnemasetnhetaftoiroknli fotf YOLOv7 for automated pallet racking inspection [96]. The selection of the architecture was inline with the stringent requirements of production floor deployment, i.e., edge device deployment, placed onto an operating forklift, requiring real-time detection as the forklift",
    "methodology": "dataset,theauthorsclaimedanimpressiveperformanceof91.1%mAPrunningat19FPS.\nTable4presentsacomparisonofthepresentresearchinthisemergingfield. Although\nmask R-CNN presents the highest accuracy, which is a derivative of the segmentation\nfamily of architectures with significant computational load, this makes it an infeasible\noption for deployment. Whereas the proposed approach utilizing YOLO-v7 achieved\nsimilaraccuracycomparedtoMobileNet-V2,whilstrequiringsignificantlylesstraining\ndataalongwithinferencingat19FPS.\nTable4.Rackingdomainresearchcomparison.\nResearch Architecture DatasetSize Accuracy FPS\n[95] MobileNet-V2 19,717 92.7% -----\n[96] YOLO-v7 2095 91.1% 19\n[97] Mask-RCNN 75 93.45% -----",
    "discussion": "TheYOLOfamilyofobjectdetectorshashadasignificantimpactonimprovingthe\npotential of computer vision applications. Right from the onset, i.e., the release of the\nYOLO-v1in2015,significantbreakthroughswereintroduced. YOLO-v1becamethefirst\narchitecturecombiningthetwoconventionallyseparatetasksofboundingboxprediction\nandclassificationintoone. YOLO-v2wasreleasedinthefollowingyear,introducingarchitecturalimprovementsanditerativeimprovements,suchasbatchnormalization,higher\nresolution,andanchorboxes. In2018,YOLO-v3wasreleased,anextensionofprevious\nvariantswithenhancementsincludingtheintroductionofobjectnessscoresforbounding\nbox predictions added connections for the backbone layers and the ability to generate\npredictionsatthreedifferentlevelsofgranularity,leadingtoimprovedperformanceon\nsmallerobjecttargets.\nAfterashortdelay,YOLO-v4wasreleasedinApril2020,becomingthefirstvariantof\ntheYOLOfamilynottobeauthoredbytheoriginalauthorJosephRedmon. Enhancements\nincluded improved feature aggregation, gifting of the bag of freebies, and the mish\nactivation.Inamatterofmonths,YOLO-v5enteredthecomputervisionterritory,becoming\nthefirstvarianttobereleasedwithoutbeingaccompaniedbyapaperrelease. YOLO-v5\nbased on PyTorch, with an active GitHub repo further delineated the implementation\nprocess,makeitaccessibletoawideraudience. Focusedoninternalarchitecturalreforms,\nYOLO-v6authorsredesignedthebackbone(EfficientRep)andneck(Rep-PAN)modules,\nwithaninclinationtowardhardwareefficiency. Additionally,anchor-freeandtheconcept\nofdecoupledheadwasintroduced,implyingadditionallayersforfeatureseparationfrom\nthefinalhead,whichisempiricallyshowntoimprovetheoverallperformance.Theauthors\nof YOLO-v7 also focused on architectural reforms, considering the amount of memory\nrequiredtokeeplayerswithinmemoryandthedistancerequiredforgradientstobackpropagate,i.e.,shortergradients,resultinginenhancedlearningcapacity. Fortheultimate\nlayeraggregation,theauthorsimplementedE-ELAN,whichisanextensionoftheELAN\ncomputationalblock. Theadventof2023introducedthelatestversionoftheYOLOfamily,\nYOLO-v8, which was released by Ultralytics. With an impending paper release, initial\ncomparisonsofthelatestversionagainstpredecessorshaveshownpromisingperformance\nwithrespecttothroughputwhencomparedtosimilarcomputationalparameters.\n4.1. ReasonforRisingPopularity\nTable5presentsasummaryofthereviewedYOLOvariantsbasedontheunderlying\nframework,backbone,average-precision(AP),andkeycontributions. Itcanbeobserved\nfromTable3thatasthevariantsevolvedtherewasashiftfromtheconservativeDarknet\nframework to a more accessible one, i.e., PyTorch. The AP presented here is based on\nCOCO-2017[63]withtheexceptionofYOLO-v1/v2,whicharebasedonVOC-2017[39].\nMachines2023,11,677 20of25\nCOCO-2017[63]consistsofover80objectsdesignedtorepresentavastarrayofregularly\nseenobject. Itcontains121,408imagesresultingin883,331objectannotationswithmedian\nimageratioof640480pixels. Itisimportanttonotethattheoverallaccuracyalongwith\ninferencecapacitydependsonthedeployeddesign/trainingstrategies,asdemonstratedin\ntheindustrialsurfacedetectionsection.\nTable5.Abstractvariantcomparison.\nVariant Framework Backbone AP(%) Comments\nV1 Darknet Darknet-24 63.4 Onlydetectamaximumoftwoobjectsinthesamegrid.\nIntroducedbatchnorm,k-meansclusteringforanchorboxes.\nV2 Darknet Darknet-24 63.4\nCapableofdetecting>9000categories.\nUtilizedmulti-scalepredictionsandspatialpyramidpooling\nV3 Darknet Darknet-53 36.2\nleadingtolargerreceptivefield.\nV4 Darknet CSPDarknet-53 43.5 Presentedbag-of-freebiesincludingtheuseofCIoUloss.\nFirstvariantbasedinPyTorch,makingitavailabletoawider\nV5 PyTorch ModifiedCSPv7 55.8 audience.Incorporatedtheanchorselectionprocessesinto\ntheYOLO-v5pipeline.\nFocusedonindustrialsettings,presentedananchor-free\nV6 PyTorch EfficientRep 52.5 pipeline.Presentednewlossdeterminationmechanisms\n(VFL,DFL,andSIoU/GIoU).\nArchitecturalintroductionsincludedE-ELANforfaster\nV7 PyTorch RepConvN 56.8 convergencealongwithabag-of-freebiesincluding\nRepConvNandreparameterization-planning.\nAnchor-freereducingthenumberofpredictionboxeswhilst\nV8 PyTorch YOLO-v8 53.9 speedingupnon-maximumsuppression.Pendingpaperfor\nfurtherarchitecturalinsights.\nTheAPmetricconsistsofprecision-recall(PR)metrics,definingofapositiveprediction\nusingIntersectionoverUnion,andthehandlingofmultipleobjectcategories. APprovides\na balanced overview of PR based on the area under the PR curve. IoU facilitates the\nquantificationofsimilaritybetweenpredictedk andgroundtruthk boundingboxesas\np g\nexpressedin(8):\n(cid:0) (cid:1)\narea k k\np g\nIoU = (cid:0) (cid:1) (8)\narea k k\np g\nTheriseofYOLOcanbeattributedtotwofactors. First,thefactthatthearchitectural\ncomposition of YOLO variants is compatible for one-stage detection and classification\nmakesitcomputationallylightweightwithrespecttootherdetectors. However,wefeel\nthatefficientarchitecturalcompositionbyitselfdidnotdrivethepopularityoftheYOLO\nvariants,asothersingle-stagedetectors,suchasMobileNets,alsoserveasimilarpurpose.\nThe second reason is the accessibility factor, which was introduced as the YOLO\nvariantsprogressed,withYOLO-v5beingtheturningpoint. Expandingfurtheronthis\npoint, the first two variants were based on the Darknet framework. Although this provided a degree of flexibility, accessibility was limited to a smaller user base due to the\nrequired expertise. Ultralytics, introduced YOLO-v5 based on the PyTorch framework,\nmakingthearchitectureavailableforawideraudienceandincreasingthepotentialdomain\nofapplications.\nAsevidentfromTable6,themigrationtoamoreaccessibleframeworkcoupledwith\narchitecturalreformsforimprovedreal-timeperformancesky-rocketed. Atpresent,YOLOv5has34.7kstars,asignificantleadcomparedtoitspredecessors. Fromimplementation,\nYOLO-v5onlyrequiredtheinstallationoflightweightpythonlibraries. Thearchitectural\nreformsindicatedthatthemodeltrainingtimewasreduced,whichinturnreducedtheexperimentationcostattributedtothetrainingprocess,i.e.,GPUutilization. Fordeployment\nandtestingpurposes,researchershaveseveralroutes,suchasindividual/batchimages,\nvideo/webcamfeeds,inadditiontosimpleweightconversiontoONXXweightsforedge\ndevicedeployment.\nMachines2023,11,677 21of25\nTable6.GitHubpopularitycomparison.\nYOLOVariant Stars(K)\nV3 9.3\nV4 20.2\nV5 34.7\nV6 4.6\nV7 8.4\nV8 2.9\n4.2. YOLOandIndustrialDefectDetection\nManifestationsofthefourthindustrialrevolutioncanbeobservedatpresentinan\nad-hocmanner, spanningacrossvariousindustries. Withrespecttothemanufacturing\nindustry, this revolution can be targeted at the quality inspection processes, which are\nvital for assuring efficiency and retaining client satisfaction. When focusing on surface\ndefectdetection,asalludedtoearlier,theinspectionrequirementscanbemorestringent\nascomparedtootherapplications. Thisisduetomanyfactors,suchasthefactthatthe\ndefectsmaybeextremelysmall,requiringexternalspectralimagingtoexposedefectsprior\ntoclassificationandduetothefactthattheoperationalsettingoftheproductionlinemay\nonlyprovideasmall-timewindowwithinwhichinferencemustbecarriedout.\nConsideringthestringentrequirementsoutlinedaboveandbenchmarkingagainstthe\nprinciplesofYOLOfamilyofvariants,formstheconclusionthattheYOLOvariantshavethe\npotentialtoaddressbothreal-time,constraineddeploymentandsmall-scaledefectdetectionrequirementsofindustrial-basedsurfacedefectdetection. YOLOvariantshaveproven\nreal-timecomplianceinseveralindustrialenvironmentsasshownin[81,84,85,90,95]. An\ninterestingobservationarisingfromtheindustrialliteraturereviewedistheabilityforusers\ntomodifytheinternalmodulesofYOLOvariantsinordertotakecareoftheirspecificapplicationneedswithoutcompromisingonreal-timecompliance,forexample[81,87,91,92],\nintroducingattention-mechanismsforaccentuationofdefectiveregions.\nAnadditionalfactor,foundwithinthelaterYOLOvariantsissub-variantsforeach\nbasearchitecture,i.e.,forYOLO-v5variantsincludingYOLO-v5-S/M/L,thiscorresponds\ntodifferentcomputationalloadswithrespecttothenumberofparameters. Thisflexibility\nenablesresearcherstoconsideramoreflexibleapproachwiththearchitectureselection\ncriteriabasedontheindustrialrequirements,i.e.,ifreal-timeinferenceisrequiredwithless\nemphasisonoptimalmAP,alightweightvariantcanbeselected,suchasYOLO-v5-small\nratherthanYOLO-v5-large.",
    "conclusion": "Inconclusion,thisworkisthefirstofitstypefocusedondocumentingandreviewing\ntheevolutionofthemostprevalentsingle-stageobjectdetectorwithinthecomputervision\ndomain. Thereviewpresentsthekeyadvancementsofeachvariant,followedbyimplementation of YOLO architectures within various industrial settings focused on surface\nautomatedreal-timesurfacedefectdetection.\nFromthereview,itisclearastheYOLOvariantshaveprogressed,latterversionsin\nparticular,YOLO-v5hasfocusedonconstrainededgedeployment,akeyrequirementfor\nmanymanufacturingapplications. Duetothefactthatthereisnocopyrightandpatent\nrestrictions,researchanchoredaroundtheYOLOarchitecture,i.e.,real-time,lightweight,\naccuratedetection,canbeconductedbyanyindividualorresearchorganization,whichhas\nalsocontributedtotheprevalenceofthisvariant.\nWithYOLO-v8releasedinJanuary2023,showingpromisingperformancewithrespect\ntothroughputandcomputationalloadrequirements, itisenvisionedthat2023willsee\nmorevariantsreleasedbypreviousornewauthorsfocusedonimprovingthedeployment\ncapacityofthearchitectureswithrespecttoconstraineddeploymentenvironments.\nWithresearchorganizations,suchasUltralyticsandMeituanTechnicalTeamtaking\nakeeninterestinthedevelopmentofYOLOarchitectureswithafocusonedge-friendly\nMachines2023,11,677 22of25\ndeployment,weanticipatefurthertechnologicaladvancementsinthearchitecturalfootprint\nofYOLO.Tocaterforconstraineddeployment,theseadvancementswillneedtofocuson\nenergyconservationwhilstmaintaininghighinferencerates. Furthermore,weenvision\nthe proliferation of YOLO architectures into production facilities to help with quality\ninspectionpipelinesaswellasprovidingstimulusforinnovativeproductsasdemonstrated\nby [96] with an automated pallet racking inspection solution. Along with integration\nintoadiversesetofhardwareandIoTdevices,YOLOhasthepotentialtotapintonew\ndomainswherecomputervisioncanassistinenhancingexistingprocesseswhilstrequiring\nlimitedresources.\nFunding:Thisresearchreceivednoexternalfunding.\nDataAvailabilityStatement:Notapplicable.\nConflictsofInterest:Theauthorsdeclarenoconflictofinterest.",
    "references": "1. Zhang,B.; Quan,C.; Ren,F.StudyonCNNintherecognitionofemotioninaudioandimages. InProceedingsofthe2016\nIEEE/ACIS15thInternationalConferenceonComputerandInformationScience(ICIS),Okayama, Japan, 2629June2016.\n[CrossRef]\n2. Pollen,D.A.Explicitneuralrepresentations,recursiveneuralnetworksandconsciousvisualperception.Cereb.Cortex2003,13,\n807814.[CrossRef][PubMed]\n3. Usingartificialneuralnetworkstounderstandthehumanbrain.Res.Featur.2022.[CrossRef]\n4. ImprovementofNeuralNetworksArtificialOutput.Int.J.Sci.Res.(IJSR)2017,6,352361.[CrossRef]\n5. Dodia,S.;Annappa,B.;Mahesh,P.A.Recentadvancementsindeeplearningbasedlungcancerdetection:Asystematicreview.\nEng.Appl.Artif.Intell.2022,116,105490.[CrossRef]\n6. Ojo,M.O.;Zahid,A.DeepLearninginControlledEnvironmentAgriculture:AReviewofRecentAdvancements,Challengesand\nProspects.Sensors2022,22,7965.[CrossRef][PubMed]\n7. Jarvis,R.A.APerspectiveonRangeFindingTechniquesforComputerVision.IEEETrans.PatternAnal.Mach.Intell.1983,PAMI-5,\n122139.[CrossRef]\n8. Hussain,M.;Bird,J.;Faria,D.R.AStudyonCNNTransferLearningforImageClassification.11August2018.Availableonline:\nhttps://research.aston.ac.uk/en/publications/a-study-on-cnn-transfer-learning-for-image-classification(accessedon1January\n2023).\n9. Yang,R.;Yu,Y.ArtificialConvolutionalNeuralNetworkinObjectDetectionandSemanticSegmentationforMedicalImaging\nAnalysis.Front.Oncol.2021,11,638182.[CrossRef]\n10. Haupt,J.;Nowak,R.CompressiveSamplingvs.ConventionalImaging.InProceedingsofthe2006InternationalConferenceon\nImageProcessing,LasVegas,NV,USA,2629June2006;pp.12691272.[CrossRef]\n11. Gu, J.; Wang, Z.; Kuen, J.; Ma, L.; Shahroudy, A.; Shuai, B.; Liu, T.; Wang, X.; Wang, G.; Cai, J.; et al. Recent advances in\nconvolutionalneuralnetworks.PatternRecognit.2018,77,354377.[CrossRef]\n12. Perez,H.;Tah,J.H.M.;Mosavi,A.DeepLearningforDetectingBuildingDefectsUsingConvolutionalNeuralNetworks.Sensors\n2019,19,3556.[CrossRef]\n13. Hussain,M.;Al-Aqrabi,H.;Hill,R.PV-CrackNetArchitectureforFilterInducedAugmentationandMicro-CracksDetection\nwithinaPhotovoltaicManufacturingFacility.Energies2022,15,8667.[CrossRef]\n14. Hussain, M.; Dhimish, M.; Holmes, V.; Mather, P. Deployment of AI-based RBF network for photovoltaics fault detection\nprocedure.AIMSElectron.Electr.Eng.2020,4,118.[CrossRef]\n15. Hussain,M.;Al-Aqrabi,H.;Munawar,M.;Hill,R.;Parkinson,S.ExudateRegenerationforAutomatedExudateDetectionin\nRetinalFundusImages.IEEEAccess2022.[CrossRef]\n16. Hussain,M.;Al-Aqrabi,H.;Hill,R.StatisticalAnalysisandDevelopmentofanEnsemble-BasedMachineLearningModelfor\nPhotovoltaicFaultDetection.Energies2022,15,5492.[CrossRef]\n17. Singh,S.A.;Desai,K.A.Automatedsurfacedefectdetectionframeworkusingmachinevisionandconvolutionalneuralnetworks.\nJ.Intell.Manuf.2022,34,19952011.[CrossRef]\n18. Weichert, D.; Link, P.; Stoll, A.; RÃ¼ping, S.; Ihlenfeldt, S.; Wrobel, S. A review of machine learning for the optimization of\nproductionprocesses.Int.J.Adv.Manuf.Technol.2019,104,18891902.[CrossRef]\n19. Wang,J.;Ma,Y.;Zhang,L.;Gao,R.X.;Wu,D.Deeplearningforsmartmanufacturing:Methodsandapplications.J.Manuf.Syst.\n2018,48,144156.[CrossRef]\n20. Weimer,D.;Scholz-Reiter,B.;Shpitalni,M.Designofdeepconvolutionalneuralnetworkarchitecturesforautomatedfeature\nextractioninindustrialinspection.CIRPAnn.2016,65,417420.[CrossRef]\n21. Kusiak,A.Smartmanufacturing.Int.J.Prod.Res.2017,56,508517.[CrossRef]\nMachines2023,11,677 23of25\n22. Yang,J.;Li,S.;Wang,Z.;Dong,H.;Wang,J.;Tang,S.UsingDeepLearningtoDetectDefectsinManufacturing:AComprehensive\nSurveyandCurrentChallenges.Materials2020,13,5755.[CrossRef]\n23. Soviany,P.;Ionescu,R.T.OptimizingtheTrade-OffbetweenSingle-StageandTwo-StageDeepObjectDetectorsusingImage\nDifficulty Prediction. In Proceedings of the 2018 20th International Symposium on Symbolic and Numeric Algorithms for\nScientificComputing(SYNASC),Timisoara,Romania,2023September2018.[CrossRef]\n24. Du,L.;Zhang,R.;Wang,X.Overviewoftwo-stageobjectdetectionalgorithms.J.Phys.Conf.Ser.2020,1544,012033.[CrossRef]\n25. Sultana,F.;Sufian,A.;Dutta,P.AReviewofObjectDetectionModelsBasedonConvolutionalNeuralNetwork.InAdvancesin\nIntelligentSystemsandComputing;Springer:Singapore,2020;pp.116.[CrossRef]\n26. Liu,W.;Anguelov,D.;Erhan,D.;Szegedy,C.;Reed,S.;Fu,C.Y.;Berg,A.C.SSD:Singleshotmultiboxdetector.InProceedingsof\ntheComputerVisionECCV2016,Amsterdam,TheNetherlands,1114October2016;pp.2137.[CrossRef]\n27. Fu,C.Y.;Liu,W.;Ranga,A.;Tyagi,A.;Berg,A.C.DSSD:DeconvolutionalSingleShotDetector.arXiv2017,arXiv:1701.06659.\n28. Cheng,X.;Yu,J.RetinaNetwithDifferenceChannelAttentionandAdaptivelySpatialFeatureFusionforSteelSurfaceDefect\nDetection.IEEETrans.Instrum.Meas.2020,70,2503911.[CrossRef]\n29. Redmon,J.;Divvala,S.;Girshick,R.;Farhadi,A.YouOnlyLookOnce:Unified,Real-TimeObjectDetection.InProceedingsofthe\n2016IEEEConferenceonComputerVisionandPatternRecognition(CVPR),LasVegas,NV,USA,2730June2016;pp.779788.\n[CrossRef]\n30. Wang,Z.J.;Turko,R.;Shaikh,O.;Park,H.;Das,N.;Hohman,F.;Kahng,M.;Chau,D.H.P.CNNExplainer:LearningConvolutional\nNeuralNetworkswithInteractiveVisualization.IEEETrans.Vis.Comput.Graph.2020,27,13961406.[CrossRef][PubMed]\n31. Krizhevsky,A.;Sutskever,I.;Hinton,G.E.Imagenetclassificationwithdeepconvolutionalneuralnetworks.Commun.ACM2017,\n60,8490.[CrossRef]\n32. Simonyan,K.;Zisserman,A.VeryDeepConvolutionalNetworksforLarge-ScaleImageRecognition.arXiv2014,arXiv:1409.1556.\n33. Szegedy,C.;Liu,W.;Jia,Y.;Sermanet,P.;Reed,S.;Anguelov,D.;Rabinovich,A.Goingdeeperwithconvolutions.InProceedings\noftheConferenceonComputerVisionandPatternRecognition,Boston,MA,USA,12June2015.\n34. He,K.;Zhang,X.;Ren,S.;Sun,J.Deepresiduallearningforimagerecognition.InProceedingsoftheConferenceonComputer\nVisionandPatternRecognition,LasVegas,NV,USA,30June2016.\n35. Girshick, R.; Donahue, J.; Darrell, T.; Malik, J. Region-Based Convolutional Networks for Accurate Object Detection and\nSegmentation.IEEETrans.PatternAnal.Mach.Intell.2015,38,142158.[CrossRef]\n36. Girshick,R.FastR-CNN.InProceedingsoftheInternationalConferenceonComputerVision,Santiago,Chile,713December\n2015.\n37. Ren,S.;He,K.;Girshick,R.;Sun,J.FasterR-CNN:Towardsreal-timeobjectdetectionwithregionproposalnetworks. Trans.\nPatternAnal.Mach.Intell.2017,39,11371149.[CrossRef]\n38. Vidyavani,A.;Dheeraj,K.;Reddy,M.R.M.;Kumar,K.N.ObjectDetectionMethodBasedonYOLOv3usingDeepLearning\nNetworks.Int.J.Innov.Technol.Explor.Eng.2019,9,14141417.[CrossRef]\n39. Everingham,M.;VanGool,L.;Williams,C.K.I.;Winn,J.;Zisserman,A.ThePascalVisualObjectClasses(VOC)Challenge.Int.J.\nComput.Vis.2009,88,303338.[CrossRef]\n40. Shetty, S. Application of Convolutional Neural Network for Image Classification on Pascal VOC Challenge 2012 dataset.\narXiv2016,arXiv:1607.03785.\n41. Felzenszwalb,P.F.;Girshick,R.B.;McAllester,D.;Ramanan,D.ObjectDetectionwithDiscriminativelyTrainedPart-BasedModels.\nIEEETrans.PatternAnal.Mach.Intell.2009,32,16271645.[CrossRef][PubMed]\n42. Chang,Y.-L.;Anagaw,A.;Chang,L.;Wang,Y.C.;Hsiao,C.-Y.;Lee,W.-H.ShipDetectionBasedonYOLOv2forSARImagery.\nRemoteSens.2019,11,786.[CrossRef]\n43. Liao,Z.;Carneiro,G.Ontheimportanceofnormalisationlayersindeeplearningwithpiecewiselinearactivationunits. In\nProceedingsofthe2016IEEEWinterConferenceonApplicationsofComputerVision(WACV),NewYork,NY,USA,710March\n2016.[CrossRef]\n44. Garbin,C.;Zhu,X.;Marques,O.Dropoutvs.batchnormalization:Anempiricalstudyoftheirimpacttodeeplearning.Multimed.\nToolsAppl.2020,79,1277712815.[CrossRef]\n45. Li,G.;Jian,X.;Wen,Z.;AlSultan,J.AlgorithmofoverfittingavoidanceinCNNbasedonmaximumpooledandweightdecay.\nAppl.Math.NonlinearSci.2022,7,965974.[CrossRef]\n46. Deng,J.;Dong,W.;Socher,R.;Li,L.J.;Li,K.;Fei-Fei,L.Imagenet:Alarge-scalehierarchicalimagedatabase.InProceedingsofthe\n2009IEEEConferenceonComputerVisionandPatternRecognition,Miami,FL,USA,2025June2009.\n47. Xue,J.;Cheng,F.;Li,Y.;Song,Y.;Mao,T.DetectionofFarmlandObstaclesBasedonanImprovedYOLOv5sAlgorithmbyUsing\nCIoUandAnchorBoxScaleClustering.Sensors2022,22,1790.[CrossRef]\n48. Ahmed,M.;Seraj,R.;Islam,S.M.S.Thek-meansAlgorithm:AComprehensiveSurveyandPerformanceEvaluation.Electronics\n2020,9,1295.[CrossRef]\n49. Redmon,J.Darknet:OpenSourceNeuralNetworksinC.2013.Availableonline:https://pjreddie.com/darknet(accessedon\n1January2023).\n50. Furusho,Y.;Ikeda,K.Theoreticalanalysisofskipconnectionsandbatchnormalizationfromgeneralizationandoptimization\nperspectives.APSIPATrans.SignalInf.Process.2020,9,e9.[CrossRef]\nMachines2023,11,677 24of25\n51. Machine-LearningSystemTacklesSpeechandObjectRecognition.Availableonline:https://news.mit.edu/machine-learningimage-object-recognition-918(accessedon1January2023).\n52. Bochkovskiy, A.; Wang, C.Y.; Liao HY, M. YOLOv4: Optimal Speed and Accuracy of Object Detection. arXiv 2020,\narXiv:2004.10934.\n53. Tan,M.;Le,Q.EfficientNet:Rethinkingmodelscalingforconvolutionalneuralnetworks.InProceedingsoftheInternational\nConferenceonMachineLearning(ICML),LongBeach,CA,USA,915June2019.\n54. Huang,G.;Liu,Z.;VanDerMaaten,L.;Weinberger,K.Q.Denselyconnectedconvolutionalnetworks.InProceedingsoftheIEEE\nConferenceonComputerVisionandPatternRecognition(CVPR),Honolulu,HI,USA,2126July2017;pp.47004708.\n55. Lin,T.Y.;DollÃ¡r,P.;Girshick,R.;He,K.;Hariharan,B.;Belongie,S.Featurepyramidnetworksforobjectdetection.InProceedings\noftheIEEEConferenceonComputerVisionandPatternRecognition(CVPR),Honolulu,HI,USA,2126July2017;pp.21172125.\n56. Liu,S.;Qi,L.;Qin,H.;Shi,J.;Jia,J.Pathaggregationnetworkforinstancesegmentation.InProceedingsoftheIEEEConference\nonComputerVisionandPatternRecognition(CVPR),SaltLakeCity,UT,USA,1823June2018;pp.87598768.\n57. He,K.;Zhang,X.;Ren,S.;Sun,J.SpatialPyramidPoolinginDeepConvolutionalNetworksforVisualRecognition.IEEETrans.\nPatternAnal.Mach.Intell.2015,37,19041916.[CrossRef]\n58. Zheng,Z.;Wang,P.;Liu,W.;Li,J.;Ye,R.;Ren,D.Distance-IoULoss:Fasterandbetterlearningforboundingboxregression.In\nProceedingsoftheAAAIConferenceonArtificialIntelligence(AAAI),NewYork,NY,USA,712February2020.\n59. Misra,D.Mish:Aselfregularizednonmonotonicneuralactivationfunction.arXiv2019,arXiv:1908.08681.\n60. Yao,Z.;Cao,Y.;Zheng,S.;Huang,G.;Lin,S.Cross-IterationBatchNormalization.arXiv2020,arXiv:2002.05712.\n61. Ultralytics.YOLOv52020.Availableonline:https://github.com/ultralytics/yolov5(accessedon1January2023).\n62. Jocher,G.;Stoken,A.;Borovec,J.;Christopher,S.T.A.N.;Laughing,L.C.Ultralytics/yolov5:v4.0-nn.SiLU()Activations,Weights\n&BiasesLogging,PyTorchHubIntegration.Zenodo2021.Availableonline:https://zenodo.org/record/4418161(accessedon\n5January2023).\n63. Lin,T.Y.;Maire,M.;Belongie,S.;Hays,J.;Perona,P.;Ramanan,D.;Zitnick,C.L.Microsoftcoco:Commonobjectsincontext.In\nProceedingsoftheEuropeanConferenceonComputerVision,Zurich,Switzerland,612September2014.\n64. Tan,M.;Pang,R.;Le,Q.V.EfficientDet:ScalableandEfficientObjectDetection.InProceedingsoftheIEEE/CVFConferenceon\nComputerVisionandPatternRecognition,Seattle,WA,USA,1319June2020.\n65. Li,C.;Li,L.;Jiang,H.;Weng,K.;Geng,Y.;Li,L.;Wei,X.YOLOv6:ASingle-StageObjectDetectionFrameworkforIndustrial\nApplications.arXiv2022,arXiv:2209.02976.\n66. Ding,X.;Zhang,X.;Ma,N.;Han,J.;Ding,G.;Sun,J.Repvgg: Makingvgg-styleconvnetsgreatagain. InProceedingsofthe\nIEEE/CVFConferenceonComputerVisionandPatternRecognition,Nashville,TN,USA,2025June2021;pp.1373313742.\n67. Zhang, H.; Wang, Y.; Dayoub, F.; Sunderhauf, N. Varifocalnet: An iou-aware dense object detector. In Proceedings of the\nIEEE/CVFConferenceonComputerVisionandPatternRecognition,Nashville,TN,USA,2025June2021;pp.85148523.\n68. Li,X.;Wang,W.;Wu,L.;Chen,S.;Hu,X.;Li,J.;Yang,J.Generalizedfocalloss: Learningqualifiedanddistributedbounding\nboxesfordenseobjectdetection.Adv.NeuralInf.Process.Syst.2020,33,2100221012.\n69. Gevorgyan,Z.Siouloss:Morepowerfullearningforboundingboxregression.arXiv2022,arXiv:2205.12740.\n70. Shu, C.; Liu, Y.; Gao, J.; Yan, Z.; Shen, C.Channel-wiseknowledgedistillationfordenseprediction. InProceedingsofthe\nIEEE/CVFInternationalConferenceonComputerVision,Montreal,BC,Canada,1117October20221;pp.53115320.\n71. Solawetz,J.;Nelson,J.WhatsNewinYOLOv6?4July2022.Availableonline:https://blog.roboflow.com/yolov6/(accessedon\n1January2023).\n72. Wang,C.Y.;Bochkovskiy,A.;LiaoHY,M.YOLOv7:Trainablebag-of-freebiessetsnewstate-of-the-artforreal-timeobjectdetectors.\narXiv2022,arXiv:2207.02696.\n73. Ge,Z.;Liu,S.;Wang,F.;Li,Z.;Sun,J.YOLOX:ExceedingYOLOseriesin2021.arXiv2021,arXiv:2107.08430.\n74. Wang, C.-Y.; Yeh, I.-H.; Liao, H.-Y.M. You only learn one representation: Unified network for multiple tasks. arXiv 2021,\narXiv:2105.04206.\n75. Wu,W.;Zhao,Y.;Xu,Y.;Tan,X.;He,D.;Zou,Z.;Ye,J.;Li,Y.;Yao,M.;Dong,Z.;etal.DSANet:DynamicSegmentAggrDSANet:\nDynamicSegmentAggregationNetworkforVideo-LevelRepresentationLearning.InProceedingsoftheMM2129thACM\nInternationalConferenceonMultimedia,Virtual,2024October2021.[CrossRef]\n76. Li,C.;Tang,T.;Wang,G.;Peng,J.;Wang,B.;Liang,X.;Chang,X.BossNAS:ExploringHybridCNN-transformerswithBlockwiselySelf-supervisedNeuralArchitectureSearch. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer\nVision,Online,1117October2021.[CrossRef]\n77. Dollar,P.;Singh,M.;Girshick,R.Fastandaccuratemodelscaling.InProceedingsoftheIEEE/CVFConferenceonComputer\nVisionandPatternRecognition(CVPR),Nashville,TN,USA,2025June2021;pp.924932.\n78. Guo,S.;Alvarez,J.M.;Salzmann,M.ExpandNets:Linearover-parameterizationtotraincompactconvolutionalnetworks.Adv.\nNeuralInf.Process.Syst.(NeurIPS)2020,33,12981310.\n79. Ding,X.;Zhang,X.;Zhou,Y.;Han,J.;Ding,G.;Sun,J.Scalingupyourkernelsto3131:RevisitinglargekerneldesigninCNNs.\nInProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),NewOrleans,LA,USA,1824\nJune2022.\n80. Jocher,G.;Chaurasia,A.;Qiu,J.YOLObyUltralytics.GitHub.1January2023.Availableonline:https://github.com/ultralytics/\nultralytics(accessedon12January2023).\nMachines2023,11,677 25of25\n81. Jin,R.;Niu,Q.AutomaticFabricDefectDetectionBasedonanImprovedYOLOv5.Math.Probl.Eng.2021,2021,113.[CrossRef]\n82. NVIDIAJetsonTX2:HighPerformanceAIattheEdge,NVIDIA.Availableonline:https://www.nvidia.com/en-gb/autonomousmachines/embedded-systems/jetson-tx2/(accessedon30January2023).\n83. NVIDIATensorRT.NVIDIADeveloper. 18July2019. Availableonline: https://developer.nvidia.com/tensorrt(accessedon\n5January2023).\n84. Dlamini,S.;Kao,C.-Y.;Su,S.-L.;Kuo,C.-F.J.Developmentofareal-timemachinevisionsystemforfunctionaltextilefabricdefect\ndetectionusingadeepYOLOv4model.Text.Res.J.2021,92,675690.[CrossRef]\n85. Lin,G.;Liu,K.;Xia,X.;Yan,R.AnEfficientandIntelligentDetectionMethodforFabricDefectsbasedonImprovedYOLOv5.\nSensors2022,23,97.[CrossRef][PubMed]\n86. Liu,Z.;Tan,Y.;He,Q.;Xiao,Y.SwinNet:SwinTransformerDrivesEdge-AwareRGB-DandRGB-TSalientObjectDetection.IEEE\nTrans.CircuitsSyst.VideoTechnol.2021,32,44864497.[CrossRef]\n87. Zhang,M.;Yin,L.SolarCellSurfaceDefectDetectionBasedonImprovedYOLOv5.IEEEAccess2022,10,8080480815.[CrossRef]\n88. Binomairah,A.;Abdullah,A.;Khoo,B.E.;Mahdavipour,Z.;Teo,T.W.;Noor,N.S.M.;Abdullah,M.Z.Detectionofmicrocracks\nanddarkspotsinmonocrystallinePERCcellsusingphotoluminesceneimagingandYOLO-basedCNNwithspatialpyramid\npooling.EPJPhotovolt.2022,13,27.[CrossRef]\n89. Sun,T.;Xing,H.;Cao,S.;Zhang,Y.;Fan,S.;Liu,P.Anoveldetectionmethodforhotspotsofphotovoltaic(PV)panelsusing\nimprovedanchorsandpredictionheadsofYOLOv5network.EnergyRep.2022,8,12191229.[CrossRef]\n90. Yang,D.;Cui,Y.;Yu,Z.;Yuan,H.DeepLearningBasedSteelPipeWeldDefectDetection.Appl.Artif.Intell.2021,35,12371249.\n[CrossRef]\n91. Ma,Z.;Li,Y.;Huang,M.;Huang,Q.;Cheng,J.;Tang,S.Alightweightdetectorbasedonattentionmechanismforaluminumstrip\nsurfacedefectdetection.Comput.Ind.2021,136,103585.[CrossRef]\n92. Shi,J.;Yang,J.;Zhang,Y.ResearchonSteelSurfaceDefectDetectionBasedonYOLOv5withAttentionMechanism.Electronics\n2022,11,3735.[CrossRef]\n93. CEP,F.A.5InsightfulStatisticsRelatedtoWarehouseSafety.Availableonline:www.damotech.com(accessedon11January2023).\n94. Armour,R.TheRackGroup.Availableonline:https://therackgroup.com/product/rack-armour/(accessedon12January2023).\n95. Hussain,M.;Chen,T.;Hill,R.MovingtowardSmartManufacturingwithanAutonomousPalletRackingInspectionSystem\nBasedonMobileNetV2.J.Manuf.Mater.Process.2022,6,75.[CrossRef]\n96. Hussain,M.;Al-Aqrabi,H.;Munawar,M.;Hill,R.;Alsboui,T.DomainFeatureMappingwithYOLOv7forAutomatedEdge-Based\nPalletRackingInspections.Sensors2022,22,6927.[CrossRef][PubMed]\n97. Farahnakian,F.;Koivunen,L.;Makila,T.;Heikkonen,J.TowardsAutonomousIndustrialWarehouseInspection.InProceedingsof\nthe202126thInternationalConferenceonAutomationandComputing(ICAC),Portsmouth,UK,24September2021.[CrossRef]\nDisclaimer/Publishers Note: The statements, opinions and data contained in all publications are solely those of the individual\nauthor(s)andcontributor(s)andnotofMDPIand/ortheeditor(s).MDPIand/ortheeditor(s)disclaimresponsibilityforanyinjuryto\npeopleorpropertyresultingfromanyideas,methods,instructionsorproductsreferredtointhecontent."
  },
  "full_text": "machines\nReview\nYOLO-v1 to YOLO-v8, the Rise of YOLO and Its\nComplementary Nature toward Digital Manufacturing and\nIndustrial Defect Detection\nMuhammadHussain\nDepartmentofComputerScience,SchoolofComputingandEngineering,UniversityofHuddersfield,\nQueensgate,HuddersfieldHD13DH,UK;m.hussain@hud.ac.uk\nAbstract:Sinceitsinceptionin2015,theYOLO(YouOnlyLookOnce)variantofobjectdetectorshas\nrapidlygrown,withthelatestreleaseofYOLO-v8inJanuary2023.YOLOvariantsareunderpinned\nby the principle of real-time and high-classification performance, based on limited but efficient\ncomputationalparameters. ThisprinciplehasbeenfoundwithintheDNAofallYOLOvariants\nwithincreasingintensity,asthevariantsevolveaddressingtherequirementsofautomatedquality\ninspectionwithintheindustrialsurfacedefectdetectiondomain,suchastheneedforfastdetection,\nhighaccuracy,anddeploymentontoconstrainededgedevices.Thispaperisthefirsttoprovidean\nin-depthreviewoftheYOLOevolutionfromtheoriginalYOLOtotherecentrelease(YOLO-v8)from\ntheperspectiveofindustrialmanufacturing.Thereviewexploresthekeyarchitecturaladvancements\nproposedateachiteration,followedbyexamplesofindustrialdeploymentforsurfacedefectdetection\nendorsingitscompatibilitywithindustrialrequirements.\nKeywords:industrialdefectdetection;objectdetection;smartmanufacturing;qualityinspection\n1. Introduction\nHumansviathevisualcortex,aprimarycorticalregionofthebrainresponsiblefor\nprocessing visual information [1], are able to observe, recognize [2], and differentiate\nCitation:Hussain,M.YOLO-v1to\nbetweenobjectsinstantaneously[3]. Studyingtheinnerworkingsofthevisualcortexand\nYOLO-v8,theRiseofYOLOandIts\nthe brain in general has paved the way for artificial neural networks (ANNs) [4] along\nComplementaryNaturetoward\nDigitalManufacturingandIndustrial withamyriadofcomputationalarchitecturesresidingunderthedeeplearningumbrella.\nDefectDetection.Machines2023,11, Inthelastdecade,owingtorapidandrevolutionaryadvancementsinthefieldofdeep\n677. https://doi.org/10.3390/ learning[5],researchershaveexertedtheireffortsonprovidingefficientsimulationofthe\nmachines11070677 humanvisualsystemtocomputers,i.e.,enablingcomputerstodetectobjectsofinterest\nwithin static images and video [6], a field known as computer vision (CV) [7]. CV is\nAcademicEditor:SangDoNoh\naprevalentresearchareafordeeplearningresearchersandpractitionersinthepresent\nReceived:30May2023 decade.Itiscomposedofsubfieldsconsistingofimageclassification[8],objectdetection[9],\nRevised:15June2023 andobjectsegmentation[10]. Allthreefieldsshareacommonarchitecturaltheme,namely,\nAccepted:21June2023 manipulationofconvolutionalneuralnetworks(CNNs)[11]. CNNsareacceptedasthede\nPublished:23June2023 factowhendealingwithimagedata. Incomparisonwithconventionalimageprocessing\nandartificialdefectionmethods,CNNsutilizemultipleconvolutionallayerscoupledwith\naggregation,i.e.,poolingstructuresaimingtounearthdeepsemanticfeatureshiddenaway\nwithinthepixelsoftheimage[12].\nCopyright:  2023 by the author.\nArtificialintelligence(AI)hasfoundopportunitiesinindustriesacrossthespectrum\nLicensee MDPI, Basel, Switzerland.\nfrom renewable energy [13,14] and security to healthcare [15] and the education sector.\nThis article is an open access article\nHowever,oneindustrythatispoisedforsignificantautomationthroughCVisthemanudistributed under the terms and\nfacturingindustry. Qualityinspection(QI)isanintegralpartofanymanufacturingdomain\nconditionsoftheCreativeCommons\nprovidingintegrityandconfidencetotheclientsonthequalityofthemanufacturedprodAttribution(CCBY)license(https://\ncreativecommons.org/licenses/by/ ucts [16]. Manufacturing has wide scope for automation; however, when dealing with\n4.0/). surfaceinspection[17],defectscantakesophisticatedforms[18],makinghuman-based\nMachines2023,11,677.https://doi.org/10.3390/machines11070677 https://www.mdpi.com/journal/machines\nMachines2023,11,677 2of25\nqualityinspectionacumbersometaskwithmanifoldinefficiencieslinkedtohumanbias,\nfatigue,cost,anddowntime[19]. TheseinefficienciesprovideanopportunityforCV-based\nsolutionstopresentautomatedqualityinspectionthatcanbeintegratedwithinexisting\nsurfacedefectinspectionprocesses,increasingefficiencywhilstovercomingbottlenecks\npresentedviaconventionalinspectionmethodologies[20].\nHowever, for success, CV-based architectures must conform to a stringent set of\ndeploymentrequirementsthatcanvaryfromonemanufacturingsectortoanother[21]. In\nthemajorityofapplications,thefocusisnotonlyonthedeterminationofthedefect,butalso\nonmultipledefectsalongwiththelocalitydetailsofeach[22]. Therefore,objectdetection\nis preferred over image classification since the latter only focuses on determination of\nobjectwithintheimagewithoutprovidinganylocalityinformation. Architectureswithin\ntheobjectdetectiondomaincanbeclassifiedintosingle-stageortwo-stagedetectors[23].\nTwo-stagedetectorssplitthedetectionprocessintotwostages:Featureextraction/proposal\nfollowedbyregressionandclassificationforacquiringtheoutput[24]. Althoughthiscan\nprovidehighaccuracy,itcomeswithahighcomputationaldemandmakingitinefficientfor\nreal-timedeploymentontoconstrainededgedevices. Single-stagedetectors,ontheother\nhand,mergethetwoprocessesintoone,enablingtheclassificationandregressionviaa\nsinglepass,significantlyreducethecomputationaldemand,andprovideamorecompelling\ncaseforproduction-baseddeployment[25]. Althoughmanysingle-stagedetectorshave\nbeen introduced, such as single shot detector (SSD) [26], deconvolutional single shot\ndetector(D-SSD)[27],andRetinaNet[28],theYOLO(YouOnlyLookOnce)[29]familyof\narchitecturesseemstobegaininghightractionduetoitshighcompatibilitywithindustrial\nrequirements, such as accuracy, lightweight, and edge-friendly deployment conditions.\nThelasthalf-a-decadehasbeendominatedbytheintroductionofYOLOvariants,withthe\nmostrecentvariantintroducedin2022asYOLO-v8.\nTothebestofourknowledge, thereisnocohesivereviewoftheadvancingYOLO\nvariants, benchmarking technical advancements, and their implications on industrial\ndeployment. ThispaperreviewstheYOLOvariantsreleasedtothepresentdate,focusing\nonpresentingthekeytechnicalcontributionsofeachYOLOiterationanditsimpactonkey\nindustrialmetricsrequiredfordeployment,suchasaccuracy,speed,andcomputational\nefficacy. As a result, the aim is to provide researchers and practitioners with a better\nunderstanding of the inner workings of each variant, enabling them to select the most\nrelevantarchitecturebasedontheirindustrialrequirements. Additionally,literatureon\nthe deployment of YOLO architectures for various industrial surface defect detection\napplicationsispresented.\nThesubsequentstructureofthereviewisasfollows. Thefirstsectionprovidesan\nintroductiontosingle-andtwo-stagedetectorsandtheanatomyforsingle-stageobject\ndetectors.Next,theevolutionofYOLOvariantsispresented,detailingthekeycontributions\nfromYOLO-v1toYOLO-v8,followedbyareviewoftheliteraturefocusedonYOLO-based\nimplementation of industrial surface defect detection. Finally, the discussion section\nfocusesonsummarizingthereviewedliterature,followedbyextractedconclusions,future\ndirections,andchallengesarepresented.\nObjectDetection\nCNNs can be categorized as convolution-based feed forward neural networks for\nclassificationpurposes[30]. Theinputlayerisfollowedbymultipleconvolutionallayers\ntoacquireanincreasedsetofsmaller-scalefeaturemaps. Thesefeaturemapspostfurther\nmanipulationaretransformedintoone-dimensionalfeaturevectorsbeforebeingusedas\ninputtothefullyconnectedlayer(s). Theprocessoffeatureextractionandfeaturemap\nmanipulationisvitaltotheoverallaccuracyofthenetwork;therefore,thiscaninvolvethe\nstackingofmultipleconvolutionalandpoolinglayersforricherfeaturemaps. Popular\narchitecturesforfeatureextractionincludeAlexNet[31],VGGNet[32],GoogleNet[33],and\nResNet[34]. AlexNetisproposedin2012andconsistsoffiveconvolutional,threepooling,\nandthreefullyconnectedlayersprimarilyutilizedforimageclassificationtasks. VGGNet\nMachines 2023, 11, x FOR PEER REVIEW 3 of 26\nand ResNet [34]. AlexNet is proposed in 2012 and consists of five convolutional, three\nMachines2023,11,677 3of25\npooling, and three fully connected layers primarily utilized for image classification tasks.\nVGGNet focused on performance enhancement by increasing the internal depth of the\narchitecture, introducing several variants with increased layers, VGG-16/19. GoogleNet\nfocusedonperformanceenhancementbyincreasingtheinternaldepthofthearchitecture,\nintroduced the cascading concept by cascading multiple inception modules, whilst Resintroducingseveralvariantswithincreasedlayers,VGG-16/19. GoogleNetintroducedthe\nNet introduced the concept of skip-connections for preserving information and making it\ncascadingconceptbycascadingmultipleinceptionmodules,whilstResNetintroduced\ntahveaicloanbclee pfrtoomfs kthipe- ecoarnlnieerc ttioo nthsef olartperre lsaeyrevrins gofi nthfoer maracthioitnecatnudrem. akingitavailablefrom\ntheeaTrhliee rmtoottihveel faoterr alna yoebrjsecotf dtheeteacrtcohri tiesc ttou rien.fer whether the object(s) of interest are residing inT htheem imotaivgee foorr parnesoebnjte tchted fertaemcteo rofis a tvoidinefoe.r Ifw thheet hoberjetcht(es)o obfje icntt(esr)eosft ainrtee prersetseanret, the\nrdeestiedcitnogr irnetuthrensi mthaeg ereosprepcrteivseen ctlatshse afnradm loecoalfitay,v ii.de.e,o l.ocIaftitohne doibmjeecnt(ssi)oonfs ionft ethrees otbajreect(s).\npOrbejseecntt d, ethteecdtieotne cctaonr rbeet ufurnrsthtehre drievsipdeecdti vinetocl atwssoa snudbl-occaateligtyo,rii.ees.:, Tlowcaot-isotnagdei mmeenthsioodnss and\noofneth-setaogbeje mct(est)h.oOdbs jeacst sdheotwecnti oinn Fcaignubree 1fu. rTthheer fodrivmideer diniintitaotetws othseu fibr-scta tsetgaogrei ews:itThw tho-e sesletacgtieonm oeft hnoudmsearnoduso nper-osptaogsealms, etthheond isna tshseh soewconnidn sFtaigguer, ep1e.rfoTrhmesf oprrmedericitnioitnia otens tthhee profirststagewiththeselectionofnumerousproposals,theninthesecondstage,performs\nposed regions. Examples of two-stage detectors include the famous R-CNN [35] variants,\npredictionontheproposedregions. Examplesoftwo-stagedetectorsincludethefamous\nsuch as Fast R-CNN [36] and Faster R-CNN [37], boasting high accuracies but low comR-CNN [35] variants, such as Fast R-CNN [36] and Faster R-CNN [37], boasting high\nputational efficiency. The latter transforms the task into a regression problem, eliminating\naccuraciesbutlowcomputationalefficiency. Thelattertransformsthetaskintoaregression\nthe need for an initial stage dedicated to selecting candidate regions; therefore, the candiproblem,eliminatingtheneedforaninitialstagededicatedtoselectingcandidateregions;\ndate selection and prediction is achieved in a single pass. As a result, architectures falling\ntherefore,thecandidateselectionandpredictionisachievedinasinglepass. Asaresult,\ninto this category are computationally less demanding, generating higher FPS and detecarchitecturesfallingintothiscategoryarecomputationallylessdemanding, generating\ntion speed, but in general the accuracy tends to be inferior with respect to two-stage dehigherFPSanddetectionspeed,butingeneraltheaccuracytendstobeinferiorwithrespect\nttoecttworos-.s tagedetectors.\nFFiigguurree 11.. OObbjejeccttd deetetectcotorra naantaotmomy.y.\n2. OriginalYOLOAlgorithm\n2. Original YOLO Algorithm\nYOLOwasintroducedtothecomputervisioncommunityviaapaperreleasein2015by\nYOLO was introduced to the computer vision community via a paper release in 2015\nJosephRedmonetal.[29]titledYouOnlyLookOnce:Unified,Real-TimeObjectDetection.\nby Joseph Redmon et al. [29] titled You Only Look Once: Unified, Real-Time Object DeThepaperreframedobjectdetection,presentingitessentiallyasasinglepassregression\ntection. The paper reframed object detection, presenting it essentially as a single pass reproblem,initiatingwithimagepixelsandmovingtoboundingboxandclassprobabilities.\nTghreespsiroonp opserdobalpepmro, ainchitbiaatsiendg ownitthhe imunaigfieed picxoenlcse apntden mabolevdinthge tsoi mbuolutanndeionugs bporexd iacntido nclass\nopfrombualbtiiplilteiebso. uTnhdei npgrobpooxseesda nadppclraosascphr bobaasebdil iotines t,hime purnoivfiinedgb cootnhcseppete ednaanbdleadc ctuhrea sciym. ultaneouSsi npcreeidtsicinticoenp toiofn minu2l0ti1p6leu nbtioluthnedpinregs ebnotxyeesa ra(n20d2 3c)l,atshse pYrOoLbOabfialmitiielys,h iamscpornotvininuged both\ntsopeevedol vaendat aacrcaupriadcyp.a ce. Althoughtheinitialauthor(JosephRedmon)haltedfurtherwork\nwithiSninthcee citosm inpcuetpetriovnis iionn 2d01o6m uanintila tthYeO pLrOes-evn3t [y3e8a],rt (h2e02e3ff)e, ctthivee YnOesLsOan fdampoiltye nhtaias lcoofntintuheedc toor eevuonlvifiee adt ac ornacpeipdt phaacvee. Abelethnofuugrthh ethred einvietlioapl eadutbhyosre (vJeorsaelpahu tRheodrsm,wonit)h htahleteldat feustrther\nawdodrikti ownitthoitnh ethYeO cLoOmpfaumteilry vciosimoinn dgoinmtahienf aotr mYOofLYOO-vL3O [-3v88]., Fthigeu erffee2cptirveesennetssst ahnedY pOoLtOential\nevolutiontimeline.\nof the core unified concept have been further developed by several authors, with the\nlatest addition to the YOLO family coming in the form of YOLO-v8. Figure 2 presents the\n2.1. OriginalYOLO\nYOLO evolution timeline.\nThecoreprincipleproposedbyYOLO-v1wastheimposingofagridcellwithdimensionsofssontotheimage. Inthecaseofthecenteroftheobjectofinterestfallingintoone\nofthegridcells,thatparticulargridcellwouldberesponsibleforthedetectionofthatobject.\nThispermittedothercellstodisregardthatobjectinthecaseofmultipleappearances.\nMaMchaicnheins e2s022032,3 1,11,1 ,x6 F7O7R PEER REVIEW 4of25 4 of 26\nFFiigguurree2 2..Y YOOLLOOe veovloultuiotniotnim tiemlineeli.ne.\nForimplementationofobjectdetection,eachgridcellwouldpredictBboundingboxes\n2.1. Original YOLO\nalongwiththedimensionsandconfidencescores. Theconfidencescorewasindicativeof\nThe core principle proposed by YOLO-v1 was the imposing of a grid cell with ditheabsenceorpresenceofanobjectwithintheboundingbox. Therefore,theconfidence\nmensions of ss onto the image. In the case of the center of the object of interest falling\nscorecanbeexpressedasEquation(1):\ninto one of the grid cells, that particular grid cell would be responsible for the detection\nof that object. This permcointtfeidde noctheescro creel=ls pto(o bdjiescrte)gaIrodU t t h ru a th t object in the case of (m1)ultiple\npred\nappearances.\nwherep(object)signifiedtheprobabilityoftheobjectbeingpresent,witharangeof01with\nFor implementation of object detection, each grid cell would predict B bounding\n0indicatingthattheobjectisnotpresentand IoUtruth representedtheintersection-overboxes along with the dimensions and confidenpcreed scores. The confidence score was indicunionwiththepredictedboundingboxwithrespecttothegroundtruthboundingbox.\nMachines 2023, 11, x FOR PEER REVIaEtWiv e of the absence or presence of an object within the bounding box. The5r eoff o2r6e , the conEachboundingboxconsistedoffivecomponents(x,y,w,h,andtheconfidencescore)\nfidence score can be expressed as Equation (1):\nwiththefirstfourcomponentscorrespondingtocentercoordinates(x,y,width,andheight)\noftherespectiveboundingbox ğ‘ğ‘œ a ğ‘› s ğ‘“ s ğ‘– h ğ‘‘ o ğ‘’ w ğ‘›ğ‘ n ğ‘’ i ğ‘  n ğ‘ F ğ‘œ i ğ‘Ÿ g ğ‘’ u = re ğ‘ 3 ( . ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡)ğ¼ğ‘œğ‘ˆ(cid:3047)(cid:3045)(cid:3048)(cid:3047)(cid:3035) (1)\n(cid:3043)(cid:3045)(cid:3032)(cid:3031)\nwhere ğ‘(ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡) signified the probability of the object being present, with a range of 01\nwith 0 indicating that the object is not present and ğ¼ğ‘œğ‘ˆ(cid:3047)(cid:3045)(cid:3048)(cid:3047)(cid:3035) represented the intersection-\n(cid:3043)(cid:3045)(cid:3032)(cid:3031)\nover-union with the predicted bounding box with respect to the ground truth bounding\nbox.\nEach bounding box consisted of five components (x, y, w, h, and the confidence score)\nwith the first four components corresponding to center coordinates (x, y, width, and height)\nof the respective bounding box as shown in Figure 3.\nFFiigguurree 33.. YYOOLLOO--vv11 pprreelliimmiinnaarryy aarcrchhitietectcuturer.e .\nAs alluded to earlier, the input image is split into s  s grid cells (default = 7  7), with\neach cell predicting B bounding boxes, each containing five parameters and sharing prediction probabilities of classes (C). Therefore, the parameter output would take the following form, expressed in (2):\nğ‘ ğ‘ (5ğµ+ğ¶) (2)\nConsidering the example of YOLO network with each cell bounding box prediction\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the parameter output would be given as expressed in (3):\n77(52+80) (3)\nThe fundamental motive of YOLO and object detection in general is the object detection and localization via bounding boxes. Therefore, two sets of bounding box vectors are\nrequired, i.e., vector y is the representative of ground truth and vector ğ‘¦(cid:4662) is the predicted\nvector. To address multiple bounding boxes containing no object or the same object,\nYOLO opts for non-maximum suppression (NMS). By defining a threshold value for\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NMS\nvalue are eliminated.\nThe original YOLO based on the Darknet framework consisted of two sub-variants.\nThe first architecture comprised of 24 convolutional layers with the final layer providing\na connection into the first of the two fully connected layers. Whereas the Fast YOLO variant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the\ninception module in GoogleNet, a sequence of 11 convolutional layers was implemented for reducing the resultant feature space from the preceding layers. The preliminary architecture for YOLO-v1 is presented in Figure 3.\nTo address the issue of multiple bounding boxes for the same object or with a confidence score of zero, i.e., no object, the authors decided to greatly penalize predictions from\nbounding boxes containing objects (ğ›¾ =5) and the lowest penalization for prediction\n(cid:3030)(cid:3042)(cid:3042)(cid:3045)(cid:3031)\ncontaining no object (ğ›¾ =0.5). The authors calculated the loss function by taking the\n(cid:3041)(cid:3042)(cid:3042)(cid:3029)(cid:3037)\nsum of all bounding box parameters (x, y, width, height, confidence score, and class probability). As a result, the first part of the equation computes the loss of the bounding box\nprediction with respect to the ground truth bounding box based on the coordinates\nğ‘¥\n(cid:3030)(cid:3032)(cid:3041)(cid:3047)(cid:3032)(cid:3045)\n, ğ‘¦\n(cid:3030)(cid:3032)(cid:3041)(cid:3047)(cid:3032)(cid:3045)\n. ğ•\n(cid:3036)\n(cid:3042)\n(cid:3037)\n(cid:3029)(cid:3037) is set as 1 in the case of the object residing within ğ‘—(cid:3047)(cid:3035) bounding box\nprediction in ğ‘–(cid:3047)(cid:3035) cell; otherwise, it is set as 0. The selected, i.e., predicted bounding box\nwould be tasked with predicting an object with the greatest IoU, as expressed in (4):\nğ›¾ (cid:3020)(cid:3118) (cid:3003) ğ•(cid:3042)(cid:3029)(cid:3037)[(ğ‘¥ ğ‘¥(cid:3549))(cid:2870)+(ğ‘¦ ğ‘¦(cid:3549))(cid:2870)] (4)\n(cid:3030)(cid:3042)(cid:3042)(cid:3045)(cid:3031) (cid:3036)(cid:2880)(cid:2868) (cid:3037)(cid:2880)(cid:2868) (cid:3036)(cid:3037) (cid:3036) (cid:3114) (cid:3036) (cid:3114)\nThe next component of the loss function computes the prediction error in width and\nheight of the bounding box, similar to the preceding component. However, the scale of\nerror in the large boxes has lesser impact compared to the small boxes. The normalization\nof width and height between the range 0 and 1 indicates that their square roots increase\nMachines 2023, 11, x FOR PEER REVIEW 5 of 26\nMachines 2023, 11, x FOR PEER REVIEW 5 of 26\nFigure 3. YOLO-v1 preliminary architecture.\nMachines 2023, 11, x FOR PEER REVIEW 5 of 26\nAs alluded to earlier, the input image is split into s  s grid cells (default = 7  7), with\neach cell predicting B bounding boxes, each containing five parameters and sharing prediction probabilities of classes (C). Therefore, the parameter output would take the following form, expressed in (2):\nFigure 3. YOLO-v1 preliminary architecture.\nğ‘   ğ‘   (5 ğµ +ğ¶) (2)\nAs aClluodnseidd etori enagr ltiheer, etxhaem inpplue to ifm YaOgeL Ois snpeltiwt ionrtko ws it hs geraicdh c ceelllsl (bdoeufnaudlitn =g 7b o x7 )p, rweditihc tion\neachs ceet ltlo p 2r eadnidct einvga lBu abtoinugn dthine gb ebnocxhems, aerakc hC OcoCnOta idnaintag s efitv ceo pnasirsatminegt eorfs 8 a0n cdla sshsaesri, nthge p prea-ramFigured 3i. cYOtiLeoOtn-ev 1rp porreuolimtbpianuabryti lawirtchioeituesc tluodref . b cel agsisveesn ( aCs) .e Txphreersesfoedre i,n t h(3e) :p arameter output would take the folMAlaosc hawilnleusidn20e2gd3 , t1fo1o ,e6ar7r7mlie,r , ethxep inrpeust simeadg ei ins s (p2lit) :in to s  s grid cells (default = 7  7), with 5of25\n7 7 (5  2+80) (3)\neach cell predicting B bounding boxes, each containing five parameters and sharing prediction probabilities of classes (C). Therefore, the parameter oğ‘ utpuğ‘ t wo(u5ld tağµke+ thğ¶e f)o l- (2)\nlowing form, expreTssehde i nf u(2)n: dameAnstaalll umdeodttioveear loiefr ,YthOeiLnpOut iamnadge oisbspjelicttin dtoestecstgiroidnc eilnls g(deefnauelrta=l7 is 7t)h,e object detecwitheachcellpredictingBboundingboxes,eachcontainingfiveparametersandsharing\ntCioonn asnidde rloincgal itzhaet iğ‘ eoxnğ‘ a mv(i5pa lğµbe+o oğ¶uf) n YdOinLgO b onxeetws. oTrhke wreiftohr( 2ee), a tcwh oc eslelt sb oouf nbdouinngd binogx bporxe dviecctitoonrs are\nprediction probabilities of classes (C). Therefore, the parameter output would take the\nCsoents itdorere i2nqg ua thnired ee xdea,mv ipa.leleu. o, afv YtfeiOonclLlgotOow tnrinhe gytewf ooibrsrmke t ,nwhecixetphh r remeeascspaehd rrcekeinlsl (Ceb2o)nO:utnaCdtiOnivg ebdo oax tfpa rgesdreioctt uiconno nds tirsutitnhg a onfd 8 v0 eccltaosrs eğ‘¦s , itsh teh pea prraemd-icted\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the parameter voeucttpourt. wToo ualddd bree sgsi vmenu latsip elxep breosusneddi inng\ns\n( 3b\ns\n)o: x(5 es\nB\nc+o\nC\nn)taining no object or t\n(2\nh\n)\ne same object,\neter output would be given as expressed in (3):\nYOLO opts for non-maximum suppression (NMS). By defining a threshold value for\n7Co7nsi(d5er2in+g8t0h)e e xamp\nle\n\nof\nY\n\nOL\n(\nO\nn\n\net\nwo\n+\nrk\nw\nit )(h3 )e achcellboundingboxprediction\n(3)\nNMS, all overlaspetptoi2nagnd pervaelduaitcintgetdhe bbeonuchnmdariknCgO CbOodxaetass ewtcoitnhsis atinng Ioof8U0c llaosswes,etrh etphaaranm etthere defined NMS\nThe fundamental motive of YOouLtOpu atnwd ooubljdecbt edgeitvecetnioans ienx pgreensesreadl iisn t(h3e) :object detecvalue are eliminated.\ntion and locaTlizhateio nfu vina dboaumndienng tbaoxl ems. Tohteirvefeor eo, ftw Yo OsetLs oOf b aounnddi nogb bjoexc vte cdtoerst earcet ion in general is the object detecrequired, i.e., vectoTr yh ies t hoer riegprienseanlt aYtivOe oLf Ogro bunads terudth o annd tvheceto 7Dr ğ‘¦a 7irs k thn (e5e p tr2 efd+ rica8t0em )d ework consisted of tw(3)o sub-variants.\ntion and localization via bounding boxes. Therefore, two sets of bounding box vectors are\nvector. To address multiple bounding boxes containing no object or the same object,\nYOLOr eoqptus T ifro h re e dn o, fi n i- r .m s ea t .x , a ivm r eu c mc h t i os t u e rp c py t r u eTiss rhsi e eot fnhuc ne o (Nd marMmeS pep)n. r rti Bae slysme ed doent fiivo tneai f not gf2 iY v 4 aOe tLc hoOo ref nas nhg vdor o lodobl ujuveca t ntl i udd o ee nt fetoca rrtuil o nt la hiny ageen rnsedr a w vlise ittchh teo otbr hj eecğ‘¦t  fi dei ntse acttliho lne a y p e r r e d p i r c o te v d id ing\nNMS,\nv\na\ne\nll\nc\no\nt\nv\no\near\nr\nlac\n.\np op\nT\nnin\no\nng ep\na\ncre\nd\ntdi\nd\nioct\nr\nne\ne\nd\ns\ni bn\ns\no a tun o\nm\nndd ti\nu\nlnh og\nl\nc e\nt\nab\ni\nloi fi\np\nzxa r\nl\net\ne\nss i o wt n\nb\niotvh\no\nf i aa\nu\ntnb h\nn\nIo eo\nd\nuU n t\ni\ndlw\nn\noinw\ng\no ge rb\nb\nf to uh\no\nxal e\nx\nnl s y\ne\n.th\ns\nTec h od\nc\neen r\no\nfie n fn\nn\noee r\nt\nde\na\nc ,N\ni\nt t\nn\nw eMd\ni\noS\nn\ns l\ng\ne a t s y\nn\no e f\no\nr b s o\no\n. u W n\nb\nd\nje\ni h n\nc\ng e.t b r o e\no\nx a\nr\nv s e\nt\nc t\nh\nt h o\ne\nr e s \ns\na F r\na\ne a\nm\nst\ne\nY\no\nO\nb\nL\nje\nO\nct\n\n,\nvarrequired,i.e.,vectoryistherepresentativeofgroundtruthandvectoryisthepredicted\nvalue are eliminated.\niant consisted ovfe cotonr.lTyo andidnreess cmounltivploelbuoutniodinngabl olxaesyceornsta ihniongstnionogbj efcetowrtehre sfiamlteeorbsje ceta,YcOhL.O Inspired by the\nTYheO oLrigOin alo YpOtLsO fboasre dn oon nth-em Daarxkniemt furamme wsourkp cponrseisstesdi oofn tw (oN suMb-vSar)i.a nBts.y defining a threshold value for\noptsfornon-maximumsuppression(NMS).BydefiningathresholdvalueforNMS,all\nThe first arcihniteccetupret icoomnp rmiseod dofu 2l4e c oinnvo lGutioonoagl llaeyNerse wt,it ha t hse efiqnaul leanyecr ep roovfid i1ng 1 convolutional layers was impleNMS, all overlappinovge rplarpepdingicptreeddi cbteodubonudndiningg bbooxexsewsit hwainthIo Uanlo wIoerUth alnotwheedre fithneadnN tMhSev dalueefined NMS\na connection into the first of the two fully connected layers. Whereas the Fast YOLO varmented for redaureceilnimgin tahtede. resultant feature space from the preceding layers. The prelimiiant covnasilsuteed oafr oen ley lniimne icnonavtoeludti.o nal layers hosting fewer filters each. Inspired by the\nTheoriginalYOLObasedontheDarknetframeworkconsistedoftwosub-variants.\ninception mnodaurlye ian rGcohoigtleeNcettu, ar ese fqouern cYe OofL 1O-1v c1o nivso lputrioensael nlatyeerds winas Fimigpule-re 3.\nThe original YTOheLfiOrs tbarachsietedct uorenc otmhper iDsedaorfk2n4ceotn vforlaumtioneawllaoyrekrs wcoithntshiesfitneadll aoyefr tpwrovoi dsinugb-variants.\nmented for reducing the resultant feature space from the preceding layers. The prelimiTo addressa ctohnene icstisonuein tooft hme fiurslttoipf tlhee btwooufunlldyicnongn ebctoedxelasy efros.rW thheere sasatmheeF oasbtjYeOcLtO or with a confinary aTrchheite cfiturrset f oar rYcOhLiOte-vc1t ius prree scenotemd ipn rFiigsuerde 3 o. f 24 convolutional layers with the final layer providing\nvariant consisted of only nine convolutional layers hosting fewer filters each. Inspired\nTo adddreessn thcee i sssuceo orfe m ouflt izpeler boo,u ni.dein.,g nboox eos bfojre tchte, stahmee oabujetcth oor rwsit dh ae ccoindfie-d to greatly penalize predictions from\na connection into thbey fithresitn coepf ttiohnem towduole fiunlGlyoo cgloeNnent,eactseeqdue lnacye eofrs1. W1 chonevroeluatsio tnhalel ayFerasswt aYsOLO vardence score of zero, i.e., no object, the authors decided to greatly penalize predictions from\nbounding boxeism cpolenmteantiendifnorgr eodbucjeincgtsth e(ğ›¾resultant=fea5tu)r easnpdac ethfroem lothwe persetc epdienngalalyiezras.tiTohne for prediction\nboundiainng tb ocxoesn csoinstatienidng o ofb joecntsl (yğ›¾\nğ‘ ğ‘œ\nn\nğ‘œğ‘Ÿ\ni\nğ‘‘\nn=e5 c) oanndv thoe lluowtieost npeanla llğ‘iazğ‘œağ‘œytğ‘Ÿieoğ‘‘rn sfo hr porsedtiicntigon fewer filters each. Inspired by the\npreliminaryarchitectureforYOLO-v1ispresentedinFigure3.\ncontai i n n in c g e n p coo t o i nb o jte n act i ( m nğ›¾ ğ‘›iğ‘œn oğ‘œğ‘d gğ‘— u =n l 0o e .5 )o i . n bTh jee GT ca o u o t a t o h( d ğ›¾o dg r ğ‘›r s le ğ‘œe c sğ‘œ a sN l ğ‘t c hğ‘— u ee l t a= i, t s e s d ua 0 e t.h o 5 s e fe )l m .o q s u T u s l t fh ei u pn en le c c ta b i e o o u n u t on bh d y f o i t n a 1 r g ksi b n o cg x a 1 t e hl s ce f co u ro l t a nh t ev e soa d ml u t e h toi e boj e l nc o tao s lr s w l f ai u thy n ea c rc t os i n o fiw n - a b s y i t m ak p i l n e- g the\nsum of all bounding box parameters (x, y, width, height, confidence score, and class probsum of all bounddenicnegsc obreooxfz peroa,ri.ae.m,noeotbejrecst ,(thxe, ayu,th worsiddtehci,d ehdetiogghreta, tlcyopnenfialdizeenprcedei csticonosrfero,m and class probabilitym). Aesn at eredsu lfto, trh er fierdst upacritn ogf t hteh eequ raetiosnu clotmanputt efse tahet ulorsse o sf pthae cboeu nfdrionmg b otxh e preceding layers. The prelimiboundingboxescontainingobjects(Î³ =5)andthelowestpenalizationforprediction\ncoord\np ğ‘¥\nğ‘\nr\nğ‘’\ne\nğ‘›\nd\nğ‘¡ğ‘’\nic\nğ‘Ÿ\nn t , i a o ğ‘¦ n\nğ‘\nr\nğ‘’\ny\nğ‘›\nw\nğ‘¡\na\nğ‘’\na i\nğ‘Ÿ\nbt . h r i ğ• c l\nğ‘–\nğ‘œ r\nğ‘—\ni ğ‘ h et ğ‘— s y i p i t s e) e s c. e t c A t t t a o u s s r t 1 ha e e i n f rg o t er h c\nt\no r s\nh\no e u\ne\nu nY c n t\ns\na ld a\nu\ns O t i e n\nm\n,t r i o Ln tu\no\nf h g t O\nf\nh th e n\na\n- e\nl\nb o\nl\nv fio o\nb\no u 1 b r\no\nb n j s\nu\nj e e d i c\nn\nt cs i t n t\nd\np r g p\ni\n( e\nn\nÎ³ a s r\ng\nb n id r oe o\nb\no i tx bs n\no\nj e g\nx\nob = n a\np\nw fs\na\nt0 e i t\nr\nt e d . h h\na\n5 d\nm\ni ) e n o . n\ne\niT ğ‘—\nt\ne ğ‘¡ n\ne\nh â„ tq\nr\nh e\ns\nb e F u a o\n(x\niu u ca g\n,\no t n h t\ny\no d u i\n,\no r i od r\nw\nn rs in g e\ni\nn\nd\nc a b\nt\na3\nh\ntc o l e\n,\nc. o x s u\nh\nm\ne\nla\nig\nte\nh\np d\nt,\nu t\nc\nh\no\nt e e\nnfi\ns lo\nd\ns t\ne\ns h\nn\nf\nc\nu e\ne\nn\ns\nc l\nc\no t\no\nio s\nr\nn\ne\ns\n,\nb\na\no y\nn\nf\nd\nt a t\nc\nk\nl\nh i\na\nn\ns\ne g\ns\nbounding box\nprediction in\np\nT\nr ğ‘–oğ‘¡eâ„ d\nace\ni\ndl\nc\nl;d\nt\no\ni\nr\no\nthe\nn\nesr ws\nw\nitseh\ni\n,\nt\nei\nh\nt ii ss\nr\nss\ne\neut\ns\nae\np\ns\ne\no0.\nc\nfT\nt\nhm e\nt o\nuse llet\nt\nci\nh\ntpe\ne\ndl ,e i\ng\n.eb\nr\n.,o\no\npu\nu\nren\nn\ndid\nd\nctie nd\nt r\ngb\nu\no bu\nt\nn\nh\nod xin\nb\neg\no\ns b\nu\nofxo\nn\nr\nd\nt\ni\nh\nn\ne\ng\ns\nb\na\no\nm\nx\ne\nb\no\na\nb\ns\nje\ne\nc\nd\nt o\no\nr\nn\nw\nth\nit\ne\nh a\nco\nc\no\no\nr\nn\nd\nfi\nin\n-\nates\nprobability).Asaresult,thefirstpartoftheequationcomputesthelossoftheboundingbox\nwould d b e e n ta cğ‘¥s e k\nğ‘\ne\nğ‘’\ns d\nğ‘›\nc w\nğ‘¡\no\nğ‘’\nit r\nğ‘Ÿ\nh e, p oğ‘¦re f\nğ‘\nd\nğ‘’\ni z c\nğ‘›\nt e i\nğ‘¡\nn r\nğ‘’\ng o\nğ‘Ÿ\na ., p n ir ğ• . o ee\nğ‘–\nğ‘œ d b\nğ‘—\nğ‘ . j i e ,c ğ‘— c t ni t oi w osn i w t os h ieb t t ht h j e erae g css r pt e ,e a 1 ct t t e ht s io t ne I t o hat U euh , g t a erh s o u e con x ar p dss r t e e r s du s to e eh d fcb i io n tdu h ( n 4 ee ) dd : ino tgbobj oegxcrbtea sareetdlsyoin dpthienencgoa olwridziinetah tpeisnrxe cdeğ‘—n ğ‘¡ tiâ„ ecr ,tbioonusn fdrionmg box\nbounpdreindgic btiooxne ğ›¾ ğ‘siğ‘œnğ‘œ cğ‘Ÿğ‘‘ o ğ‘– n ğ‘† ğ‘– ğ‘¡y = 2â„c0tea ntğµ ğ‘—cie=rne . 0 ğ• ili o ğ‘– ğ‘œ nlğ‘—j b ğ‘ ; j ğ‘— g [i(o sğ‘¥ s otğ‘– ehb tğ‘¥e a jğ‘– s er)21 cw+i t n si(ğ‘¦s t h ğ‘–(e e ğ›¾, c ğ‘¦a iğ‘– )s t 2e ]i o s f th s= e e o t5 b a j ) e c sa t r n0 es .d i d T i t n hh g ee w(4 i l )s t o h e i w n le j ec th st b et o d u p n ,e d in i . n ea g .l b ,i o zp x a p rt r ei e do di ni c c t i t o feo n dr in pbroeudnicdtiinogn box\nith cell;otherwise,itisseta ğ‘ s ğ‘œ 0 ğ‘œ . ğ‘Ÿ T ğ‘‘ heselected,i.e.,predictedboundingboxwouldbetasked\nTchoe nnetxwat icnoomuinplgdon enbnoet o tof atbhsejke loecsdts f (uwğ›¾ncittiohn pcor=mepd0uit.ec5st )tih.n eTg ph raeedn iac touiobnthj eerocrortr s wi nc wiatilhdct uht hlaanedt e gdre tahtee slto Isos Ufu, nacs teioxnp rbeys steadk iinng ( 4th):e\nwithpğ‘›rğ‘œeğ‘œdğ‘icğ‘—tinganobjectwiththegreatestIoU,asexpressedin(4):\nheight of the bounding box, similar to the preceding component. However, the scale of\nerror\ns\nin\nu\nth\nm\ne l a\no\nrg\nf\ne\na\nb\nl\no\nl\nx e\nb\ns\no\nha\nu\ns\nn\nle\nd\nsse\ni\nr\nn\ni\ng\nm p\nb\nac\no\nt\nx\nco m\np\np\na\na\nr\nr\na\ned\nm\nt ğ›¾ o\ne\nt\nt\nh\ne\ne Î³\nr\nsm\ns\n a\n(\nl\nx\nlğ‘† b\n,\n2S o2\ny\nx  e\n,\n sğµ\nw\n. BT\ni\nh\nd\ne ğ•\nt\noğ‘œn\nh\nbğ‘o j\n(cid:104),\nrğ‘— ( m [\nh\nx( a\ne\nğ‘¥ l \ni\niz\ng\nx a Ë†\nh\nt ) i\nt\no 2ğ‘¥\n,\n n +\nc\n)(\no\n2 y\nn\n+\nfi\ny(Ë†\nd\nğ‘¦)2\ne(cid:105)n\n\nc\nğ‘¦\ne\n)\ns\nc\n]\no r e, and\n(4 )\nclass prob-\n(4)\nof widatbh ialnidty h)e.ig Aht sb eatw reeens tuhel tr,a ntghee 0 fianrds 1t ipndaicrağ‘ ttğ‘œ eo ğ‘œs ğ‘Ÿtf cğ‘‘ho oat rth d tğ‘– eh=ei =0ier0 qsqu ğ‘—u=j= aa00rtei iğ‘– orjğ‘—onot sc iionğ‘–cmreiapseu ğ‘– te i s th i e ğ‘– los ğ‘– s of the bounding box\npredictionT hwe inthex tr ecsopmeTcp hto e ntnoe e x n ttc tho m oep f ogt n hreon e tu l o onft sdh s e l fto urssun ftu chn t c i t o ibo n no cuc o onm mdpuip tne u sgt t h e ebs po r t exh d i e cbt i paonsree erdd ro i r coi t nni w o i ntdh th eer a r ncd ooro irnd winiadttehs and\nheightoftheboundingbox,similartotheprecedingcomponent. However,thescaleof\nğ‘¥ he,i gğ‘¦ht of t.h ğ•e ğ‘œ ğ‘ b ğ‘— e oriurso nrsidnetithn eaglsa rb1geo ibxno,x etsshihmea siclleaasssree rt iomo fpt ahtchtece op morpbearcjeeedcdtto irtnhegess imcdoailmlnbgopx eowsn.iTtehhneitnn.o rHmğ‘—ğ‘¡aâ„ol iwzbatoeiovunenrd, itnhge bsocaxl e of\nğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ ğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ ğ‘–ğ‘—\npredeircrtoiorn i nin t hğ‘–eğ‘¡ â„la crge o le f l w ;b i o d ot t x h he a e n sr d wh h a e i i ss g e h l t ,e b si e ts t w eis e r e i n sme th tp e a r a a sc n g t0 e c. 0 oT a m n h d ep 1 as in re d el i e c d a c t tt e eo st d h t a ,h t ie t . h es e . i m, r s p q a u rl a el r db er io o cx o t t ee s sd in . c Tb re oh as ue e nndoirnmga bliozxa tion\nthe differences for smaller values to a higher degree compared to that of larger values,\nwouoldf wbei dtathsk aendd w hieetxhipgr ephssrte edbdaesitc(w5t)i:enegn a tnh eo brjaencgt ew 0it han tdhe 1 g irnedaitceastte Iso Uth,a at st hexeiprr seqssueadr ei nr o(4o)t:s increase\nğ›¾ Î³ ğ‘†2S2 ğµB ğ•oğ‘œbğ‘j (cid:34) ğ‘—(cid:16) [(  ğ‘¥w  (cid:112) ğ‘¥w)Ë† 2 (cid:17)2 ++( (cid:18) ğ‘¦ (cid:112) h ğ‘¦ (cid:113) )2 hË†] (cid:19) 2 (cid:35) (5) (4)\nğ‘ğ‘œğ‘œğ‘Ÿcğ‘‘oord ğ‘–=i=00 ğ‘—=j=00 iğ‘–jğ‘— ğ‘–i ğ‘– i ğ‘– i ğ‘– i\nThe next componeNnextt ,othf ethloses loofsthse fcuonnficdteinocens ccooremispcoumtpeust etdhbea spedreondwichteitohenr tehreroobrje citnis width and\npresentorabsentwithrespecttotheboundingbox. Penalizationoftheobjectconfidence\nheight of the bounding box, similar to the preceding component. However, the scale of\nerrorisonlyexecutedbythelossfunctionifthatpredictorwasresponsiblefortheground\nerror in the large boxes has lesser impact compared to the small boxes. The normalization\nof width and height between the range 0 and 1 indicates that their square roots increase\nMachines 2023, 11, x FOR PEMEaRch RinEeVs I2E0W23 , 11, x FOR PEER REVIEW 5 of 26 5 of 26\nMachines 2023, 11, x FOR PEER REVIEW 5 of 26\nMachines 2023, 11, x FOR PEER REVIEW 5 of 26\nMachines 2023, 11, x FOR PEER REVIEW 5 of 26\nMachines 2023, 11, x FOR PEER REVIEW 5 of 26\nFigure 3. YOLO-v1 preliFmiginuarery 3 a. rYcOhiLteOc-tvu1r ep. reliminary architecture.\nAs alluded to earlier, tAhse ainllpuudte idm toag eea risli sepr,l itth ien itnop su t sim graigde c iesl lssp (ldite ifnatuol ts = 7s g r7i)d, wceiltlhs (default = 7  7), with\nFigure 3. YOLO-v1 preliminary architecture.\neach cell predicting Be baochu ncdeliln pg rbeodxicetsi,n ega Bch b coounntdaiinnign gb ofixvees ,p eaarcahm ceotnertas iannindg s fihvaeri npga rparme-eters and sharing preFigure 3. YOFLigOu-vre1 p3r. eYliOmLinOar-yv a1r cphrietelicmtuirne.a ry architecture.\ndiction probabilities odfi cctliaosns eps r(oCb)a. bTilhiteireesf oorfe c, ltahses epsa (rCa)m. Tetheerr oefuotrpeu, tt hweo pualdra tmaketee rth oeu ftoplu-t would take the folAs alluded to earlier, the input image is split into s  s grid cells (default = 7  7), with\nlowing form, expresselodw inin (g2 )f:o rm, expressed in (2):\nAse aalclhu dceeldl ptor eedaircliteinr,g t hBe b ionupnudt iinmga gbeo xise ss,p eliatc ihn tcoo sn ta sin ginrigd ficevlels p (adreafmaueltte =rs 7 a n 7d) ,s whaitrhin g preAs alluded to earlier, the input image is split into s  s grid cells (default = 7  7), with\neach cedlli cptrioedni cptrinobga Bb ibliotiuens doifn gcl absosxeess (, Ce)a.c Thh ceornetfaoirnei,n tgh efi vpea rpaamraemteert eorust panudt wshoaurlidn gt apkree -the foleach cell predicting B boundinğ‘ g bğ‘ ox(e5s, eğµac+hğ¶ c)o n tainğ‘ ingğ‘  fiv(5e pğµar+amğ¶)e t ers and sh(2a)r ing pre- (2)\ndictionl opwroibnagb fiolirtmies, eoxfp crleasssseeds i(nC )(.2 )T: herefore, the parameter output would take the folFigure 3. YOLO-v1 preliminary architecture.\ndiction probabilities of classes (C). Therefore, the parameter output would take the following form, expressed in (2):\nConsidering the examCpolen soifd YerOinLgO t hnee tewxaomrkp wlei tohf eYaOchL Oce nlle btwouonrkd iwngit hb oexa cphr ecdeilcl tbioonu nding box prediction\nğ‘ ğ‘ (5ğµ+ğ¶) (2)\nlowing form, expressed in (2):\nAs alluded to earlier, the input image is split into s  s grid cells (default = 7  7), with\nset to 2 and evaluatingse tth teo b2e anncdhğ‘ m evağ‘ arklu( 5CatOiğµnC+gO ğ¶th )d e a btaesnecth cmonasriks tCinOgC oOf 8d0a ctalassest(e 2c)so , nthsies tpinarga omf -80 classes, the paramConsidering the exaemapchle c oefll YpOreLdOic tnientgw Bo rbko wunitdhi nega cbho cxeelsl, beoauchn dcionngt abinoxin pgr fiedviec tpioarna meters and sharing preeter output would be egtievre onu atsp euxt pwreosusledd b ien g(3iv)ğ‘ :e n ağ‘ s e(x5preğµs+seğ¶d) i n (3): (2)\nCosnest itdoe 2ri nangd t heev aelxuaamtipngle tohdfe i YcbtOeionLncOh p mnreoatbrwka boCirlOkit CiweOsit hodf ae ctaalcashse stc ecesol ln( Cbso)is.u tTinnhdgei nroegff o8br0oe xc, l taphsrese edpsi,ac trthiaoemn pe aterra mou-tput would take the folset to 2e atnerd o euvtapluuta twinogu ltdh eb be egnicvhleomnw aaisrn keg x CfpoOrreCmsOs, ee ddx apitnrae s(s3es)te: cdo inns i(s2t)i:n g of 80 classes, the paramConsidering the exampl7e of7 YO(L5O 2n+et8w0o) r k w7ith7 eac(h5 ce2ll+ b8o0u)n ding box (p3)r ediction (3)\neter output would be given as expressed in (3):\nset to 2 and evaluating the ben7ch7ma(r5k 2C+O8C0)O datasğ‘ et cğ‘ on(s5istğµin+gğ¶ o) f 80 cla(3s)s es, the param- (2)\nThe fundamental motTivhee offu YndOaLmOe anntadl ombojeticvt ed oeft eYcOtioLnO i na ngde noebrjaelc ti sd tehtee cotbiojenc tin d geteence-ral is the object deteceter output would be gi7ven7 as( 5ex2p+re8s0s)e d in (3): (3)\ntion and lTohcae lfiuznadtiaomne vntiitoaal n bm oaountindvd el Coioncofg anY lsbOiizdoLaexOtreii onsa.ngn T d tvh hoieaeb r ejbeexcofatomu drnepetdl,ee tic wnotifgoo Y nb s OioenLxt gsOe eso nn.f eeT brthawole uoirsrne ktdfh oweirn ioetghb, jtbeewacotcxo hd vseceteeetlcclst - booorfus b naodruien ngd bionxg pbroedxi cvteiocnto rs are\nrequ T i h r tei e o f d nu , n a i nd .e da . m , l o v ec e na c tlai t zl o am r t i y oo r t ni ei v sq ve i tu ao hi fb er s Ye e o r t d uO e tn ,p oL d i O r. 2i ee n a . a s g , nn e v bdd ne o t eo c x a vb t e t a o js i el v . r u cT e ta y h dt o ie i en fs rt ge 7 e gt fc h t r o  th o r e ieoe 7u n r ,b ne t eiw pd nn ( o r c 5 g te h r se s me u n e t 2 e t sa nh r r + oa t k a fl a 8 t Cbi n s 0i oO v d) tu e h C n v e o Od e o f i c b nd tg jga o e r t c ro bat os u dğ‘¦ ex net vt di ce s eo c c t n t -t rh so u ir e s t s t h p ian r ra ge en d o d f i c 8 vt 0 ee c dc l t a o s r se s ğ‘¦ ,  t (i h 3s e ) t p h a e r a p m re - dicted\ntion andre lqouciarleizda, tii.oe.n, vveiac tboor uyn ids itenhtgee r br ooepuxertpes.su eTtn hwteaorteuivfloedr oeb,fe tg wgrioovu esnnedt as stor efu xbtphor uaennssddei dnveg icn bt oo(3rx ) :ğ‘¦v  eicst othrse aprree dicted\nvector. To address mvuelcttioprle. Tboo uandddirnegs sb moxuelst ipcolen tbaoinuinndgi nngo boobxjeecst coorn ttahien isnagm ne oo bobjejcetc,t or the same object,\nrequirevde, cit.eo.r,. vTeToct hoaerd ydfu risen stdsh aem mrueepltnripetsaleel n mbtaootuitvniedv ieonf og g fr boYouOxneLds Otcro uantnhta dain noidnb gvje encctot o droe bğ‘¦tjee cicstt itoohrne tpihnree dgsieacnmteeedr aolb ijesc tth, e object detecYOLO opts for non-mYOaxLiOm uompt ss ufporp rneossni-omn a(xNimMuSm). Bsuyp dper7efisns7iinogn(5 a(N 2tMh+r8Se0s)).h oBlyd dveafilunein fgo ra threshold v(a3)l ue for\nvector. YTOoL aOd dorpetsss fmoru lntoipnl-em baoxuimnduimng sbuopxperse scsoionnta i(nNinMgS )n. oB yo bdjeecfit noinr gt hae tsharmeseh oolbdje vcta,l ue for\ntion and localization via bounding boxes. Therefore, two sets of bounding box vectors are\nNMS, all overlapping NprMedSi, catleld o vbeorulanpdpiningg b porxeeds iwctietdh banou IonUdi nlogw beorx tehsa wn itthhe a dne IfionUe dlo NwMerS t han the defined NMS\nYOLO NopMtsS ,f aolrl onvoenr-lmapapxiinmgu pmre dsuicptepdr ebsosuionnd i(nNgM bSo)x.e Bs yw idthe fianni nIogU a l otwhreers thhoalnd tvhael dueefi fnoerd NMS\nrequired, i.e., vector y i\nT\ns\nh\nt\ne\nh\nf\ne\nu n\nre\nd\np\nam\nre\ne\ns\nn\ne\nt\nn\nal\nt a\nm\nti\no\nv\nti\ne\nv e\no f\no f\ng\nY\nro\nO\nu\nL\nn\nO\nd\na n\ntr\nd\nu\no\nth\nbj e\na\nc\nn\nt\nd\nde\nv\nte\ne\nc\nc\nt\nt\nio\no\nn\nr\niğ‘¦n g\ni\ne\ns\nn\nt\ne\nh\nra\ne\nl\np\nis\nr\nt\ne\nh\nd\ne\ni\no\nct\nb\ne\nje\nd\nc t detecNvMalSu, eav lala rolueve ee ralliramep epilniinmagti enpdaret.e dvdiac. lteude t bi a oor nue n a edn lid inm glo ibc n aoa lxi t zeesa d twi . o inth v aian bIooUun lodwinegr bthoaxnes t.h Teh deerefifnoerde, NtwMoS s ets of bounding box vectors are\nvector. To address multiple bounding boxes containing no object or the same object,\nvalue aTrhe ee lioTmrhiiegn aiontreiagdli. n YaOl YLOOL ObaT r bs e hae q sed u e do i o r r e oni d ng , t i th i nh .e eae . , l D D v Y e aaO c rr t kk o Lnn r Oe e y t t f i b r s faa r t msa h em e ed w r e e oow p rn r ko e t s rch e ko n e nc t s a Doi t sn i at v esr e dik s o nto f e ef g dtt r w o for u ofa n smt d uw be t o r -w u v sa t o h urri ba a kn n - cvt d soa . v nr e is c ai t ns o tt r es d.ğ‘¦  o is f t t h w e o pr s e u di b c - te v d a riants.\nYOLO opts for non-maximum suppression (NMS). By defining a threshold value for\nTheT fihT reh s ot e r a fiigr ric snt ha a ilt rYe ch cOt itLu eOr c e t ub Tc rae ohs em c edo p fi mo vrrnp eis sc r tti et h s oadee rr d . Dc o hT o fa o f ir2 tk 2 e4a n4 c d ect c d tuo o r fn rnr ee va sv m s o co l oe ml u uwm t u i t ooilpo n t r irka npi l slac e l eol a dn b y las e ooiy r u ss ften ew r2d ds4i i n to w h cg f o i ttt h bnwho e vo x fi to e sh n ls uue a b c l tfi- io lvo a nn ay ntar a e aila i r ln ln p ailt na r yso gy.e v ern i r d ops i n rwoo g bv ijteihdct i tnhogre tfihne asla lmaye eorb pjercot,v iding\nT a h e c o fi n rs n at e ca c orN t nc i hn o Mie n tceS tc i i, n to ua t nrl o lei n o t c h tvoo e me t rh fia ple r ar c s ifip o s t rep n s o dit nYf noo eOt gff hc L t2 t p e h iO4 o er t ce wn t oodw pn oi i n ovt c sf t otf uo eulfu l d o l tl ltry y h ib o en c c ono oofi uan nn l r nn -l snm dae te cy i ao t c nex e ft rgd i e smt dh lbwa u e oy l ima txe t h yw r e s st e s.huo r Wweps f .pfi u h i W t rne l h el rasy h ela s a ie l c n oas ro yn t e I n he o a r( e n U sN p e  F t Mr c l h aoo t s See vwt ) d i .dY F e Bi l Or a n ay s Lgt y h t O ed a Yre n sfiO v . n a t LW rh in - O e gh  d e av e r ae fi thar n -rs e e ts d hh e N o ld M F av S sa tl uYeO foLrO  vara conneicatniotv ncao ilnnusteois attehrdee fioefrl siomtn olifyn t nhaietne\nN\ntedw\nM\nc.o o\nS\nnf\n,\nuv\na\nlol\nl\ny\nl\nl u\no\nct\nv\noio\ne\nnn\nr\nn\nl\na\na\nel\np\nc lt\np\naey\ni\nd\nn\ne\ng\nlras\np\ny h\nr\ne\ne\nor\nd\nss.t\ni\ni\nc\nWn\nte\ngh\nd\nfe e\nb\nrwe\no\na\nu\nesr\nn\ntfi\nd\nhl\ni\net\nn\ne\ng\nrF s\nb\na es\no\nta\nx\ncY\ne\nh\ns\nO.\nw\nLInO\nit\ns\nh\np vi\na\nra\nn\ner dIo\nb\nU\ny\nl\nt\no\nh\nw\ne\ner than the defined NMS\niant consisted of only inainnte c coonnsvisoteludt ioofn oanl llya yneirnse h coosntvinoglu fteiwonear lfi llatyeersr se hacohst. iInngs pfeirweder b fiyl ttehres each. Inspired by the\niant coninsicsetpedti oonfT om hne oly d o nu ril ine g ei i n nco a Gnl vo Yo v o Ol a gu l l Lu teiN eOo n a e a rb t, e l a la esa ley i s mdeerq i son u h an e t on etsc dhtei . en o gD f fea 1wr  kenr1 efic tlot ef n rrv as o melau ecthw io. n oIna rsl k p lia cry oeedn r s sbi ys w tte ahs de i o m f p t l w e- o sub-variants.\ninception module in iGncoeopgtlieoNn emt, oad suelqe uienn Gceo oogf le1Ne1t, ac osnevqouleunticoen aolf l1ayer1s cwoansv oimluptiloen- al layers was impleinceptiomne nm T t h eod e d uf fi ole r r s ri t ne a d G r u c co h ion i g t gl e et c Nh t e u e t r r,e e as c u s oT lte mh aqn eu p t oe r fnr e iic sg ae e t i u nd oa r fe lo Yf 1s p O2 a 4L c1 O e c o cf b ro n o an v m sveo o dl tl u h uo e t tni ipo o tnr nh eaea cl l e D ld l aaa iyr n y ek g e rn s r l e a stwy wf ear r as i s mt .i h mT e wt hp h eloe e r p- kfi r e cn l o i a m nls i li - astyeedr o pf rtwovoi dsuinb-gv ariants.\nmented for reducing mtheen rteesdu lftoarn rte fdeuatcuinreg stphae cree sfruoltman tth fee aptruercee dspinagc el afyroerms. tThhee p prerecleidmini-g layers. The prelimimentedn aforry raerdchuictiencgtu trhee f orers YuOltaLnOt -fve1a tius rper espseanctee dfr oinm F tihgeu rper 3e.c eding layers. The prelimiThe first architecture comprised of 24 convolutional layers with the final layer providing\na connection into the first of the two fully connected layers. Whereas the Fast YOLO varnnaaryr ya racrh M ci a thc e h Tii c ntt e oes u 2car 0t2 ed 3u , d f 1ro 1 re, r e x sY Ffs OoO R tr h P L EYenO E ROai- s R vrsLEy1u VO I ei Ea as W-or cpv cf o r1hm ne isi n tusee e l nt c cpitt tper i u odele n rs ie ebni n no fFt out oierng d t duY h ire inOen fi g 3L r F.b s Oio t g x- o uve f sr1 t h e fio e s3 r t . p w thr o ee f ss u ea l nm ly te e c do o b n ij n ne e c c tF t oi e gr d uw l r a ie y th e 3 r a. s . c W on h fi e - 6 r e of a 2 s 6 the Fast YOLO variant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the\nToTd oaed nadcdere dsscrsoe trshes eo tfihs zseeu rieos ,os ifu. eme., uonTl i oft ao i nmop bat l euj cde olbcdtn toir, spu et i h s nlset edse aibdtnuh og o teuf h b no oiosn rdxss l ei yudns ee ngfc i oo n ibrdfe o et mh c dxo e etun ossvla tgfo miorpl e u erla t eottio l hbybn je eop a csluet an l na oma ydrl e iweiz rn e s iog tp h hb r ob jaeesod tcc ixito n ce n g tosifi or ffe - nwow sr fie rttrhoh fi m ealt secraosm neafiec -oh.b Ijnescpt iorerd w byit hth ea confid d e e n n ce c s e bc s oo c ur o i enn r odc e fie nz o pge f r t b z o ioo e , n ix r .ee o s. m , , c i n . oo o ed nd . o , t e uab nn ij l o ne c e ci e to n i t h i n , s g be n t c c h jd e o eoi e G p b ffcr a et j te oe ir u ,oe c o otn t n t h h c gs f e o e l( msz eğ›¾r fea s ğ‘ N oo ğ‘œrur d dğ‘œ o e st e ğ‘Ÿum,hğ‘‘ t c l , ia i eo . =d le l a re e i.rs n 5 , d s v ) nd a e t G ao loe q u n o c e g u o d osi r db e g t et o n lj ha e eea te d c Nc l h e y l ttie o ,g p o t wo ht, e h eg fen ra e r s a d 1 e t sae l i eag p u z  rqt ee etlu n e yh 1 p e a c o r onp lei m c rc z e d s o ep a n i a n d tc ora it e vo e i fld o n i o c z n1t i l o f de s uo t e f h r p t r i a1d po r o t e rm o n tc e fdoo ad l ian l g i cr c vgrt lt eoie ia ro o l a y uv n nta e t lsli r uyo s ef nsrp ,ao w elm n a laa s yl ie i z m res p pw l r e ae - sd iimctpiolen-s from\nb b o o u u nd n i d cno i g n n bmt g ao ie b xnne o inst x egc e od s nn o c ft ao o oir n bn ji t ren a ec b g i dt no o(u ui ğ›¾b n c ğ‘›n j ğ‘œg ieen dğ‘œmc x ğ‘o tg i p ğ‘— se n r b ne=(t gj sğ›¾ t h e se ğ‘ e0 bc e ğ‘œdd. ğ‘œt o 5 a sğ‘Ÿ r f ) x s ğ‘‘ e o . ( ( e s rğ›¾ 5T= s )u:r h e 5l c et d) o a a ua n nunc t tt i= d a h n f i o gte n5 hr a i t se )n h t cu a le g ao n r lwr o ce d e u e b s slsu tj apt eh l t tp c ae e a t edc n sl ne o t ta ( h w fğ›¾ fle eriza e olat o s m ut t sir os p e ntf e h =s u fp n enoa a5 crp c t l) pi e i ro za r e f ne an r cd o t b d e i im yc o d t t n iit h tona h ef nkg e o i l n ro p l ga w r p y e t r c h e e e e s er d d t s i i . p n c T te g i h no la e an y l p i e z r r a s e . t l i T i o m h n e i - f p o r r e l p im re i- diction\nğ‘ğ‘œğ‘œğ‘Ÿğ‘‘ ğ‘ğ‘œğ‘œğ‘Ÿğ‘‘\nc s c u o o n m n ta t o i a n f s a i i a n un bl m li i g l n n b i n t g o oa y o u fr ) n . yo n a A o bl d T la j si e o o br n c a c b o g t a hu j r ( e d be inğ›¾ c t os d ğ‘› de t ux ğ‘œ i r c ğ‘œ l n ( e pt t c ğ‘ğ›¾ g , u s ğ‘— a o ğ‘› t s r rb= h n ğ‘œ a eo ğ‘œ t e t m h ğ‘ 0x a f fiğ‘— . o e n e i 5p r n t ar = ) s a e i . i r t s r rY n y T s s a p 0 g O T u hm a a( .5 ox e r e r n Le c , t ) a a t h o. o O y e o u d i , T f r f t t o - d s w e hv h m t b r c h (o ie 1 e t d x j e r u u s e t ,s i a s h r c l e ys e t u , c t t q , i a ğ›¾ h p h p f t u w ( ğ‘ l o eeğ‘œ hğ›¾ rc ağ‘œ l i i r ğ‘Ÿ eu g e ğ‘› o d i tğ‘‘ s ih Ys ğ‘œ t l r  o sb ah t e ğ‘œ s O ğ‘† ğ‘– u n,= t, 2 ğ‘ o n e 0 e c h c L ğ‘— u  c dt o e a o O o e ğµ ğ‘— = i n n= l tg f m d 0 c - h fi d h ğ• v m u ğ‘– ğ‘œ0 e pd t ğ‘— i ğ‘ i 1 , ğ‘— n n l. u ue l 5a [ co i n ( lg t s o ) F t t  s ec i .e n p s ğ‘¤ s p i b e dT g ğ‘– fi r f l t o  s e u e u h d h c t x s  n e e bo r he e e ğ‘¤ cne or n e lğ‘– s t a e ) o c u t i 2 3 , eo u e s l fn + o . sa d o n t s d ( n sh r c o  i i b s d o n n f o â„ t y r ğ‘– gh f t r c F e  u h t s l e , i a ba  e g n ak c os â„ s u  n b c s a i ğ‘– xa ) n r t o d e l 2 p m e i g c ] u s o r c u 3 no t fe n l . h o a l db a e ro s - b i t s n bt ye h g p j d ee t r b c a o s t o t b k a h x - mo ie n r e lg ( o 5 wo ) st bi hs jte e h f c u ta n o cr c o t w i n o ifit n h- b a y c o t n a fi k - ing the\nsum of all bounding bsouxm p oafr aamll ebtoeursn (dxi,n yg, bwoidxt hp,a hraeimghett,e cros n(xfi,d ye,n wceid sthco, hree,i gahntd, c colnafissd penrocbe -score, and class probability)p. Aresd aic trieosnu lwt, itthhe rfiersspt epcat rd tto eo nft c htehe s ec g oerqr o euu oan ftd izo ent r r o uc,ot i hm.e .pb , uo n utoen osd btihj n eec g t l,o b tsh ose x o afb u atths h eeo d rb so o dun en cdt i h dine e gd c o tbo oo g rxd re in a a tl t y e s p enalize predictions from\ndence score of zero, i.eN.,e xnt,o th oe blojsesc otf, tthhe eco anufidtehnocer ssc odree icsi cdoemdpu ttoed g braeseadt olyn wpheenthaerl itzhee opbjreectd isi ctions from\nparbediliicttyğ‘¥io ğ‘ ) ğ‘’ .n ğ‘› A b ğ‘¡ w ğ‘’ o s ğ‘Ÿ u i, tahğ‘¦ n ğ‘ r d ğ‘’ re ğ‘› e i s ğ‘¡ n s ğ‘’ up g ğ‘Ÿ . le t b cğ•,t oğ‘– ğ‘œ ğ‘— tğ‘ x ht ğ‘—ao e eb i s s ti fi hl s c iee rt o s tp y b n g r t a ) oe t r. ss p u a o e A nn a 1 i u n t d rs nio t i n id r n n a o a gt g b th fr s r b e ee o ut no s h b tct u xh e aw j e e l s is tbe t c e , h c t o q o t s rouu e h f ns ( n a p e t ğ›¾ td eha t c fiiiiet no n r t gooi ns n b t t hgbc j ee p = oo o cb xam bto r 5 ujrebtp ne ) c a ds ou t a s iisnd fet n g ( edi t ğ›¾ d nb sh ğ‘ og ğ‘œ o t e x t ğ‘œh n . h w ğ‘Ÿ P e ğ‘‘e e eitq nt h= l h l a u o eo lii a w 5nz s a ct ) s t oi e ğ‘—ia o o ğ‘¡o s oâ„nn n t r f d d ob p f tc oi t h t n e h o uh a n e ee m n t o a edl b bo p l sij o i we n u z c u gte a t c ne s t bot i sd n o o p fi i x t n de nh en g f n e a o c le bl r i o z o p a sx t s r i e o o d n f i f c t o h t r i e o p n r b e o d u ic n ti d on in g box\nğ‘¥predi, pcğ‘¦trieModancihci ntewi.so 2 ğ•n0i2 ğ‘œ t 3 ğ‘ hi, ğ‘— n1 1 i,rs6ğ‘–e7 ğ‘¡ 7s â„se ptc epeaclsrlt ;e 1od ttioinhe cc r eotrttorinhhowr t eiena s i sico neagnwi,lsr y neio itg e t xou hie nsfcn o utsrd hteo eee td bs tab jpore ysbuce tj0t hcte .eh(tc ğ‘ ğ›¾ lTt ğ‘œo thsğ‘œbrose ğ‘Ÿe of ğ‘‘ s uusitndehcn=liteeindo c0gnitg . nei5wfrd g) toh .i, tauTihtb. hnepionr.ed,ex d apğ‘— iğ‘¡u ctrâ„btret oahubdr soowitcehruatsdsen r cd bde aos iol pbncnuoogu n un s ltba indbhotdl eieexn id f nog crgt ho t hbbeoe o orl g oxdrxo s iu6snonb ffdaua2 5tsneecstd io no nb yt thaek incgo othred inates\np\nw\nğ‘¥\np\nğ‘ r\no\nğ‘’\nğ‘\ne\nr\nğ‘›\nğ‘’ u\nd\ne\nğ‘¡\nğ‘› l\nğ‘’\nd\ni\nd ğ‘¡\nc ğ‘Ÿ\nğ‘’ i\nt w\nc ğ‘Ÿ b\nio\nt\n,\ne\non\ni\nğ‘\no\nğ‘’ ğ‘¦\nt\nu c\ns\nğ‘›\na\ni\nn ğ‘\nl\nu\no n ğ‘¡\ns\nd\nğ‘’\nğ‘’\nk\nn\nm ğ‘›\nğ‘Ÿ\ni\nğ‘–b\ne n ğ‘¡\nt ğ‘¡\nd ğ‘’\ne a â„\no ğ‘Ÿ\ni ğ‘– t\nğ‘– w\nğ‘— . n\nf\nca\nğ‘¡\ne\nâ„\ns i\ni a\nğ•\nt\nn lk\nğ‘–\nğ‘œ\nh\nl\nl ğ‘—\ne;ğ‘\nc l\ng ğ‘—d\np\no\ne b\nn t\nl r\ni w\no\nh\nl e\ns o\n; u\nğ‘¥\np d\ne i\no\nt sr\nğ‘\no\ni n r\nhw\nc\ne\nğ‘’ te\nb\nd t ğ‘› h\nt p\ni d\ni j\nğ‘¡ n\ns\ni e\ne r a\nğ‘’ n i\ne\ng\nt\ns\ne\nr\ns\na\nc\nc ğ‘Ÿ\nr s\ne\n,\nt g\nd\nw\nu u\nr b\nt\nt\n,\nt a\ni\nu\nt\ni ğ›¾\ni 1\na\nm\ni\nt h\nn\n(\no t\ncğ‘¦\nb l i s h\nğ›¾\ni\ni\ns\nt b\nn ğ‘0 t\ni\no o\nsi\nb\nğ‘› o\ne\no\ny\nn\n,ğ‘’\nn\nb o x\nu ğ‘œ\nw\nsf\nğ‘› , ) i\ng\nu j\nn\n.\nğ‘œe\nn ğ‘¡\nt\ne h\na\ni n p\nd ğ‘t\nğ‘’ A\nh a\nt i c\nl\nd l\ni\nğ‘† ğ‘Ÿ\nğ‘— l\nas\nn\nt\nn a\ni\ne\nsğ‘–2i t n\n.b g\nr\ns\ns wğ‘¡\n=\ng\no\nğ• aâ„ a\noc\n\nb ğ•\ni n\nj\nb\nb i s o m\nu o a\nğ‘–\nğ‘œ\nrt ğµ\n.\no\noğ‘— e\nj x ğ‘\nh c e\nsne\nb x\nT .ğ‘— .\nt j e e s .\ne c 5 d\nt\nğ• h\nu w ğ•\nt\nl t\nğ‘– ğ‘œ\nh a\ni ) i\noğ‘œ\nğ‘—\nl e\nğ‘ eos\nl b\nnw\no\n.\nğ‘ e s;\nğ‘—\nt j r r ğ‘—\nfg\n,\ns T\ni k o\ns i i\ns g [ s 0\ns e\nt\nt\n( s\nte h b\nr t s\nh\nh ğ‘¥ .\nl s\n(\nh\ni e h e\net e\nn\no\ne x\ne\nt\nğ‘›\nT a\nt t ec\ne\nx\n t t\nğ‘œ a h\n,\nt t\nt o h fi\na\nh\nğ‘œ\nr\no e\ne\ns e\ny\no\ne\np\nğ‘¥\nğ‘\n1 r w\nu\n e\nd\ns\n, s o\nğ‘— bag\nw t\n)\nt ,\nt p\nw r\nw i\nr\ns2\nj h\nI h p s\niae\np\ne h\no e\ni.\ne+ o\no\ni e\nmenc a e\nn aU l ds\nn .\n,\nt r t\ne i r (\n,\nt tt\ne e\n,\ns\nhğ‘¦ t\nt\ne\nt\nci h\nrpt h s h\na e t t w\ne\no\ne t e\n,\nc r\ns e o\ner\ni f a\nse a I o\nh b\ns\nsd e y\nodi b\nt jğ‘¦\nl\n e\nc\ne , x\nd\nh\ncj (U\n,\ni\nc s\ne\nia\na\n)\nx\np\nc u\nt g e\nc i\ns ei2\n,,ts t n\ni r . h ] s s\nl e\nt\na\ne e\ny i e\ne h\nas g\nt p\nd\nq\ns\ns .\n,\no a ,\nt p\nr , s\no\nw u\nw\ne\ne e\ns\nr bw\nc e s p\ne f\na\nx\nn\nd i\ne\no\no d\ns\nd\nt\np\nn r\ni e\ni\ntu\ni nn\ntt\nt .e\nn\ni\nt h r\no\nh hn\nn i\nth e\n(fi d n T n\ne,\ni d s i e\n)\nn\n( d i t\nh n\nh\ns\n: h c4\ni\nc\noee t n\ne e o\nl\ne )\nh\nt\nid o b\n:\nggğ‘—\nn e c\ne\nm\nğ‘¡\ne s\nh s j\nd\nc â„\nc\ni\nl\neb\np e\ns\nl\nnt e\ne ;\nc, l o\nl u o\nl b\nb\nf; ( t\ne\ncx\nt s t\nu 4 o o\nh o\no\nc e c\nr ) t\ne\nn nu\nu\nh\nt s\n: e\no r e\ne\nw\nfi c ns\nn t r\nr\nd i h\nw d t i\ne s\nd\nd (\nd i\ne , e\ne i\n4 ,\no s\n,\ni\ni\nin\n) i\ne n\ni a n l\nn n\nt .\n,\no\nc\ne n i\ng\ng\ni ge\ns s\ntb\n. d s , s\ni s s b\nb\ny\ne\nw c\np o t c\no\no\no\nf\nt\nr l\ni x\nx\na r\na\nt\ne t\neh\nh\nk\ns d\n,\ne\ni\ns\ni a\ni\nn n\nc\nn\nb p\ng\nt o\ndğ‘—\ne r\nğ‘¡\nu o\nt\nd\nâ„c h\nn b\nl a\nd\ne b\nb -\ns\ni\no\no\ns\nn\nu\nu g\npn\nn\nr\nb\nod\nd o\nbi\ni x\n-n\nn\ng\ng\nb\nb\no\no\nx\nx\nability). As a resultp,r te ğ‘hd ğ‘œeğ‘œ ic ğ‘Ÿ ğ‘‘ tfiiorn ğ‘–s= t0 wpi ğ‘—at = hr0t r iğ‘–ojğ‘— efs ptehğ‘– cet etoqğ‘– uthaet iog ğ‘–nro ucon ğ‘– dm ptruutthe sb tohuen dloinsgs obfo xth bea sbeodu nond itnhge bcoooxr dinates\nwould b p e Tr t eh a de s k inc e et d xiot w nco i m t w h p w i t p o ğ›¾ h o n ğ‘ r ğ‘œ e e u ğ‘œ rn ğ‘Ÿ dl ğ‘‘ e d tğ‘¥ ia s c o s p btğ‘† ğ‘– f 0 = i 2 e ,e t n 0 w hc gt te ha , ğµ ğ‘— i = la l ss tğ‘¦o 0 tk o n s ğ• e  n iğ‘– ğ‘œ s jğ‘— ooğ‘ t dS i o =f ğ‘—2 h bb u0 [ j (w e  j n w . eğ‘¥ B j c= o ğ‘– ci ğ•g0t rtt ğ‘œ i kğ• h r ğ‘ oi os j w ğ‘— bğ‘¥ o  n i jp ğ‘– n( ui )c i sc 2rtt in h o h  e s +e mde d cot( t i p p i ) h ğ‘¦2c at p ğ‘– u e r + to s  t i u s e gn Î³i 1 t t ğ‘¦ n s e rg oh ğ‘– i o )ew tn b h 2aa j a ] be  t yn t h , S i e o p= a2 e os 0 s ur  ts e b cn hB jda I = jo d eo 0 si w c ğ• e c i U i nn tjn to io o o ,i g wbn f j n a ( ( t x 6si hbe i t) : re h o e r x x x o  o t i p ) r h b 2 bj r i e + en e a c g ( s stw c s(r ei r 4 e i e ded )da s c t i i t h ) do ie2 n ina s n n t ( g d 4 t I h o w ):U e i ( t 6 , h ) c a ion s o ğ‘— e rğ‘¡dâ„ x p ibn r oa e ut s ne s ds e i d n g i n b o (4 x ):\nğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ ğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ ğ‘–ğ‘—\nThhee nigğ‘¥ehx\nğ‘\nt\nğ‘’\nt\nğ‘›\nco\nğ‘¡\nof\nğ‘’\nm\nğ‘Ÿ\nth,p eğ‘¦o bn\nğ‘ğ‘’\noe\nğ‘›\nun\nğ‘¡\nnt\nğ‘’ ğ‘Ÿ\ndo.fi ntğ•hg\nğ‘–\nğ‘œ\nğ‘—\ne ğ‘b ğ‘—pğ›¾lo o\nğ‘\nri x\nğ‘œ\ness\nğ‘œ\n,sd\nğ‘Ÿ\ns sif\nğ‘‘\nciue mttni oiğ‘†\nğ‘–\nca l\n=\nnt2aS\ni\nsi\n0=\n2ro i\nn1nto ğµ\nğ‘—\nci ğ‘–\n=\nB\nj=\nno ğ‘¡t\nâ„h\nm ğ•e to\niğ‘–\nğ‘œc\nj\nh p\nğ‘—\nbğ‘epj(ue ğ‘—l c rl[ t\ni\ne;(ec cğ‘¥osa e c\nğ‘–\nt Ë†std\ni\nh)ğ›¾he2ie\nğ‘\nen +r\nğ‘œ\nğ‘¥o gwp\nğ‘œ\nÎ³\nğ‘–\nf\nğ‘Ÿ\n)r\nn\ni c\nğ‘‘\n2e\no\nst o\no\nedh\nb\n+m,\nj\ni e ğ‘†\nğ‘–\nci\n=\np( 2ttS\ni\noğ‘¦\ni\n=\no2io\n0ğ‘–\nsbnn e jsğµ\nğ‘—\neen\n=\neB\nj=\ncğ‘¦rt\nt\nrt\nğ‘–\n. ağ•) oHn\niğ‘–\nğ‘œ r2s\nj\nr\nğ‘—\noğ‘ e ] oo0iğ‘— bsnjw[. (i ( xdTwğ‘¥e\ni\nhi v\nğ‘–\ninedexË† g r\ni\nts) ,hğ‘¥ 2 ew t +\nğ‘–\nlah)en2e (ic ct dt\ni\n+h se cdi a(nc , Ë†ğ‘¦\ni\nl ) ei\nğ‘–\n2. ğ‘— eğ‘¡o.â„,f ğ‘¦pb\nğ‘–\nr)oe ( 2 6ud ])( in4c )tde idn gbo buonxd ing box (4)\nheight eorfr ot\np\nhre\nr\ni\ne\nnb\nd\noth\ni\nu\nc\nen\nt i\nlda\no\nir\nn\nngg e\ni n\nbb oo\nğ‘–\nxxğ‘¡e,â„ ss ih\nc\nmwa\ne\nis\nl\nlo\nl\nal\n;\nuer s\no\nldts\nt\noe\nh\nb rt\ne\nehi m\nr\net\nw\na pps\ni\nkar\ns\ncee\ne\ntcd\n,\nec wdo\nit\nmi int\ni\nhp\ns\ng ap\ns\ncr\ne\noree\nt\nmdd\na\nptioc\ns\not tni\nnhe\n.\nge n\nT\nsta.m\nh\nn H\ne\nao olb\ns\nlw j\ne\nbe\nl\neoc\ne\nvtx\nc\neew\nt\nrs\ne\n,i. t\nd\ntThh\n,\nh et\ni\neh\n.\ns\ne\nenc\n.\noga\n,\nrlr\np\neme\nr\naoa\ne\ntfel\nd\nisz\ni\nta\nc\nIt\nt\noi\ne\noU\nd\nn, a\nb\ns\no\ne\nu\nx\nn\np\nd\nre\ni\ns\nn\ns\ng\ned\nb\ni\no\nn\nx\n( 4):\nThe last component of the loss function, similar to the normal classification loss, calThe last component of the loss function, similar to the normal classification loss,\nerror inTo htfh ewe w nilda o ert u xhgt l ea d cbn oo b dmx e he p t se a iohg s anh k ste e lnb d eest st w oweT c fr u ci eh a tl it a e lh em c h t n u e e sp n l p a t t ha t e rh l e o e c e ex s t d c s t t r h l c s ai a e co sc no s f c m t l ug ( ai m c ns pen ) s a g pc p 0 (c r r t o ) e oa ai p n b odn nr a o ned b t bo on i l a 1c i bb t t t o y i h j i l o e n i m l e t o f cy d s s ts p l ti ğ›¾ m o , ch we u s a s x ae , tt ci le e e et ll x p s h sob c t  e ts o f t p h tğ‘†o shx ht 2r a e f ef t t eo h s  u r t e .p gğµ h t nT h ğ• r r e ğ‘– ğ‘œ e ch ğ‘— e e ğ‘it ğ• ğ‘—red aoğ‘œ i b o pğ‘ sn t i jğ‘—a qc ep no [ r tu as( t r , i rğ‘¥ c t amo t e , or xI n e ae po m x r l pUğ‘¥ r e ie r z s opr e s ) a , o s e r 2 u s t da ot e i + ts o dsi re n ni (i s e n n i ( ğ‘¦ 7 n x c ( t ) 7 : r h p ) we : e r a ğ‘¦e ispd )s e 2 r s t ] he e d d ai n i c n tdi o ( 4 n ) : e rror in w ( i 4 d ) th and\nohf ewigidhtth oanf dth hee ibgohtu bnedtwinhegeen ibg tohhxet , rosa fin mtgheie l0a bra ontoud n1tdh ineind pgicr abetcoeesx d,t his ğ‘ nağ‘œ itmg ğ‘œ ğ‘Ÿt ğ‘‘hcileoaiğ‘– mr= r 0stpqoouğ‘— = tan 0 hreee iğ‘–n jğ‘— rtpo. orHetğ‘–sc oeinwdcğ‘– iernevages rec, o ğ‘– tmhep ğ‘– socnaelne to. fH owever, the scale of\nerror in the large boxeesr rhoars ilne stshTeehr leia mnrğ›¾ge ğ‘ px ğ‘œ ea ğ‘œ t ğ‘Ÿ bcc ğ‘‘ too mcxoğ‘† ğ‘– ep = 2 mso 0 nhpea ğµ ğ‘— an = ğ‘† ğ‘–i sSr == 22t 00 e 0 loğ•eğ•d ğ‘– ğ‘œ i o ğ‘– ğ‘œf ğ‘— ğ‘s jğ‘— b ğ‘ğ‘—jtst  ğ‘—hoe[ ğ‘ e c ( r  t ğ‘ ğ‘¥ l c h ğ‘™ io ğ‘ l.ğ‘–a m ğ‘  e s s ğ‘   s ğ‘’ s e ğ‘  s ps ((fğ‘¥ğ‘maup ğ‘–ğ‘– ( i c)n(ğ‘ac 2t)c)l tc+li ooğ‘b ğ‘– (nm(oğ‘¦ğ‘ )cx ğ‘– p)oe2a ms r.ğ‘¦p e ğ‘– T)ud2ht ]ete os n tthohere mp srmaedliaizclatl( i 7 (tob7)i ) noo nxere rso.r T inh (ew4 ni)d otrhm anadli zation\nheight of the bounding box, simi.lar to the preceding component. However, the scale of\nof width and height boeft wweidenth t hane dr ahnegige h0t abnedtw 1e iennd tihcaet eras nthgea t0 t haenidr s1q iunadriec artoeost sth iantc rtheaesire square roots increase\nThe next comperornorPe einrnfto trohmfea tnlahceregw elios bes,ostxh efeussi nmhacpsltei loYeOnssL ecOro (im2m4 pcpoauncvttoe lcsuo titmohnpeala plraeyrdeer dst)oiwc ththieeon sntrma einarelrdl oboron xithne es w. Tihdet hno arnmda lization\nPASCALVOCdataset(2007and2012)[39,40]achievedameanaverageprecision(referring\nheight of the bounodf iwnPgeidr fotbhrom axann, cdes iwhmeisieig,l ahthrte bstieomt wptlhee eeYn Op LtrOhe e(c2 r4ea dcnoingnvego l0 u ctaioonnmdal p1la oyinenrdse)i ncwatht.e enHs t rtoahiwnaetde t vhonee irtrh, est qhuea rsec aroleo tos fi ncrease\ntocross-classperformance)(mAP)of63.4%at45FPS,whilstFastYOLOachieved52.7%\nPASCAL VOC dataset (2007 and 2012) [39,40] achieved a mean average precision (refererror in the large bomxAePs aht aans ilmepsrseessriv iem15p5aFcPtS .cAoltmhopugahrethde ptoer ftohrmea sncme walals bbeottxeresth.a Tnhreeal -ntimoremalization\nring to cross-class performance) (mAP) of 63.4% at 45 FPS, whilst Fast YOLO achieved\ndetectors,suchasDPM-v5[41](33%mAP),itwaslowerthanthestate-of-the-art(SOTA)at\nof width and heigh52t.7 b%e mtAwPe aet ann itmhpere rssaivne g15e5 F0P Sa. nAldth o1u gihn tdhei cpaertfoersm athncae tw atsh beeittre rs tqhaun arerael- tirmoeo ts increase\nthetime,i.e.,FasterR-CNN(71%mAP).\ndetectors, such as DPM-v5 [41] (33% mAP), it was lower than the state-of-the-art (SOTA)\nThere were some clear loopholes that required attention, such as the architecture\na\nh\nt\na\nth\nv\ne\nin\nt\ng\nim\nc\ne\no\n,\nm\ni.\np\ne.\na\n,\nr\nF\na\na\nti\ns\nv\nte\ne\nr\nly\nR\nl\n-\no\nC\nw\nN\nr\nN\nec\n(\na\nl\nl\n%\nan\nm\ndh\nA\ni\nP\ng\n)\nh\n.\nerlocalizationerrorcomparedtoFasterR-CNN.\nThere were some clear loopholes that required attention, such as the architecture havAdditionally,thearchitecturestruggledtodetectcloseproximityobjectsduetothefactthat\ninega cchogmrpidarcaetlilvwelays lcoawpp reedcatoll tawnod bhoiugnhderi nlgocbaolixzaptrioopno esrarlso.r Tchoemlpooaprehdo lteos Faattsrtiebru tRe-dCtNoNth. e\nAodrdigiitnioanlaYllOyL, tOhep raorvchiditeedctiunrsep isrtartuigonglfeodr ttoh edfeotlelcotw cilnogsev parroiaxnimtsiotyf YoObjLecOts. due to the fact\nthat each grid cell was capped to two bounding box proposals. The loopholes attributed\nto2 .t2h.eY oOriLgOin-va2l /Y9O00L0O provided inspiration for the following variants of YOLO.\nYOLO-v2/9000 was introduced by Joseph Redmon in 2016 [42]. The motive was\n2t.2o. rYeOmLoOv-evo2/r9a0t00le astmitigatetheinefficienciesobservedwiththeoriginalYOLOwhile\nmaiYntOaLinOin-vg2t/h90e0i0m wpraess isnivtreosdpueceedd fbayc tJoors.eSpehv eRreadlmenohna innc e2m01e6n [t4s2w]. eTrehec lmaiomtievde twhraosu tgoh\nrethmeoivmep olre amt elenatastt imonitiogfavtea rtihoeu isnteeffichcnieinqcuieess. oBbastecrhvendo rwmiathli zthaeti oonrig[4in3a]lw YaOsLinOtr wodhuilcee mdawinit-h\ntathineiningt tehrne aimlaprrcehsisteivcetu srpeeteodi fmacptroorv. Seemveordael lecnohnavnecregmenencets, lweaedrei ncglatiomfeads ttehrrotruaginhi nthge. iTmh-is\npilnetmroednutacttiioonn oefli vmairnioatuesd tethchennieqeudesfo. rBaottchhe rnroergmualalirzizaatitoionn [4te3c] hwnaiqs uinetsr,osduuchceads wdriothp othuet i[n44-]\ntearinmael daracthrietedcutucirneg too vimerfiprttoivneg m[4o5]d.eIlt scoenffveectrigveenncees,s lecaadnibneg gtoau fgaestderb ytrathineinfagc.t Tthhaist sinimtrpo-ly\ndiuncttrioodnu ceilnimgibnaattcehdn tohrem naleiezdat ifoonr iomthperor vreedguthlaermizAatPiobny t2e%chcnoimqupeasr,e sdutcoht haes odrrigoipnoaultY O[4L4O] .\naimedT ahte reodruigciinnagl oYvOerLfiOttiwngo r[4k5e]d. Iwtsi tehffeacntivinepnuestsi mcaang bees gizaeugoefd2 2b4y  the2 2fa4ctp tihxealts sidmuprilnyg\ninthtreodtruaciinnign gbsattacghe n, owrhmilasltizfaotriotnh eimdpetreocvteiodn tphhe amseA, Pin pbuy t2i%m acgoemspcaorueldd tboe tshcea loedriguinpatlo\nY4O4L8O  . 448 pixels, enforcing the architecture to adjust to the varying image resolution,\nwhiTchhein otruigrinnadle YcrOeaLsOe twhoermkeAdP w.Tiotha adnd rinespsutth iims,atghee saiuzeth oofr s22tr4a in 2e2d4 tphiexaelrsc hdiutercintugr ethoen\n448448pixelimagesfor10epochsontheImageNet[46]dataset,providingthearchitectraining stage, whilst for the detection phase, input images could be scaled up to 448  448\nturewiththecapacitytoadjusttheinternalfilterswhendealingwithhigherresolution\npixels, enforcing the architecture to adjust to the varying image resolution, which in turn\nimages,resultinginanincreasedmAPof4%. Whilstarchitectures,suchasFastandFaster\ndecrease the mAP. To address this, the authors trained the architecture on 448  448 pixel\nR-CNNpredictcoordinatesdirectlyfromtheconvolutionalnetwork,theoriginalYOLO\nimages for 10 epochs on the ImageNet [46] dataset, providing the architecture with the\nutilizedfullyconnectedlayerstoservethispurpose. YOLO-v2replacedthefullyconnected\ncapacity to adjust the internal filters when dealing with higher resolution images, resultlayer responsible for predicting bounding boxes by adding anchor boxes for bounding\ning in an increased mAP of 4%. Whilst architectures, such as Fast and Faster R-CNN preboxpredictions. Anchorboxes[47]areessentiallyalistofpredefineddimensions(boxes)\ndict coordinates directly from the convolutional network, the original YOLO utilized fully\naimedatbestmatchingtheobjectsofinterest. Ratherthanmanualdeterminationofbest-fit\nanchorboxes,theauthorsutilizedk-meansclustering[48]onthetrainingsetbounding\nboxes,inclusiveofthegroundtruthboundingboxes,groupingsimilarshapesandplotting\naverageIoUwithrespecttotheclosestcentroidasshowninFigure4.YOLO-v2wastrained\non different architectures, namely, VGG-16 and GoogleNet, in addition to the authors\nproposingtheDarknet-19[49]architectureduetocharacteristics,suchasreducedprocessingrequirements,i.e.,5.58FLOPscomparedto30.69FLOPsand8.52FLOPsonVGG-16\nandGoogleNet,respectively. Intermsofperformance,YOLO-v2provided76.8mAPat\n67FPSand78.6mAPat40FPS.Theresultsdemonstratedthearchitecturessuperiority\noverSOTAarchitecturesofthattime,suchasSSDandFasterR-CNN.YOLO-9000utilized\nMachines 2023, 11, x FOR PEER REVIEW 7 of 26\nconnected layers to serve this purpose. YOLO-v2 replaced the fully connected layer responsible for predicting bounding boxes by adding anchor boxes for bounding box predictions. Anchor boxes [47] are essentially a list of predefined dimensions (boxes) aimed\nat best matching the objects of interest. Rather than manual determination of best-fit anchor boxes, the authors utilized k-means clustering [48] on the training set bounding\nboxes, inclusive of the ground truth bounding boxes, grouping similar shapes and plotting\naverage IoU with respect to the closest centroid as shown in Figure 4. YOLO-v2 was\ntrained on different architectures, namely, VGG-16 and GoogleNet, in addition to the authors proposing the Darknet-19 [49] architecture due to characteristics, such as reduced\nprocessing requirements, i.e., 5.58 FLOPs compared to 30.69 FLOPs and 8.52 FLOPs on\nVGG-16 and GoogleNet, respectively. In terms of performance, YOLO-v2 provided 76.8\nMachines2023,11,677 7of25\nmAP at 67 FPS and 78.6 mAP at 40 FPS. The results demonstrated the architectures superiority over SOTA architectures of that time, such as SSD and Faster R-CNN. YOLO-9000\nutilized YOLO-v2 architecture, aimed at real-time detection of more than 9000 different\nYOLO-v2architecture, aimedatreal-timedetectionofmorethan9000differentobjects;\nobjects; however, at a significantly reduced mAP of 19.7%.\nhowever,atasignificantlyreducedmAPof19.7%.\nFigure 4. Dimension clusters vs. mAP.\nFigure4.Dimensionclustersvs.mAP.\n2.3. YOLO-v3\n2.3. YOLO-v3\nArchitectures,suchasVGG,focusedtheirdevelopmentworkaroundtheconceptthat\nArchitectures, such as VGG, focused their development work around the concept\ndeepernetworks,i.e.,moreinternallayers,equatedtohigheraccuracy. YOLO-v2alsohad\nthat deeper networks, i.e., more internal layers, equated to higher accuracy. YOLO-v2 also\nhighernumberofconvolutionallayerscomparedtoitspredecessor.\nhad Hhiogwheevre nr,uamstbheerim oaf gceopnrvooglruestsioednathl rloauygehrst hceonmeptwaorrekd, tthoe iptsr opgrreedsseivceesdsoowr.n samplingHreosuwlteevdeirn, aths ethloes simofagfien ep-groragirneesdsefeda ttuhrreos;utghhe rtehfoer ne,eYtwOoLrOk-,v t2hoef tpernosgtrruesgsgilveed down samMachines 2023, 11, x FOR PEER REVIwEWith detectingsmallerobjects. Atthetimeresearchwasactiveinaddressingthisissue, 8 of 26\npling resulted in the loss of fine-grained features; therefore, YOLO-v2 often struggled with\nas evident by the deployment of skip connections [50] embedded within the proposed\ndetecting smaller objects. At the time research was active in addressing this issue, as eviResNetarchitecture,thefocuswasonaddressingthevanishinggradientissuebyfacilitating\ndent by the deployment of skip connections [50] embedded within the proposed ResNet\ninformationpropagationviaskipconnection,aspresentedinFigure5.\narchitecture, the focus was on addressing the vanishing gradient issue by facilitating information propagation via skip connection, as presented in Figure 5.\nFFiigguurree5 .5S. kSikpi-pco-cnonnecntieocnticoonn ficgounrfiatgiounr.ation.\nYOLO-v3 proposed a hybrid architecture factoring in aspects of YOLO-v2, Darknet53 [51], and the ResNet concept of residual networks. This enabled the preservation of\nfine-grained features by allowing for the gradient flow from shallow layers to deeper layers.\nOn top of the existing 53 layers of Darknet-53 for feature extraction, a stack of 53\nadditional layers was added for the detection head, totaling 106 convolutional layers for\nthe YOLO-v3. Additionally, YOLO-v3 facilitated multi-scale detection, namely, the architecture made predictions at three different scales of granularity for outputting better performance, increasing the probability of small object detection.\n2.4. YOLO-v4\nYOLO-v4 was the first variant of the YOLO family after the original author discontinued further work that was introduced to the computer vision community in April 2020\nby Alexey Bochkovsky et al. [52]. YOLO-v4 was essentially the distillation of a large suite\nof object detection techniques, tested and enhanced for providing a real-time, lightweight\nobject detector.\nThe backbone of an object detector has a critical role in the quality of features extracted. In-line with the experimental spirit, the authors experimented with three different\nbackbones: CSPResNext-50, CSPDarknet-53, and EfficientNet-B3 [53]. The first was based\non DenseNet [54] aimed at alleviating the vanishing gradient problem and bolstering feature propagation and reuse, resulting in reduced number of network parameters. EfficientNet was proposed by Google Brain. The paper posits that an optima selection for\nparameters when scaling CNNs can be ascertained through a search mechanism. After\nexperimenting with the above feature extractors, the authors based on their intuition and\nbacked by their experimental results selected CSPDarknet-53 as the official backbone for\nYOLO-v4.\nFor feature aggregation, the authors experimented with several techniques for integration at the neck level including feature pyramid network (FPN) [55] and path aggregation network (PANet) [56]. Ultimately, the authors opted for PANet as the feature aggregator. The modified PANet, as shown in Figure 6, utilized the concatenation mechanism.\nPANet can be seen as an advanced version of FPN, namely, PANet proposed a bottom-up\naugmentation path along with the top-down path (FPN), adding a shortcut connection\nfor linking fine-grained features from high- and low-level layers. Additionally, the authors\nintroduced a SPP [57] block post CSPDarknet-53 aimed at increasing the receptive field\nand separation of the important features arriving from the backbone.\nMachines2023,11,677 8of25\nYOLO-v3proposedahybridarchitecturefactoringinaspectsofYOLO-v2,Darknet53[51], andtheResNetconceptofresidualnetworks. Thisenabledthepreservationof\nfine-grainedfeaturesbyallowingforthegradientflowfromshallowlayerstodeeperlayers.\nOntopoftheexisting53layersofDarknet-53forfeatureextraction,astackof53additionallayerswasaddedforthedetectionhead,totaling106convolutionallayersforthe\nYOLO-v3. Additionally,YOLO-v3facilitatedmulti-scaledetection,namely,thearchitecture\nmadepredictionsatthreedifferentscalesofgranularityforoutputtingbetterperformance,\nincreasingtheprobabilityofsmallobjectdetection.\n2.4. YOLO-v4\nYOLO-v4wasthefirstvariantoftheYOLOfamilyaftertheoriginalauthordiscontinuedfurtherworkthatwasintroducedtothecomputervisioncommunityinApril2020\nbyAlexeyBochkovskyetal.[52]. YOLO-v4wasessentiallythedistillationofalargesuite\nofobjectdetectiontechniques,testedandenhancedforprovidingareal-time,lightweight\nobjectdetector.\nThebackboneofanobjectdetectorhasacriticalroleinthequalityoffeaturesextracted.\nIn-linewiththeexperimentalspirit,theauthorsexperimentedwiththreedifferentbackbones: CSPResNext-50,CSPDarknet-53,andEfficientNet-B3[53]. Thefirstwasbasedon\nDenseNet[54]aimedatalleviatingthevanishinggradientproblemandbolsteringfeature\npropagationandreuse,resultinginreducednumberofnetworkparameters. EfficientNet\nwasproposedbyGoogleBrain. Thepaperpositsthatanoptimaselectionforparameters\nwhenscalingCNNscanbeascertainedthroughasearchmechanism. Afterexperimenting\nwiththeabovefeatureextractors,theauthorsbasedontheirintuitionandbackedbytheir\nexperimentalresultsselectedCSPDarknet-53astheofficialbackboneforYOLO-v4.\nForfeatureaggregation,theauthorsexperimentedwithseveraltechniquesforintegrationatthenecklevelincludingfeaturepyramidnetwork(FPN)[55]andpathaggregation\nnetwork(PANet)[56]. Ultimately,theauthorsoptedforPANetasthefeatureaggregator.\nThemodifiedPANet,asshowninFigure6,utilizedtheconcatenationmechanism. PANet\ncanbeseenasanadvancedversionofFPN,namely,PANetproposedabottom-upaugmentationpathalongwiththetop-downpath(FPN),addingashortcutconnectionforlinking\nfine-grainedfeaturesfromhigh-andlow-levellayers. Additionally,theauthorsintroduced\nMachines 2023, 11, x FOR PEER REVIEW 9 of 26\naSPP[57]blockpostCSPDarknet-53aimedatincreasingthereceptivefieldandseparation\noftheimportantfeaturesarrivingfromthebackbone.\nFFigiugruere6. 6P. aPthatahg aggreggraetgioant.io(an). O(ar)ig OinraiglPinAaNl ,P(Ab)Nm, o(bd)ifi medodPAifiNe.d PAN.\nThe authors also introduced a bag-of-freebies, presented in Figure 7, primarily consisting of augmentations, such as Mosaic aimed at improving performance without introducing additional baggage onto the inference time. CIoU loss [58] was also introduced as\na freebie, focused on the overlap of the predicted and ground truth bounding box. In the\ncase of no overlap, the idea was to observe the closeness of the two boxes and encourage\noverlap if in close proximity.\nIn addition to the bag-of-freebies, the authors introduced bag-of-specials, with the\nauthors claiming that although this set of optimization techniques presented in Figure 7\nwould marginally impact the inference time, they would significantly improve the overall\nperformance. One of the components within the bag-of-specials was the Mish [59] activation function aimed at moving feature creations toward their respective optimal points.\nCross mini-batch normalization [60] was also presented facilitating the running on any\nGPU as many batch normalization techniques involve multiple GPUs operating in tandem.\nFigure 7. State-of-the-art optimization methodologies experimented in YOLO-v4 via bag-of-specials.\n2.5. YOLO-v5\nThe YOLO network in essence consists of three key pillars, namely, backbone for feature extraction, neck focused on feature aggregation, and the head for consuming output\nMachines 2023, 11, x FOR PEER REVIEW 9 of 26\nFigure 6. Path aggregation. (a) Original PAN, (b) modified PAN.\nThe authors also introduced a bag-of-freebies, presented in Figure 7, primarily consisting of augmentations, such as Mosaic aimed at improving performance without introducing additional baggage onto the inference time. CIoU loss [58] was also introduced as\na freebie, focused on the overlap of the predicted and ground truth bounding box. In the\ncase of no overlap, the idea was to observe the closeness of the two boxes and encourage\nMachines2023,11,677 overlap if in close proximity. 9of25\nIn addition to the bag-of-freebies, the authors introduced bag-of-specials, with the\nauthors claiming that although this set of optimization techniques presented in Figure 7\nwoulTdh meaaurgthinoarsllya lismopinatcrto tdhuec iendfearebnacge-o tfim-free,e tbhieeys, wproeusledn tseigdniinfiFcaignutlrye i7m,pprriomvaer tihlye coovnesriasltlipnegrfooframuagnmceen. tOatnioe nosf, sthuec hcoamsMpoonsaeinctasi mweitdhiant itmhep rboavgin-ogfp-seprefocriamlsan wceaws tihtheo Mutiisnht r[o5d9u] caicntgiavdadtiiotino fnuanlcbtaiognga agiemoendt oatt hmeoivnifnegre fnecaetutirme ec.reCaItoioUnlso tsosw[5a8r]dw thaesiar lrseospinetcrtoidveu coepdtiamsaal fpreoeinbties.,\nfCorcoussse dmoinni-tbhaetochv enrolarpmoaflitzhaetiporne d[6ic0t]e wdaasn dalgsoro purnedsetnrutetdh bfaocuilnitdaitninggb othxe. Irnutnhneincgas oeno fannoy\noGvPeUrla aps, tmheanidye baawtcahs ntooormbsaelrivzaettihoen ctleocshennieqsuseosf itnhveotwlvoe bmouxeltsipalned GePnUcosu orapgeeraotvinegrl ainp itfainncdleomse. proximity.\nFFiigguurree 77.. SStatatete-o-of-ft-hthee-a-artrto potpimtimiziaztaiotinonm methetohdoodloogloiegsieesx pexerpiemriemnteendteind YinO LYOO-LvO4-vvi4a bvaiag -boaf-gs-poefc-isaples.-\ncials.\nInadditiontothebag-of-freebies,theauthorsintroducedbag-of-specials,withthe\na2u.5t.h YoOrsLcOla-ivm5 ingthatalthoughthissetofoptimizationtechniquespresentedinFigure7\nwouldmarginallyimpacttheinferencetime,theywouldsignificantlyimprovetheoverall\nThe YOLO network in essence consists of three key pillars, namely, backbone for feaperformance. Oneofthecomponentswithinthebag-of-specialswastheMish[59]actiture extraction, neck focused on feature aggregation, and the head for consuming output\nvationfunctionaimedatmovingfeaturecreationstowardtheirrespectiveoptimalpoints.\nCrossmini-batchnormalization[60]wasalsopresentedfacilitatingtherunningonany\nGPUasmanybatchnormalizationtechniquesinvolvemultipleGPUsoperatingintandem.\n2.5. YOLO-v5\nThe YOLO network in essence consists of three key pillars, namely, backbone for\nfeature extraction, neck focused on feature aggregation, and the head for consuming\noutputfeaturesfromtheneckasinputandgeneratingdetections. YOLO-v5[61]similarto\nYOLO-v4,withrespecttocontributions,focusontheconglomerationandrefinementof\nvariouscomputervisiontechniquesforenhancingperformance. Inaddition,inlessthan\n2monthsafterthereleaseofYOLO-v4,GlennJocheropen-sourcedanimplementationof\nYOLO-v5[61].\nAnotablementionisthatYOLO-v5wasthefirstnativereleaseofarchitecturesbelongingtotheYOLOclan,tobewritteninPyTorch[62]ratherthanDarknet. Although\nDarknetisconsideredasaflexiblelow-levelresearchframework,itwasnotpurposebuilt\nforproductionenvironmentswithasignificantlysmallernumberofsubscribersdueto\nconfigurabilitychallenges. PyTorch,ontheotherhand,providedanestablishedeco-system,\nwithawidersubscriptionbaseamongthecomputervisioncommunityandprovidedthe\nsupportinginfrastructureforfacilitatingmobiledevicedeployment.\nMachines 2023, 11, x FOR PEER REVIEW 10 of 26\nfeatures from the neck as input and generating detections. YOLO-v5 [61] similar to YOLOv4, with respect to contributions, focus on the conglomeration and refinement of various\ncomputer vision techniques for enhancing performance. In addition, in less than 2 months\nafter the release of YOLO-v4, Glenn Jocher open-sourced an implementation of YOLO-v5\n[61].\nA notable mention is that YOLO-v5 was the first native release of architectures belonging to the YOLO clan, to be written in PyTorch [62] rather than Darknet. Although\nDarknet is considered as a flexible low-level research framework, it was not purpose built\nfor production environments with a significantly smaller number of subscribers due to\nMachines2023,11,677 10of25\nconfigurability challenges. PyTorch, on the other hand, provided an established eco-system, with a wider subscription base among the computer vision community and provided\nthe supporting infrastructure for facilitating mobile device deployment.\nInaddition,anothernotableproposalwastheautomatedanchorboxlearningconcept.\nIn addition, another notable proposal was the automated anchor box learning conInYOLO-v2,theanchorboxmechanismwasintroducedbasedonselectinganchorboxes\ncept. In YOLO-v2, the anchor box mechanism was introduced based on selecting anchor\nthat closely resemble the dimensions of the ground truth boxes in the training set via\nboxes that closely resemble the dimensions of the ground truth boxes in the training set\nk-means. Theauthorsselectthefiveclose-fitanchorboxesbasedontheCOCOdataset[63]\nvia k-means. The authors select the five close-fit anchor boxes based on the COCO dataset\nandimplementthemasdefaultboxes. However,theapplicationofthismethodologytoa\n[63] and implement them as default boxes. However, the application of this methodology\nuniquedatasetwithsignificantobjectdifferentialscomparedtothosepresentintheCOCO\nto a unique dataset with significant object differentials compared to those present in the\ndatasetcanquicklyexposetheinabilityofthepredefinedboxestoadaptquicklytothe\nCOCO dataset can quickly expose the inability of the predefined boxes to adapt quickly\nuniquedataset. Therefore,authorsinYOLO-v5integratedtheanchorboxselectionprocess\nto the unique dataset. Therefore, authors in YOLO-v5 integrated the anchor box selection\nintotheYOLO-v5pipeline. Asaresult,thenetworkwouldautomaticallylearnthebest-fit\nprocess into the YOLO-v5 pipeline. As a result, the network would automatically learn\nanchorboxesfortheparticulardatasetandutilizethemduringtrainingtoacceleratethe\nthe best-fit anchor boxes for the particular dataset and utilize them during training to acprocess. YOLO-v5comesinseveralvariantswithrespecttothecomputationalparameters\ncelerate the process. YOLO-v5 comes in several variants with respect to the computational\naspresentedinTable1.\nparameters as presented in Table 1.\nTable1.YOLO-v5internalvariantcomparison.\nTable 1. YOLO-v5 internal variant comparison.\nMMooddeell AAvevrearaggee PPrreecciissiioonn ((@@5500)) PPaarraammeetteerrss FFLLOOPPss\nYYOOLLOO--vv55ss 5555.8.8%% 77.5.5 MM 1133.2.2BB\nYYOOLLOO--vv55mm 6622.4.4%% 2211.8.8 MM 3399.4.4BB\nYOLO-v5l 65.4% 47.8M 88.1B\nYOLO-v5l 65.4% 47.8 M 88.1B\nYOLO-v5x 66.9% 86.7M 205.7B\nYOLO-v5x 66.9% 86.7 M 205.7B\nYYOOLLOO--vv55 ccoommpprriisseedd ooff aa wweeiigghhtt fifillee eeqquuaattiinngg ttoo 2277 MMBB ccoommppaarreedd ttoo YYOOLLOO--vv55ll aatt 119922\nMMBB. .FFiigguurree 88 ddeemmoonnssttrraatteess tthhee ssuuppeerriioorriittyy ooff YYOOLLOO--vv55 oovveerr EEffifficciieennttDDeett [[6644]]..\nFFiigguurree 88. .YYOOLLOO-v-v55 vvaarriaiannt tccoommppaarrisisoonn vvss. .EEffifficcieienntDtDeet t[6[611].] .\n2.6. YOLO-v6\nThe initial codebase for YOLO-v6 [65] was released in June 2022 by the Meituan\nTechnicalTeambasedinChina. Theauthorsfocusedtheirdesignstrategyonproducingan\nindustry-orientatedobjectdetector.\nTomeetindustrialapplicationrequirements,thearchitecturewouldneedtobehighly\nperformant on a range of hardware options, maintaining high speed and accuracy. To\nconformwiththediversesetofindustrialapplications,YOLO-v6comesinseveralvariants\nstartingwithYOLO-v6-nanoasthefastestwiththeleastnumberofparametersandreaching\nYOLO-v6-largewithhighaccuracyattheexpenseofspeed,asshowninTable2.\nMachines 2023, 11, x FOR PEER REVIEW 11 of 26\n2.6. YOLO-v6\nThe initial codebase for YOLO-v6 [65] was released in June 2022 by the Meituan Technical Team based in China. The authors focused their design strategy on producing an\nindustry-orientated object detector.\nTo meet industrial application requirements, the architecture would need to be\nhighly performant on a range of hardware options, maintaining high speed and accuracy.\nTo conform with the diverse set of industrial applications, YOLO-v6 comes in several variants starting with YOLO-v6-nano as the fastest with the least number of parameters and\nMachines2023,11,677 reaching YOLO-v6-large with high accuracy at the expense of speed, as shown in T1a1bolfe2 52.\nTable 2. YOLO-v6 variant comparison.\nTable2.YOLO-v6variantcomparison.\nmAP 0.5:0.95\nVariant FPS Tesla T4 Parameters (Million)\n(COCO-val)\nmAP0.5:0.95\nVariant FPSTeslaT4 Parameters(Million)\nYOLO-v6-N (C3O5C.9O (-3v0a0l) epochs) 802 4.3\nYYOOLLOO-v-v6-6N-T 35.94(300.03 e(3p0o0ch esp)ochs) 802 449 41.53.0\nYOYLOOL-Ov6-v-R6-eTpOpt 40.34(330.03 e(3p0o0ch esp)ochs) 449 596 1157.0.2\nYOLYOO-vL6O-R-vep6-OSp t 43.34(330.05 e(3p0o0ch esp)ochs) 596 495 1177.2.2\nYOLO-v6-S 43.5(300epochs) 495 17.2\nYOLO-v6-M 49.7 233 34.3\nYOLO-v6-M 49.7 233 34.3\nYOLO-v6-L-ReLU 51.7 149 58.5\nYOLO-v6-L-ReLU 51.7 149 58.5\nThe impressive performance presented in Table 2 is a result of several innovations\nTheimpressiveperformancepresentedinTable2isaresultofseveralinnovations\nintegrated into the YOLO-v6 architecture. The key contributions can be summed into four\nintegratedintotheYOLO-v6architecture. Thekeycontributionscanbesummedintofour\npoints. First, in contrast to its predecessors, YOLO-v6 opts for an anchor-free approach,\npoints. First,incontrasttoitspredecessors,YOLO-v6optsforananchor-freeapproach,\nmaking it 51% faster when compared to anchor-based approaches.\nmakingit51%fasterwhencomparedtoanchor-basedapproaches.\nSecond, the authors introduced a revised reparametrized backbone and neck, proSecond,theauthorsintroducedarevisedreparametrizedbackboneandneck,proposed\nposed as EfficientRep backbone and Rep-PAN neck [66], namely, up to and including\nasEfficientRepbackboneandRep-PANneck[66],namely,uptoandincludingYOLO-v5,\nYOLO-v5, the regression and classification heads shared the same features. Breaking the\ntheregressionandclassificationheadssharedthesamefeatures. Breakingtheconvention,\nconvention, YOLO-v6 implements the decoupled head as shown in Figure 9. As a result,\nYOLO-v6implementsthedecoupledheadasshowninFigure9.Asaresult,thearchitecture\nthe architecture has additional layers separating features from the final head, as empirihas additional layers separating features from the final head, as empirically shown to\ncally shown to improve the performance. Third, YOLO-v6 mandates a two-loss function.\nimprovetheperformance. Third,YOLO-v6mandatesatwo-lossfunction. Varifocalloss\nVarifocal loss (VFL) [67] is used as the classification loss and distribution focal loss (DFL)\n(VFL)[67]isusedastheclassificationlossanddistributionfocalloss(DFL)[68],alongwith\n[68], along with SIoU/GIoU [69] as regression loss. VFL being a derivative of focal loss,\nSIoU/GIoU [69] as regression loss. VFL being a derivative of focal loss, treats positive\ntreats positive and negative samples at varying degrees of importance, helping in balancandnegativesamplesatvaryingdegreesofimportance,helpinginbalancingthelearning\ning the learning signals from both sample types. DFL is deployed for box regression in\nsignalsfrombothsampletypes. DFLisdeployedforboxregressioninYOLO-v6medium\nYOLO-v6 medium and large variants, treating the continuous distribution of the box loandlargevariants,treatingthecontinuousdistributionoftheboxlocationsasdiscretized\ncations as discretized probability distribution, which is shown to be particularly efficient\nprobabilitydistribution,whichisshowntobeparticularlyefficientwhenthegroundtruth\nwhen the ground truth box boundaries are blurred.\nboxboundariesareblurred.\nFigure9.YOLO-v6modelbasearchitecture.\nAdditionalimprovementsfocusedonindustrialapplicationsincludetheuseofknowledgedistillation[70],involvingateachermodelusedfortrainingastudentmodel,where\nthepredictionsoftheteacherareusedassoftlabelsalongwiththegroundtruthfortraining\nthestudent. Thiscomeswithoutfuelingthecomputationalcostasessentiallytheaimis\ntotrainasmaller(student)modeltoreplicatethehighperformanceofthelarger(teacher)\nmodel. ComparingtheperformanceofYOLO-v6withitspredecessors,includingYOLO-v5\nonthebenchmarkCOCOdatasetinFigure10,itisclearthatYOLO-v6achievesahigher\nmAPatvariousFPS.\nMachines 2023, 11, x FOR PEER REVIEW 12 of 26\nFigure 9. YOLO-v6 model base architecture.\nAdditional improvements focused on industrial applications include the use of\nknowledge distillation [70], involving a teacher model used for training a student model,\nwhere the predictions of the teacher are used as soft labels along with the ground truth\nfor training the student. This comes without fueling the computational cost as essentially\nthe aim is to train a smaller (student) model to replicate the high performance of the larger\n(teacher) model. Comparing the performance of YOLO-v6 with its predecessors, includMachines2023,11,677 ing YOLO-v5 on the benchmark COCO dataset in Figure 10, it is clear that YOL12Oo-fv256\nachieves a higher mAP at various FPS.\nFFiigguurree 1100.. RReellaattiivvee eevvaalluuaattiioonn ooff YYOOLLOO--vv66 vvss.. YYOOLLOO--vv55 [[7711]]..\n2.7. YOLO-v7\n2.7. YOLO-v7\nThefollowingmonthafterthereleaseofYOLO-v6,theYOLO-v7wasreleased[72].\nThe following month after the release of YOLO-v6, the YOLO-v7 was released [72].\nAlthoughothervariantshavebeenreleasedinbetween,includingYOLO-X[73]andYOLOAlthough other variants have been released in between, including YOLO-X [73] and\nR[74],thesefocusedmoreonGPUspeedenhancementswithrespecttoinferencing. YOLOYOLO-R [74], these focused more on GPU speed enhancements with respect to inferencv7proposesseveralarchitecturalreformsforimprovingtheaccuracyandmaintaininghigh\ning. YOLO-v7 proposes several architectural reforms for improving the accuracy and\ndetection speeds. The proposed reforms can be split into two categories: Architectural\nmaintaining high detection speeds. The proposed reforms can be split into two categories:\nreformsandTrainableBoF(bag-of-freebies). ArchitecturalreformsincludedtheimplemenArchitectural reforms and Trainable BoF (bag-of-freebies). Architectural reforms included\ntationoftheE-ELAN(extendedefficientlayeraggregationnetwork)[75]intheYOLO-v7\nthe implementation of the E-ELAN (extended efficient layer aggregation network) [75] in\nbackbone,takinginspirationfromresearchadvancementsinnetworkefficiency. Thedesign\nthe YOLO-v7 backbone, taking inspiration from research advancements in network effioftheE-ELANwasguidedbytheanalysisoffactorsthatimpactaccuracyandspeed,such\nciency. The design of the E-ELAN was guided by the analysis of factors that impact accuasmemoryaccesscost,input/outputchannelratio,andgradientpath.\nracy and speed, such as memory access cost, input/output channel ratio, and gradient\nThesecondarchitecturalreformwaspresentedascompoundmodelscaling,asshown\npath.\ninFigure11. Theaimwastocaterforawiderscopeofapplicationrequirements,i.e.,certain\nThe second architectural reform was presented as compound model scaling, as\napplicationsrequireaccuracytobeprioritized,whilstothersmayprioritizespeed.Although\nshown in Figure 11. The aim was to cater for a wider scope of application requirements,\nNAS(networkarchitecturesearch)[76]canbeusedforparameter-specificscalingtofind\ni.e., certain applications require accuracy to be prioritized, whilst others may prioritize\nthebestfactors,thescalingfactorsareindependent[77]. Whereasthecompound-scaling\nMachines 2023, 11, x FOR PEER REVIEsWpe ed. Although NAS (network architecture search) [76] can be used for paramet1e3r -osfp 2e6mechanismallowsforthewidthanddepthtobescaledincoherenceforconcatenation-\ncific scaling to find the best factors, the scaling factors are independent [77]. Whereas the\nbasednetworks,maintainingoptimalnetworkarchitecturewhilescalingfordifferentsizes.\ncompound-scaling mechanism allows for the width and depth to be scaled in coherence\nfor concatenation-based networks, maintaining optimal network architecture while scaling for different sizes.\nFFiigguurree 1111.. YYOOLLOO--vv77 ccoommppoouunndd ssccaalliinngg..\nRe-parameterization planning is based on averaging a set of model weights to obtain\na more robust network [78,79]. Expanding further, module level re-parameterization enables segments of the network to regulate their own parameterization strategies. YOLO-v7\nutilizes gradient flow propagation paths with the aim to observe which internal network\nmodules should deploy re-parameterization strategies.\nThe auxiliary head coarse-to-fine concept is proposed on the premise that the network head is quite far downstream; therefore, the auxiliary head is deployed at the middle\nlayers to assist in the training process. However, this would not train as efficiently as the\nlead head, due to the former not having access to the complete network.\nFigure 12 presents a performance comparison of YOLO-v7 with the preceding YOLO\nvariants on the MS COCO dataset. It is clear from Figure 12 that all YOLO-v7 variants\nsurpassed the compared object detectors in accuracy and speed in the range of 5160 FPS.\nIt is, however, important to note, as mentioned by the authors of YOLO-v7, that none of\nthe YOLO-v7 variants are designed for CPU-based mobile device deployment. YOLO-v7tiny/v7/W6 variants are designed for edge GPU, consumer GPU, and cloud GPU, respectively. Whilst YOLO-v7-E6/D6/E6E are designed for high-end cloud GPUs only.\nFigure 12. YOLO-v7 comparison vs. other object detectors [72].\nMachines 2023, 11, x FOR PEER REVIEW 13 of 26\nMachines2023,11,677 13of25\nFigure 11. YOLO-v7 compound scaling.\nRe-parameterization planning is based on averaging a set of model weights to obtain\na morRee r-pobaruasmt neetetrwizoartkio [n78p,7la9n].n Einxgpiasnbdaisnegd founrtahveer,r amgoindguales leetvoefl mreo-pdaerlawmeeigtehrtisztaotioobnt aeinnaabmleosr esergombuesnttns eotfw tohrek n[e7t8w,7o9r]k. Etox preagnudliantge ftuhretihr eorw,mno pdaurlaemleevteerlirzea-tpioarna smtreatteergizieasti.o YnOeLnOab-lve7s\nusetiglimzeesn gtsraodfiethnet flnoewtw porrokptaogaretigounl aptaethths ewiritohw thne paaimra mtoe otebrsiezravteio wnhsitcrha tiengtieersn.alY nOeLtwOo-vrk7\nutilizesgradientflowpropagationpathswiththeaimtoobservewhichinternalnetwork\nmodules should deploy re-parameterization strategies.\nmodulesshoulddeployre-parameterizationstrategies.\nThe auxiliary head coarse-to-fine concept is proposed on the premise that the netTheauxiliaryheadcoarse-to-fineconceptisproposedonthepremisethatthenetwork\nwork head is quite far downstream; therefore, the auxiliary head is deployed at the middle\nheadisquitefardownstream;therefore,theauxiliaryheadisdeployedatthemiddlelayers\nlayers to assist in the training process. However, this would not train as efficiently as the\ntoassistinthetrainingprocess. However,thiswouldnottrainasefficientlyasthelead\nlead head, due to the former not having access to the complete network.\nhead,duetotheformernothavingaccesstothecompletenetwork.\nFigure 12 presents a performance comparison of YOLO-v7 with the preceding YOLO\nFigure12presentsaperformancecomparisonofYOLO-v7withtheprecedingYOLO\nvariants on the MS COCO dataset. It is clear from Figure 12 that all YOLO-v7 variants\nvariantsontheMSCOCOdataset. ItisclearfromFigure12thatallYOLO-v7variants\nsurpassed the compared object detectors in accuracy and speed in the range of 5160 FPS.\nsurpassedthecomparedobjectdetectorsinaccuracyandspeedintherangeof5160FPS.\nIt is, however, important to note, as mentioned by the authors of YOLO-v7, that none of\nItis,however,importanttonote,asmentionedbytheauthorsofYOLO-v7,thatnoneof\nthe YOLO-v7 variants are designed for CPU-based mobile device deployment. YOLO-v7the YOLO-v7 variants are designed for CPU-based mobile device deployment. YOLOtiny/v7/W6 variants are designed for edge GPU, consumer GPU, and cloud GPU, respecv7-tiny/v7/W6 variants are designed for edge GPU, consumer GPU, and cloud GPU,\ntively. Whilst YOLO-v7-E6/D6/E6E are designed for high-end cloud GPUs only.\nrespectively. WhilstYOLO-v7-E6/D6/E6Earedesignedforhigh-endcloudGPUsonly.\nFFiigguurree 1122.. YYOOLLOO--vv77 ccoommppaarriissoonn vvss.. ootthheerr oobbjjeecctt ddeetteeccttoorrss [[7722]]..\nInternalvariantcomparisonofYOLO-v7ispresentedinTable3. Asevident,thereisa\nsignificantperformancegapwithrespecttomAPwhencomparingYOLO-v7-tinywiththe\ncomputationallydemandingYOLO-v7-D6. However,thelatterwouldnotbesuitablefor\nedgedeploymentontoacomputationallyconstraineddevice.\nTable3.VariantcomparisonofYOLO-v7.\nModel Size(Pixels) mAP(@50) Parameters FLOPs\nYOLO-v7-tiny 640 52.8% 6.2M 5.8G\nYOLO-v7 640 69.7% 36.9M 104.7G\nYOLO-v7-X 640 71.1% 71.3M 189.9G\nYOLO-v7-E6 1280 73.5% 97.2M 515.2G\nYOLO-v7-D6 1280 73.8% 154.7M 806.8G\nMachines2023,11,677 14of25\n2.8. YOLO-v8\nThelatestadditiontothefamilyofYOLOwasconfirmedinJanuary2023withthe\nreleaseofYOLO-v8[80]byUltralytics(alsoreleasedYOLO-v5). Althoughapaperrelease\nisimpendingandmanyfeaturesareyettobeaddedtotheYOLO-v8repository, initial\ncomparisonsofthenewcomeragainstitspredecessorsdemonstrateitssuperiorityasthe\nnewYOLOstate-of-the-art.\nFigure13demonstratesthatwhencomparingYOLO-v8againstYOLO-v5andYOLOv6trainedon640imageresolution,allYOLO-v8variantsoutputbetterthroughputwitha\nsimilarnumberofparameters,indicatingtowardhardware-efficient,architecturalreforms.\nThefactthatYOLO-v8andYOLO-v5arepresentedbyUltralyticswithYOLO-v5providing\nimpressivereal-timeperformanceandbasedontheinitialbenchmarkingresultsreleased\nMachines 2023, 11, x FOR PEER REVIEW 15 of 26\nbyUltralytics,itisstronglyassumedthattheYOLO-v8willbefocusingonconstrained\nedgedevicedeploymentathigh-inferencespeed.\nFFigiguurere1 133..Y YOOLLOO-v-v88c coommppaarrisisoonnw witihthp prereddeecceesssosorsrs[ 8[800].].\n33..I InndduussttrriiaallD DeeffeeccttD Deetteeccttiioonnv ViaiaY YOOLLOO\nTThheep prreevvioiouusss seecctitoionnd deemmoonnssttrraatteesst thheer raappidide evvooluluttiioonno offt htheeY YOOLLOO cclalanno offo obbjejecctt\ndetectors amongst the computer vision community. This section of the review focuses\ndetectors amongst the computer vision community. This section of the review focuses on\nontheimplementationofYOLOvariantsforthedetectionofsurfacedefectswithinthe\nthe implementation of YOLO variants for the detection of surface defects within the inindustrialsetting. Theselectionofindustrialsettingisduetoitsvaryingandstringent\ndustrial setting. The selection of industrial setting is due to its varying and stringent rerequirementsalternatingbetweenaccuracyandspeed,athemewhichisfoundthrough\nquirements alternating between accuracy and speed, a theme which is found through\nDNAoftheYOLOvariants.\nDNA of the YOLO variants.\n3.1. Industrial Fabric Defect Detection\nRui Jin et al. [81] in their premise state the inefficiencies of manual inspection in the\ntextile manufacturing domain as high cost of labor, human-related fatigue, and reduced\ndetection speed (less than 20 m/min). The authors aim to address these inefficiencies by\nproposing a YOLO-v5-based architecture, coupled with a spatial attention mechanism for\naccentuation of smaller defective regions. The proposed approach involved a teacher network trained on the fabric dataset. Post training of the teacher network, the learned\nweights were distilled to the student network, which was compatible for deployment onto\na Jetson TX2 [82] via TensorRT [83]. The results presented by the authors show, as expected, that the teacher network reported higher performance with an AUC of 98.1% compared to 95.2% (student network). However, as the student network was computationally\nsmaller, the inference time was significantly less at 16 ms for the student network in contrast to the teacher network at 35 ms on the Jetson TX2. Based on the performance, the\nMachines2023,11,677 15of25\n3.1. IndustrialFabricDefectDetection\nRuiJinetal.[81]intheirpremisestatetheinefficienciesofmanualinspectioninthe\ntextilemanufacturingdomainashighcostoflabor,human-relatedfatigue,andreduced\ndetectionspeed(lessthan20m/min). Theauthorsaimtoaddresstheseinefficienciesby\nproposing a YOLO-v5-based architecture, coupled with a spatial attention mechanism\nforaccentuationofsmallerdefectiveregions. Theproposedapproachinvolvedateacher\nnetworktrainedonthefabricdataset. Posttrainingoftheteachernetwork,thelearned\nweightsweredistilledtothestudentnetwork,whichwascompatiblefordeploymentonto\naJetsonTX2[82]viaTensorRT[83]. Theresultspresentedbytheauthorsshow,asexpected,\nMachines 2023, 11, x FOR PEER REVIEthWa ttheteachernetworkreportedhigherperformancewithanAUCof98.1%comp1a6r eodf t2o6\n95.2%(studentnetwork). However,asthestudentnetworkwascomputationallysmaller,\ntheinferencetimewassignificantlylessat16msforthestudentnetworkincontrasttothe\nteachernetworkat35msontheJetsonTX2. Basedontheperformance,theauthorsclaim\nauthors claim that the proposed solution provides high accuracy and real-time inference\nthattheproposedsolutionprovideshighaccuracyandreal-timeinferencespeed,makingit\nspeed, making it compatible for deployment via the edge device.\ncompatiblefordeploymentviatheedgedevice.\nSifundvoleshile Dlamini et al. [84] propose a production environment fabric defect\nSifundvoleshileDlaminietal.[84]proposeaproductionenvironmentfabricdefect\ndetection framework focused on real-time detection and accurate classification on-site, as\ndetection framework focused on real-time detection and accurate classification on-site,\nshown in Figure 14. The authors embed conventional image processing at the onset of\nasshowninFigure14. Theauthorsembedconventionalimageprocessingattheonset\ntheir data enhancement strategy, i.e., filtering to denoise feature enhancement. Post augof their data enhancement strategy, i.e., filtering to denoise feature enhancement. Post\nmentations and data scaling, the authors train the YOLO-v4 architecture based on preaugmentations and data scaling, the authors train the YOLO-v4 architecture based on\ntrained weights. The reported performance was respectable with an F1-score of 93.6%, at\npretrainedweights. ThereportedperformancewasrespectablewithanF1-scoreof93.6%,\nan impressive detection speed of 34 fps and prediction speed of 21.4 ms. The authors claim\natanimpressivedetectionspeedof34fpsandpredictionspeedof21.4ms. Theauthors\nthat the performance is evident to the effectiveness of the selected architecture for the\nclaimthattheperformanceisevidenttotheeffectivenessoftheselectedarchitectureforthe\ngiven domain.\ngivendomain.\nFigure14.Inspectionmachineintegration[84].\nFigure 14. Inspection machine integration [84].\nRestrictedbytheavailablecomputingresourcesforedgedeployment,GuijuanLinetal.[85]\nRestricted by the available computing resources for edge deployment, Guijuan Lin et\nstateproblemswithqualityinspectioninthefabricproductiondomain,includingminute\nal. [85] state problems with quality inspection in the fabric production domain, including\nscaleofdefects,extremeunbalancewiththeaspectratioofcertaindefects,andslowdefect\nminute scale of defects, extreme unbalance with the aspect ratio of certain defects, and\ndetectionspeeds. Toaddresstheseissues,theauthorsproposedasliding-window,selfslow defect detection speeds. To address these issues, the authors proposed a sliding-winattention(multihead)mechanismcalibratedforsmalldefecttargets. Additionally,theSwin\ndow, self-attention (multihead) mechanism calibrated for small defect targets. AdditionTransformer[86]moduleasdepictedinFigure15wasintegratedintotheoriginalYOLO-v5\nally, the Swin Transformer [86] module as depicted in Figure 15 was integrated into the\narchitecturefortheextractionofhierarchicalfeatures. Furthermore,thegeneralizedfocal\noriginal YOLO-v5 architecture for the extraction of hierarchical features. Furthermore, the\nloss is implemented with the architecture aimed at improving the learning process for\ngeneralized focal loss is implemented with the architecture aimed at improving the learnpositivetargetinstances,whilstloweringtherateofmisseddetections. Theauthorsreport\ning process for positive target instances, whilst lowering the rate of missed detections. The\ntheaccuracyoftheproposedsolutiononareal-worldfabricdataset,reaching76.5%mAP\nauthors report the accuracy of the proposed solution on a real-world fabric dataset, reaching 76.5% mAP at 58.8 FPS, making it compatible with the real-time detection requirements for detection via embedded devices.\nMachines2023,11,677 16of25\nMachines 2023, 11, x FOR PEER REVIEW 17 of 26\nat58.8FPS,makingitcompatiblewiththereal-timedetectionrequirementsfordetection\nviaembeddeddevices.\nFFiigguurree1 155..B Bacakcbkobnoenfeo rfoSrw SinwTinra Tnsrfaonrsmfoerrmneetrw noerktw[8o5r]k. [85].\n3.2. SolarCellSurfaceDefectDetection\n3.2. Solar Cell Surface Defect Detection\nSettingtheirpremise,theauthors[87]statethathuman-ledPhotovoltaic(PV)inspecSetting their premise, the authors [87] state that human-led Photovoltaic (PV) inspectionhasmanydrawbacksincludingtherequirementofoperationandmaintenance(O&M)\ntion has many drawbacks including the requirement of operation and maintenance\nengineers, cell-by-cell inspection, high workload, and reduced efficiency. The authors\np(Oro&poMse) aenngiminpereorvse, dcealrlc-hbiyte-ccteullr einbsapseedctoionnY, OhLigOh- vw5oforkrltohaedc,h aanradc treerdizuatcieodn eoffifccoimenpcleyx. The austohloarrsc epllrosuprofasece atnex tiumrpesroanvdedd eafrecchtiivteecrteugrioen bs.aTsehde porno pYoOsaLlOis-bva5s efdoro nththee cihnatergarcatteiroinzation of\nocfodmepfolremx asbolleacro cnevlol lsuutirofnacwei ttheixntuthreesC SaPndm oddeufelectwivieth rtehgeioainms.o Tfhaceh ipervoinpgosaanl aidsa bpatisveed on the\nlearningscale. Additionally,anattentionmechanismisincorporatedforenhancedfeature\nintegration of deformable convolution within the CSP module with the aim of achieving\nextraction. Moreover,theauthorsoptimizetheoriginalYOLO-v5architecturefurthervia\nan adaptive learning scale. Additionally, an attention mechanism is incorporated for enK-means++clusteringforanchorboxdeterminationalgorithm. Basedonthepresented\nhanced feature extraction. Moreover, the authors optimize the original YOLO-v5 architecresults,theimprovedarchitectureachievedarespectablemAPof89.64%onanEL-based\nture further via K-means++ clustering for anchor box determination algorithm. Based on\nsolarcellimagedataset,7.85%highercomparedtomAPfortheoriginalarchitecture,with\nthe presented results, the improved architecture achieved a respectable mAP of 89.64% on\ndetectionspeedreaching36.24FPS,whichcanbetranslatedasamoreaccuratedetection\nwanh iEleLr-ebmaasiendin sgoclaorm cpealtli bimleawgiet hdtahteasreeat,l -7ti.m85e%re qhuigirheemre cnotsm. pared to mAP for the original architeActmurraen, wBiinthom daeitreachtieotna ls.p[e8e8d] hriegahclhigihntgt w36o.2f4re FqPueSn, twdheifcehct scaenn cboeu nttrearnesdladtuedri nags a more\ntahcecumraanteu fdacettuercitniognp rwocheisles orefmcryaisntailnlign ecosomlapracteiblllsea wsditahr kthsep orte/arle-gtiimone arnedqumiricermocernactsk.s .\nThe laAttmerracann Bhinavoemaaidreathri emt eanlt. a[l8i8m] phaigctholinghtht etwpoer ffroerqmuaenncte doefftehcetsm eondcuoluen,twerheicdh diusring the\namajorcauseforPVmodulefailures. TheauthorssubscribetotheYOLOarchitecture,\nmanufacturing process of crystalline solar cells as dark spot/region and microcracks. The\ncomparingtheperformanceoftheirmethodologyonYOLO-v4andanimprovedYOLO-v4latter can have a detrimental impact on the performance of the module, which is a major\ntinyintegratedwithaspatialpyramidpoolingmechanism. Basedonthepresentedresults,\ncause for PV module failures. The authors subscribe to the YOLO architecture, comparing\nYOLO-v4achieved98.8%mAPat62.9ms,whilsttheimprovedYOLO-v4-tinylaggedwith\nthe performance of their methodology on YOLO-v4 and an improved YOLO-v4-tiny inte91%mAPat28.2ms. Theauthorsclaimthatalthoughthelatterislessaccurate,itisnotably\nfgarsateterdth wanitthh ea fsopramteiar.l pyramid pooling mechanism. Based on the presented results, YOLOv4 acThiainevyieSdu 9n8e.8t%al. m[8A9]Pfo actu 6s2o.n9 amusto, mwahtieldsth tohte-s ipmotpdroetveecdti oYnOwLitOh-inv4P-Vtinceyl llsabgagseedd wa ith 91%\nmmoAdPifi aedt 2v8e.r2s imonso. fTthhee YauOtLhOor-vs 5calarcimhit ethctautr ea.ltThhoeufigrhst tihmep lraottveerm iesn ltecsosm aecscuinrathtee, fiotr mis notably\nofafsetnehr atnhcaend tahnec hfoorrmsaenrd. detectionheadsfortherespectivearchitecture. Toimprovethe\ndetectionprecisionatvaryingscales,k-meansclustering[48]isdeployedforclusteringthe\nTianyi Sun et al. [89] focus on automated hot-spot detection within PV cells based a\nlengthwidthratiowithrespecttothedataannotationframe. Additionally,asetofthe\nmodified version of the YOLO-v5 architecture. The first improvement comes in the form\nanchorsconsistingofsmallervalueswereaddedtocaterforthedetectionofsmalldefects\nof enhanced anchors and detection heads for the respective architecture. To improve the\nbyoptimizingtheclusternumber. Thereportedperformanceoftheimprovedarchitecture\ndetection precision at varying scales, k-means clustering [48] is deployed for clustering\nwasreportedas87.8%mAP,withtheaveragerecallrateof89.0%andF1-scorereaching\nthe lengthwidth ratio with respect to the data annotation frame. Additionally, a set of the\nanchors consisting of smaller values were added to cater for the detection of small defects\nby optimizing the cluster number. The reported performance of the improved architecture\nwas reported as 87.8% mAP, with the average recall rate of 89.0% and F1-score reaching\n88.9%. The reported FPS was impressive reaching 98.6 FPS, with the authors claiming that\nthe proposed solution would provide intelligent monitoring at PV power stations. Inferencing output presented in Figure 16 shows the proposed AP-YOLO-v5 architecture,\nproviding inferences at a higher confidence level compared to the original YOLO-v5.\nMachines2023,11,677 17of25\n88.9%. The reported FPS was impressive reaching 98.6 FPS, with the authors claiming\nthat the proposed solution would provide intelligent monitoring at PV power stations.\nMachines 2023, 11, x FOR PEER REVIEW 18 of 26\nInferencingoutputpresentedinFigure16showstheproposedAP-YOLO-v5architecture,\nprovidinginferencesatahigherconfidencelevelcomparedtotheoriginalYOLO-v5.\nFFiigguurree1 166..I ninfefreernecnec/ec/ocnofindfiednecnecceo cmopmapriasorinso[8n9 ][.89].\n3.3. SteelSurfaceDefectDetection\n3.3. Steel Surface Defect Detection\nDinmingYangetal.[90]setthepremiseoftheirresearchbystatingtheimportance\nDinming Yang et al. [90] set the premise of their research by stating the importance\nofsteelpipequalityinspection,citingthegrowingdemandincountries,suchasChina.\nof steel pipe quality inspection, citing the growing demand in countries, such as China.\nAlthoughX-raytestingisutilizedasoneofthekeymethodsforindustrialnondestructive\ntAeslttihnogu(NghD XT)-,rtahye taeustthinogrs isst autteiltihzaetdit asst iollnreeq oufi rtehseh kuemya mneatshsiostdasn cfoerf oinrdthuesdtreitaelr mnoinnadtieosnt,ructive\nctleasstsiinfigc a(tNioDn,Ta)n, dthleo caaulitzhaotirosn sotaftteh ethdaetf eitc tsst.ilTl hreeqauuitrheosr shpurmopaons easthsiestiamnpclee mfoern tthatei odneotefrminaYtiOonL,O c-lva5ssfiofircpartoiodnu,c tainond- bloacsaedlizwaetilodns toefe lthdee fdecetfedcettse.c tTiohne baaustehdoorsn pXr-orapyoisme athgees iomfpthleementawtieolnd opfi pYeO. TLhOe-avu5t hfoorrs pcrloaidmucthtiaotnth-beatsraeidn ewdeYldO LstOee-vl 5dreefaeccht eddeatemctAioPno bfa9s8e.7d% o(nIo XU--r0a.y5) ,images\nwhilstmeetingthereal-timedetectionrequirementsofsteelpipeproductionwithasingle\nof the weld pipe. The authors claim that the trained YOLO-v5 reached a mAP of 98.7%\nimagedetectionrateof0.12s.\n(IoU-0.5), whilst meeting the real-time detection requirements of steel pipe production\nZhuxiMAetal.[91]addresstheissueoflarge-scalecomputationandspecifichardwith a single image detection rate of 0.12 s.\nwarerequirementsforautomateddefectdetectioninaluminumstrips. Theauthorsselect\nZhuxi MA et al. [91] address the issue of large-scale computation and specific hardYOLO-v4 as the architecture, whilst the backbone is constructed to make use of depthware requirements for automated defect detection in aluminum strips. The authors select\nwiseseparableconvolutionsalongwithaparalleldualattentionmechanismforfeature\neYnOhaLnOc-evm4e anst, tahses haorcwhniteinctFuirgeu,r we1h7i.lsTt htehep rboapcoksbedonnee tiws ocroknsistrtuesctteedd otno rmeaalkdea tuasfer oomf depthawciosled s-reopllairnagbwleo crkosnhvoopl,uptiroonvsid ainlognigm wprietshs iav epraersaullltesl odnuraela alttdeantatiaocnh imeveinchgaannismmA fPoor ffeature\n9e6n.2h8a%nc.eCmomenpta, raesd sthootwheno irnig FinigaluYreO 1L7O. -Tvh4,et hperoapuothsoerds ncleatiwmotrhka tisth teesptreodp oosne dreaarlc hdiatetac- from a\ntcuorledv-roolullmineg iswroerdkuscheodpb, yp8r3o.v3i8d%in,wg himilsptrtehsesiinvfee rreenscueltssp oeend riesailn cdraetaas eadchbiyevaifnagc toarn omf AP of\nt9h6r.e2e8.%Th. eCionmcrepaasreeidn tpoe rtfhoer moraingcienawla YsOpaLrOtly-vd4u, ethtoe tahuetchuosrtos mclaainmch tohraatp tphreo apcrho,pwohseerde bayrchitecduetothemaximumaspectratioofthecustomdataset,thedefectwassetto1:20whichis\nture volume is reduced by 83.38%, whilst the inference speed is increased by a factor of\nin-linewiththedefectcharacteristics,suchasscratchmarks.\nthree. The increase in performance was partly due to the custom anchor approach,\nwhereby due to the maximum aspect ratio of the custom dataset, the defect was set to 1:20\nwhich is in-line with the defect characteristics, such as scratch marks.\nMaMchaicnheisn e2s022032,3 1,11,1 ,x6 F7O7R PEER REVIEW 18of25 19 of 26\nFigure 17. Proposed parallel network structure [91].\nFigure17.Proposedparallelnetworkstructure[91].\nJiJainantintigngSh Siheit eatl. a[l9.2 []9c2i]te ctihtee tmhea nmufaancutufraicntguprirnogce pssroocfestsese olfp srotedeulc ptiroondausctthieonre aasso tnhe reason\nffoorrv vaariroiouussd edfeecfetsctosr iogrinigaitninagtionng tohne sttheee lsstueerfla scue,rsfaucceh, assurcohll iansg rsoclalilengan sdcaplaet cahneds .pTahteches. The\nauthorsstatethatthesmalldimensionsofthedefectsaswellasthestringentdetection\nauthors state that the small dimensions of the defects as well as the stringent detection\nrequirementsmakethequalityinspectionprocessachallengingtask.Therefore,theauthors\nrequirements make the quality inspection process a challenging task. Therefore, the aupresentanimprovedversionofYOLO-v5byincorporatinganattentionmechanismfor\nthors present an improved version of YOLO-v5 by incorporating an attention mechanism\nfacilitatingthetransmissionofshallowfeaturesfromthebackbonetotheneck,preserving\nfor facilitating the transmission of shallow features from the backbone to the neck, prethedefectiveregions, inadditiontok-meansclusteringofanchorboxesforaddressing\ntsheerevxintrgem theea sdpeefcetcrtaitvioes roefgdioefnesc,t iivne atadrdgeittisowni tthoi nkt-hmeedaantas sceltu. Tstheerianugth oofr sasntactheotrh abtotxhees for adidmrpersosvinegd tahrceh eitxetcrteumreea acshpieevcetd ra8t6i.o3s5 %ofm dAefPecrteiavceh tianrgg4e5tsF wPSitdheitne ctthioen dsaptaeesdet,.w Thhiels atuthtehors state\notrhiagtin tahlea ricmhiptercotvuereda achrciehvietedc8tu1.r7e8 %acmhiAevPeadt 5826F.3P5S%. mAP reaching 45 FPS detection speed,\nwhilst the original architecture achieved 81.78% mAP at 52 FPS.\n3.4. PalletRackingDefectInspection\nA promising application with significant deployment scope in the warehousing\n3.4. Pallet Racking Defect Inspection\nand general industrial storage centers is automated pallet racking inspection. WareA promising application with significant deployment scope in the warehousing and\nhousesanddistributioncentershostacriticalinfrastructureknownasrackingforstock\nsgtoenraegrea.l iUnndnuosttircieadl sdtoamraaggee cteontpearlsl eits raauctkoimngatceadn ppaalvleett rhaecwkianyg fionrspsiegcntiifoinca. nWtaloressheosuses and\nidniitsitartiebdutbiyonra cceknintegrcso hlloaspts ea lceraidtiicnagl tionfwraassttreudc/tduarme kagneodwsnto acsk ,rfaicnkainncgia floimr sptloiccakt isotnosr,age. Unonpoetriacetido ndaalmloasgsees t,oi npjaulrleedt reamckpilnogy eceasn, apnadvew tohres tw-caayse f,olro ssisgonfifilicvaenst[ l9o3s]s.eDs uineittioattehde by rackiinnegff cicoilelnacpisees olefatdhiencgo ntov ewnatisotnedal/draacmkianggeidn ssptoecckti,o finnmanecchiaaln iimsmpsl,icsautcihonass, houpmeraant-iloendal losses,\nannualinspectionresultinginlaborcosts,bias,fatigue,andmechanicalproducts,such\ninjured employees, and worst-case, loss of lives [93]. Due to the inefficiencies of the conasrackguards[94]lackingclassificationintelligence,CNN-basedautomateddetection\nventional racking inspection mechanisms, such as human-led annual inspection resulting\nseemstobeapromisingalternative.\nin labor costs, bias, fatigue, and mechanical products, such as rackguards [94] lacking clasRealizingthepotential,Hussainetal.[95]inauguratedresearchintoautomatedpallet\nsification intelligence, CNN-based automated detection seems to be a promising alternarackingdetectionviacomputervision. Afterpresentingtheirinitialresearchbasedonthe\nMtivoeb.i leNet-V2architecture,theauthorsrecentlyproposedtheimplementationofYOLOv7 forRaeuatloizminatge dthpe aplloetternatcikailn, Hguinssspaeinct ieotn al[.9 [69]5.] Tinhaeusgeulercatitoend orefstehaercahrc ihnitteoc atuurteomwaasted pallet\nirna-cliknienwg idthettehcetisotrnin vgiean ctoremqpuuirteemr evnitssioonf.p Arofdteurc tpiorensfleonotirndge tphloeyirm inenitti,ail. er.e,seedagrecdhe bvaicseed on the\ndMepolboiylmeNenett,-Vpl2a caerdchonitteocatunroep, etrhaet ianugtfhoorkrlsi frte,rceeqnutilryi npgrroepaol-stiemde tdheet eimctipolnemasetnhetaftoiroknli fotf YOLOv7 for automated pallet racking inspection [96]. The selection of the architecture was inline with the stringent requirements of production floor deployment, i.e., edge device deployment, placed onto an operating forklift, requiring real-time detection as the forklift\napproaches the racking. Evaluating the performance of the proposed solution on a real\ndataset, the authors claimed an impressive performance of 91.1% mAP running at 19 FPS.\nMachines2023,11,677 19of25\napproachestheracking. Evaluatingtheperformanceoftheproposedsolutiononareal\ndataset,theauthorsclaimedanimpressiveperformanceof91.1%mAPrunningat19FPS.\nTable4presentsacomparisonofthepresentresearchinthisemergingfield. Although\nmask R-CNN presents the highest accuracy, which is a derivative of the segmentation\nfamily of architectures with significant computational load, this makes it an infeasible\noption for deployment. Whereas the proposed approach utilizing YOLO-v7 achieved\nsimilaraccuracycomparedtoMobileNet-V2,whilstrequiringsignificantlylesstraining\ndataalongwithinferencingat19FPS.\nTable4.Rackingdomainresearchcomparison.\nResearch Architecture DatasetSize Accuracy FPS\n[95] MobileNet-V2 19,717 92.7% -----\n[96] YOLO-v7 2095 91.1% 19\n[97] Mask-RCNN 75 93.45% -----\n4. Discussion\nTheYOLOfamilyofobjectdetectorshashadasignificantimpactonimprovingthe\npotential of computer vision applications. Right from the onset, i.e., the release of the\nYOLO-v1in2015,significantbreakthroughswereintroduced. YOLO-v1becamethefirst\narchitecturecombiningthetwoconventionallyseparatetasksofboundingboxprediction\nandclassificationintoone. YOLO-v2wasreleasedinthefollowingyear,introducingarchitecturalimprovementsanditerativeimprovements,suchasbatchnormalization,higher\nresolution,andanchorboxes. In2018,YOLO-v3wasreleased,anextensionofprevious\nvariantswithenhancementsincludingtheintroductionofobjectnessscoresforbounding\nbox predictions added connections for the backbone layers and the ability to generate\npredictionsatthreedifferentlevelsofgranularity,leadingtoimprovedperformanceon\nsmallerobjecttargets.\nAfterashortdelay,YOLO-v4wasreleasedinApril2020,becomingthefirstvariantof\ntheYOLOfamilynottobeauthoredbytheoriginalauthorJosephRedmon. Enhancements\nincluded improved feature aggregation, gifting of the bag of freebies, and the mish\nactivation.Inamatterofmonths,YOLO-v5enteredthecomputervisionterritory,becoming\nthefirstvarianttobereleasedwithoutbeingaccompaniedbyapaperrelease. YOLO-v5\nbased on PyTorch, with an active GitHub repo further delineated the implementation\nprocess,makeitaccessibletoawideraudience. Focusedoninternalarchitecturalreforms,\nYOLO-v6authorsredesignedthebackbone(EfficientRep)andneck(Rep-PAN)modules,\nwithaninclinationtowardhardwareefficiency. Additionally,anchor-freeandtheconcept\nofdecoupledheadwasintroduced,implyingadditionallayersforfeatureseparationfrom\nthefinalhead,whichisempiricallyshowntoimprovetheoverallperformance.Theauthors\nof YOLO-v7 also focused on architectural reforms, considering the amount of memory\nrequiredtokeeplayerswithinmemoryandthedistancerequiredforgradientstobackpropagate,i.e.,shortergradients,resultinginenhancedlearningcapacity. Fortheultimate\nlayeraggregation,theauthorsimplementedE-ELAN,whichisanextensionoftheELAN\ncomputationalblock. Theadventof2023introducedthelatestversionoftheYOLOfamily,\nYOLO-v8, which was released by Ultralytics. With an impending paper release, initial\ncomparisonsofthelatestversionagainstpredecessorshaveshownpromisingperformance\nwithrespecttothroughputwhencomparedtosimilarcomputationalparameters.\n4.1. ReasonforRisingPopularity\nTable5presentsasummaryofthereviewedYOLOvariantsbasedontheunderlying\nframework,backbone,average-precision(AP),andkeycontributions. Itcanbeobserved\nfromTable3thatasthevariantsevolvedtherewasashiftfromtheconservativeDarknet\nframework to a more accessible one, i.e., PyTorch. The AP presented here is based on\nCOCO-2017[63]withtheexceptionofYOLO-v1/v2,whicharebasedonVOC-2017[39].\nMachines2023,11,677 20of25\nCOCO-2017[63]consistsofover80objectsdesignedtorepresentavastarrayofregularly\nseenobject. Itcontains121,408imagesresultingin883,331objectannotationswithmedian\nimageratioof640480pixels. Itisimportanttonotethattheoverallaccuracyalongwith\ninferencecapacitydependsonthedeployeddesign/trainingstrategies,asdemonstratedin\ntheindustrialsurfacedetectionsection.\nTable5.Abstractvariantcomparison.\nVariant Framework Backbone AP(%) Comments\nV1 Darknet Darknet-24 63.4 Onlydetectamaximumoftwoobjectsinthesamegrid.\nIntroducedbatchnorm,k-meansclusteringforanchorboxes.\nV2 Darknet Darknet-24 63.4\nCapableofdetecting>9000categories.\nUtilizedmulti-scalepredictionsandspatialpyramidpooling\nV3 Darknet Darknet-53 36.2\nleadingtolargerreceptivefield.\nV4 Darknet CSPDarknet-53 43.5 Presentedbag-of-freebiesincludingtheuseofCIoUloss.\nFirstvariantbasedinPyTorch,makingitavailabletoawider\nV5 PyTorch ModifiedCSPv7 55.8 audience.Incorporatedtheanchorselectionprocessesinto\ntheYOLO-v5pipeline.\nFocusedonindustrialsettings,presentedananchor-free\nV6 PyTorch EfficientRep 52.5 pipeline.Presentednewlossdeterminationmechanisms\n(VFL,DFL,andSIoU/GIoU).\nArchitecturalintroductionsincludedE-ELANforfaster\nV7 PyTorch RepConvN 56.8 convergencealongwithabag-of-freebiesincluding\nRepConvNandreparameterization-planning.\nAnchor-freereducingthenumberofpredictionboxeswhilst\nV8 PyTorch YOLO-v8 53.9 speedingupnon-maximumsuppression.Pendingpaperfor\nfurtherarchitecturalinsights.\nTheAPmetricconsistsofprecision-recall(PR)metrics,definingofapositiveprediction\nusingIntersectionoverUnion,andthehandlingofmultipleobjectcategories. APprovides\na balanced overview of PR based on the area under the PR curve. IoU facilitates the\nquantificationofsimilaritybetweenpredictedk andgroundtruthk boundingboxesas\np g\nexpressedin(8):\n(cid:0) (cid:1)\narea k k\np g\nIoU = (cid:0) (cid:1) (8)\narea k k\np g\nTheriseofYOLOcanbeattributedtotwofactors. First,thefactthatthearchitectural\ncomposition of YOLO variants is compatible for one-stage detection and classification\nmakesitcomputationallylightweightwithrespecttootherdetectors. However,wefeel\nthatefficientarchitecturalcompositionbyitselfdidnotdrivethepopularityoftheYOLO\nvariants,asothersingle-stagedetectors,suchasMobileNets,alsoserveasimilarpurpose.\nThe second reason is the accessibility factor, which was introduced as the YOLO\nvariantsprogressed,withYOLO-v5beingtheturningpoint. Expandingfurtheronthis\npoint, the first two variants were based on the Darknet framework. Although this provided a degree of flexibility, accessibility was limited to a smaller user base due to the\nrequired expertise. Ultralytics, introduced YOLO-v5 based on the PyTorch framework,\nmakingthearchitectureavailableforawideraudienceandincreasingthepotentialdomain\nofapplications.\nAsevidentfromTable6,themigrationtoamoreaccessibleframeworkcoupledwith\narchitecturalreformsforimprovedreal-timeperformancesky-rocketed. Atpresent,YOLOv5has34.7kstars,asignificantleadcomparedtoitspredecessors. Fromimplementation,\nYOLO-v5onlyrequiredtheinstallationoflightweightpythonlibraries. Thearchitectural\nreformsindicatedthatthemodeltrainingtimewasreduced,whichinturnreducedtheexperimentationcostattributedtothetrainingprocess,i.e.,GPUutilization. Fordeployment\nandtestingpurposes,researchershaveseveralroutes,suchasindividual/batchimages,\nvideo/webcamfeeds,inadditiontosimpleweightconversiontoONXXweightsforedge\ndevicedeployment.\nMachines2023,11,677 21of25\nTable6.GitHubpopularitycomparison.\nYOLOVariant Stars(K)\nV3 9.3\nV4 20.2\nV5 34.7\nV6 4.6\nV7 8.4\nV8 2.9\n4.2. YOLOandIndustrialDefectDetection\nManifestationsofthefourthindustrialrevolutioncanbeobservedatpresentinan\nad-hocmanner, spanningacrossvariousindustries. Withrespecttothemanufacturing\nindustry, this revolution can be targeted at the quality inspection processes, which are\nvital for assuring efficiency and retaining client satisfaction. When focusing on surface\ndefectdetection,asalludedtoearlier,theinspectionrequirementscanbemorestringent\nascomparedtootherapplications. Thisisduetomanyfactors,suchasthefactthatthe\ndefectsmaybeextremelysmall,requiringexternalspectralimagingtoexposedefectsprior\ntoclassificationandduetothefactthattheoperationalsettingoftheproductionlinemay\nonlyprovideasmall-timewindowwithinwhichinferencemustbecarriedout.\nConsideringthestringentrequirementsoutlinedaboveandbenchmarkingagainstthe\nprinciplesofYOLOfamilyofvariants,formstheconclusionthattheYOLOvariantshavethe\npotentialtoaddressbothreal-time,constraineddeploymentandsmall-scaledefectdetectionrequirementsofindustrial-basedsurfacedefectdetection. YOLOvariantshaveproven\nreal-timecomplianceinseveralindustrialenvironmentsasshownin[81,84,85,90,95]. An\ninterestingobservationarisingfromtheindustrialliteraturereviewedistheabilityforusers\ntomodifytheinternalmodulesofYOLOvariantsinordertotakecareoftheirspecificapplicationneedswithoutcompromisingonreal-timecompliance,forexample[81,87,91,92],\nintroducingattention-mechanismsforaccentuationofdefectiveregions.\nAnadditionalfactor,foundwithinthelaterYOLOvariantsissub-variantsforeach\nbasearchitecture,i.e.,forYOLO-v5variantsincludingYOLO-v5-S/M/L,thiscorresponds\ntodifferentcomputationalloadswithrespecttothenumberofparameters. Thisflexibility\nenablesresearcherstoconsideramoreflexibleapproachwiththearchitectureselection\ncriteriabasedontheindustrialrequirements,i.e.,ifreal-timeinferenceisrequiredwithless\nemphasisonoptimalmAP,alightweightvariantcanbeselected,suchasYOLO-v5-small\nratherthanYOLO-v5-large.\n5. Conclusions\nInconclusion,thisworkisthefirstofitstypefocusedondocumentingandreviewing\ntheevolutionofthemostprevalentsingle-stageobjectdetectorwithinthecomputervision\ndomain. Thereviewpresentsthekeyadvancementsofeachvariant,followedbyimplementation of YOLO architectures within various industrial settings focused on surface\nautomatedreal-timesurfacedefectdetection.\nFromthereview,itisclearastheYOLOvariantshaveprogressed,latterversionsin\nparticular,YOLO-v5hasfocusedonconstrainededgedeployment,akeyrequirementfor\nmanymanufacturingapplications. Duetothefactthatthereisnocopyrightandpatent\nrestrictions,researchanchoredaroundtheYOLOarchitecture,i.e.,real-time,lightweight,\naccuratedetection,canbeconductedbyanyindividualorresearchorganization,whichhas\nalsocontributedtotheprevalenceofthisvariant.\nWithYOLO-v8releasedinJanuary2023,showingpromisingperformancewithrespect\ntothroughputandcomputationalloadrequirements, itisenvisionedthat2023willsee\nmorevariantsreleasedbypreviousornewauthorsfocusedonimprovingthedeployment\ncapacityofthearchitectureswithrespecttoconstraineddeploymentenvironments.\nWithresearchorganizations,suchasUltralyticsandMeituanTechnicalTeamtaking\nakeeninterestinthedevelopmentofYOLOarchitectureswithafocusonedge-friendly\nMachines2023,11,677 22of25\ndeployment,weanticipatefurthertechnologicaladvancementsinthearchitecturalfootprint\nofYOLO.Tocaterforconstraineddeployment,theseadvancementswillneedtofocuson\nenergyconservationwhilstmaintaininghighinferencerates. Furthermore,weenvision\nthe proliferation of YOLO architectures into production facilities to help with quality\ninspectionpipelinesaswellasprovidingstimulusforinnovativeproductsasdemonstrated\nby [96] with an automated pallet racking inspection solution. Along with integration\nintoadiversesetofhardwareandIoTdevices,YOLOhasthepotentialtotapintonew\ndomainswherecomputervisioncanassistinenhancingexistingprocesseswhilstrequiring\nlimitedresources.\nFunding:Thisresearchreceivednoexternalfunding.\nDataAvailabilityStatement:Notapplicable.\nConflictsofInterest:Theauthorsdeclarenoconflictofinterest.\nReferences\n1. Zhang,B.; Quan,C.; Ren,F.StudyonCNNintherecognitionofemotioninaudioandimages. InProceedingsofthe2016\nIEEE/ACIS15thInternationalConferenceonComputerandInformationScience(ICIS),Okayama, Japan, 2629June2016.\n[CrossRef]\n2. Pollen,D.A.Explicitneuralrepresentations,recursiveneuralnetworksandconsciousvisualperception.Cereb.Cortex2003,13,\n807814.[CrossRef][PubMed]\n3. Usingartificialneuralnetworkstounderstandthehumanbrain.Res.Featur.2022.[CrossRef]\n4. ImprovementofNeuralNetworksArtificialOutput.Int.J.Sci.Res.(IJSR)2017,6,352361.[CrossRef]\n5. Dodia,S.;Annappa,B.;Mahesh,P.A.Recentadvancementsindeeplearningbasedlungcancerdetection:Asystematicreview.\nEng.Appl.Artif.Intell.2022,116,105490.[CrossRef]\n6. Ojo,M.O.;Zahid,A.DeepLearninginControlledEnvironmentAgriculture:AReviewofRecentAdvancements,Challengesand\nProspects.Sensors2022,22,7965.[CrossRef][PubMed]\n7. Jarvis,R.A.APerspectiveonRangeFindingTechniquesforComputerVision.IEEETrans.PatternAnal.Mach.Intell.1983,PAMI-5,\n122139.[CrossRef]\n8. Hussain,M.;Bird,J.;Faria,D.R.AStudyonCNNTransferLearningforImageClassification.11August2018.Availableonline:\nhttps://research.aston.ac.uk/en/publications/a-study-on-cnn-transfer-learning-for-image-classification(accessedon1January\n2023).\n9. Yang,R.;Yu,Y.ArtificialConvolutionalNeuralNetworkinObjectDetectionandSemanticSegmentationforMedicalImaging\nAnalysis.Front.Oncol.2021,11,638182.[CrossRef]\n10. Haupt,J.;Nowak,R.CompressiveSamplingvs.ConventionalImaging.InProceedingsofthe2006InternationalConferenceon\nImageProcessing,LasVegas,NV,USA,2629June2006;pp.12691272.[CrossRef]\n11. Gu, J.; Wang, Z.; Kuen, J.; Ma, L.; Shahroudy, A.; Shuai, B.; Liu, T.; Wang, X.; Wang, G.; Cai, J.; et al. Recent advances in\nconvolutionalneuralnetworks.PatternRecognit.2018,77,354377.[CrossRef]\n12. Perez,H.;Tah,J.H.M.;Mosavi,A.DeepLearningforDetectingBuildingDefectsUsingConvolutionalNeuralNetworks.Sensors\n2019,19,3556.[CrossRef]\n13. Hussain,M.;Al-Aqrabi,H.;Hill,R.PV-CrackNetArchitectureforFilterInducedAugmentationandMicro-CracksDetection\nwithinaPhotovoltaicManufacturingFacility.Energies2022,15,8667.[CrossRef]\n14. Hussain, M.; Dhimish, M.; Holmes, V.; Mather, P. Deployment of AI-based RBF network for photovoltaics fault detection\nprocedure.AIMSElectron.Electr.Eng.2020,4,118.[CrossRef]\n15. Hussain,M.;Al-Aqrabi,H.;Munawar,M.;Hill,R.;Parkinson,S.ExudateRegenerationforAutomatedExudateDetectionin\nRetinalFundusImages.IEEEAccess2022.[CrossRef]\n16. Hussain,M.;Al-Aqrabi,H.;Hill,R.StatisticalAnalysisandDevelopmentofanEnsemble-BasedMachineLearningModelfor\nPhotovoltaicFaultDetection.Energies2022,15,5492.[CrossRef]\n17. Singh,S.A.;Desai,K.A.Automatedsurfacedefectdetectionframeworkusingmachinevisionandconvolutionalneuralnetworks.\nJ.Intell.Manuf.2022,34,19952011.[CrossRef]\n18. Weichert, D.; Link, P.; Stoll, A.; RÃ¼ping, S.; Ihlenfeldt, S.; Wrobel, S. A review of machine learning for the optimization of\nproductionprocesses.Int.J.Adv.Manuf.Technol.2019,104,18891902.[CrossRef]\n19. Wang,J.;Ma,Y.;Zhang,L.;Gao,R.X.;Wu,D.Deeplearningforsmartmanufacturing:Methodsandapplications.J.Manuf.Syst.\n2018,48,144156.[CrossRef]\n20. Weimer,D.;Scholz-Reiter,B.;Shpitalni,M.Designofdeepconvolutionalneuralnetworkarchitecturesforautomatedfeature\nextractioninindustrialinspection.CIRPAnn.2016,65,417420.[CrossRef]\n21. Kusiak,A.Smartmanufacturing.Int.J.Prod.Res.2017,56,508517.[CrossRef]\nMachines2023,11,677 23of25\n22. Yang,J.;Li,S.;Wang,Z.;Dong,H.;Wang,J.;Tang,S.UsingDeepLearningtoDetectDefectsinManufacturing:AComprehensive\nSurveyandCurrentChallenges.Materials2020,13,5755.[CrossRef]\n23. Soviany,P.;Ionescu,R.T.OptimizingtheTrade-OffbetweenSingle-StageandTwo-StageDeepObjectDetectorsusingImage\nDifficulty Prediction. In Proceedings of the 2018 20th International Symposium on Symbolic and Numeric Algorithms for\nScientificComputing(SYNASC),Timisoara,Romania,2023September2018.[CrossRef]\n24. Du,L.;Zhang,R.;Wang,X.Overviewoftwo-stageobjectdetectionalgorithms.J.Phys.Conf.Ser.2020,1544,012033.[CrossRef]\n25. Sultana,F.;Sufian,A.;Dutta,P.AReviewofObjectDetectionModelsBasedonConvolutionalNeuralNetwork.InAdvancesin\nIntelligentSystemsandComputing;Springer:Singapore,2020;pp.116.[CrossRef]\n26. Liu,W.;Anguelov,D.;Erhan,D.;Szegedy,C.;Reed,S.;Fu,C.Y.;Berg,A.C.SSD:Singleshotmultiboxdetector.InProceedingsof\ntheComputerVisionECCV2016,Amsterdam,TheNetherlands,1114October2016;pp.2137.[CrossRef]\n27. Fu,C.Y.;Liu,W.;Ranga,A.;Tyagi,A.;Berg,A.C.DSSD:DeconvolutionalSingleShotDetector.arXiv2017,arXiv:1701.06659.\n28. Cheng,X.;Yu,J.RetinaNetwithDifferenceChannelAttentionandAdaptivelySpatialFeatureFusionforSteelSurfaceDefect\nDetection.IEEETrans.Instrum.Meas.2020,70,2503911.[CrossRef]\n29. Redmon,J.;Divvala,S.;Girshick,R.;Farhadi,A.YouOnlyLookOnce:Unified,Real-TimeObjectDetection.InProceedingsofthe\n2016IEEEConferenceonComputerVisionandPatternRecognition(CVPR),LasVegas,NV,USA,2730June2016;pp.779788.\n[CrossRef]\n30. Wang,Z.J.;Turko,R.;Shaikh,O.;Park,H.;Das,N.;Hohman,F.;Kahng,M.;Chau,D.H.P.CNNExplainer:LearningConvolutional\nNeuralNetworkswithInteractiveVisualization.IEEETrans.Vis.Comput.Graph.2020,27,13961406.[CrossRef][PubMed]\n31. Krizhevsky,A.;Sutskever,I.;Hinton,G.E.Imagenetclassificationwithdeepconvolutionalneuralnetworks.Commun.ACM2017,\n60,8490.[CrossRef]\n32. Simonyan,K.;Zisserman,A.VeryDeepConvolutionalNetworksforLarge-ScaleImageRecognition.arXiv2014,arXiv:1409.1556.\n33. Szegedy,C.;Liu,W.;Jia,Y.;Sermanet,P.;Reed,S.;Anguelov,D.;Rabinovich,A.Goingdeeperwithconvolutions.InProceedings\noftheConferenceonComputerVisionandPatternRecognition,Boston,MA,USA,12June2015.\n34. He,K.;Zhang,X.;Ren,S.;Sun,J.Deepresiduallearningforimagerecognition.InProceedingsoftheConferenceonComputer\nVisionandPatternRecognition,LasVegas,NV,USA,30June2016.\n35. Girshick, R.; Donahue, J.; Darrell, T.; Malik, J. Region-Based Convolutional Networks for Accurate Object Detection and\nSegmentation.IEEETrans.PatternAnal.Mach.Intell.2015,38,142158.[CrossRef]\n36. Girshick,R.FastR-CNN.InProceedingsoftheInternationalConferenceonComputerVision,Santiago,Chile,713December\n2015.\n37. Ren,S.;He,K.;Girshick,R.;Sun,J.FasterR-CNN:Towardsreal-timeobjectdetectionwithregionproposalnetworks. Trans.\nPatternAnal.Mach.Intell.2017,39,11371149.[CrossRef]\n38. Vidyavani,A.;Dheeraj,K.;Reddy,M.R.M.;Kumar,K.N.ObjectDetectionMethodBasedonYOLOv3usingDeepLearning\nNetworks.Int.J.Innov.Technol.Explor.Eng.2019,9,14141417.[CrossRef]\n39. Everingham,M.;VanGool,L.;Williams,C.K.I.;Winn,J.;Zisserman,A.ThePascalVisualObjectClasses(VOC)Challenge.Int.J.\nComput.Vis.2009,88,303338.[CrossRef]\n40. Shetty, S. Application of Convolutional Neural Network for Image Classification on Pascal VOC Challenge 2012 dataset.\narXiv2016,arXiv:1607.03785.\n41. Felzenszwalb,P.F.;Girshick,R.B.;McAllester,D.;Ramanan,D.ObjectDetectionwithDiscriminativelyTrainedPart-BasedModels.\nIEEETrans.PatternAnal.Mach.Intell.2009,32,16271645.[CrossRef][PubMed]\n42. Chang,Y.-L.;Anagaw,A.;Chang,L.;Wang,Y.C.;Hsiao,C.-Y.;Lee,W.-H.ShipDetectionBasedonYOLOv2forSARImagery.\nRemoteSens.2019,11,786.[CrossRef]\n43. Liao,Z.;Carneiro,G.Ontheimportanceofnormalisationlayersindeeplearningwithpiecewiselinearactivationunits. In\nProceedingsofthe2016IEEEWinterConferenceonApplicationsofComputerVision(WACV),NewYork,NY,USA,710March\n2016.[CrossRef]\n44. Garbin,C.;Zhu,X.;Marques,O.Dropoutvs.batchnormalization:Anempiricalstudyoftheirimpacttodeeplearning.Multimed.\nToolsAppl.2020,79,1277712815.[CrossRef]\n45. Li,G.;Jian,X.;Wen,Z.;AlSultan,J.AlgorithmofoverfittingavoidanceinCNNbasedonmaximumpooledandweightdecay.\nAppl.Math.NonlinearSci.2022,7,965974.[CrossRef]\n46. Deng,J.;Dong,W.;Socher,R.;Li,L.J.;Li,K.;Fei-Fei,L.Imagenet:Alarge-scalehierarchicalimagedatabase.InProceedingsofthe\n2009IEEEConferenceonComputerVisionandPatternRecognition,Miami,FL,USA,2025June2009.\n47. Xue,J.;Cheng,F.;Li,Y.;Song,Y.;Mao,T.DetectionofFarmlandObstaclesBasedonanImprovedYOLOv5sAlgorithmbyUsing\nCIoUandAnchorBoxScaleClustering.Sensors2022,22,1790.[CrossRef]\n48. Ahmed,M.;Seraj,R.;Islam,S.M.S.Thek-meansAlgorithm:AComprehensiveSurveyandPerformanceEvaluation.Electronics\n2020,9,1295.[CrossRef]\n49. Redmon,J.Darknet:OpenSourceNeuralNetworksinC.2013.Availableonline:https://pjreddie.com/darknet(accessedon\n1January2023).\n50. Furusho,Y.;Ikeda,K.Theoreticalanalysisofskipconnectionsandbatchnormalizationfromgeneralizationandoptimization\nperspectives.APSIPATrans.SignalInf.Process.2020,9,e9.[CrossRef]\nMachines2023,11,677 24of25\n51. Machine-LearningSystemTacklesSpeechandObjectRecognition.Availableonline:https://news.mit.edu/machine-learningimage-object-recognition-918(accessedon1January2023).\n52. Bochkovskiy, A.; Wang, C.Y.; Liao HY, M. YOLOv4: Optimal Speed and Accuracy of Object Detection. arXiv 2020,\narXiv:2004.10934.\n53. Tan,M.;Le,Q.EfficientNet:Rethinkingmodelscalingforconvolutionalneuralnetworks.InProceedingsoftheInternational\nConferenceonMachineLearning(ICML),LongBeach,CA,USA,915June2019.\n54. Huang,G.;Liu,Z.;VanDerMaaten,L.;Weinberger,K.Q.Denselyconnectedconvolutionalnetworks.InProceedingsoftheIEEE\nConferenceonComputerVisionandPatternRecognition(CVPR),Honolulu,HI,USA,2126July2017;pp.47004708.\n55. Lin,T.Y.;DollÃ¡r,P.;Girshick,R.;He,K.;Hariharan,B.;Belongie,S.Featurepyramidnetworksforobjectdetection.InProceedings\noftheIEEEConferenceonComputerVisionandPatternRecognition(CVPR),Honolulu,HI,USA,2126July2017;pp.21172125.\n56. Liu,S.;Qi,L.;Qin,H.;Shi,J.;Jia,J.Pathaggregationnetworkforinstancesegmentation.InProceedingsoftheIEEEConference\nonComputerVisionandPatternRecognition(CVPR),SaltLakeCity,UT,USA,1823June2018;pp.87598768.\n57. He,K.;Zhang,X.;Ren,S.;Sun,J.SpatialPyramidPoolinginDeepConvolutionalNetworksforVisualRecognition.IEEETrans.\nPatternAnal.Mach.Intell.2015,37,19041916.[CrossRef]\n58. Zheng,Z.;Wang,P.;Liu,W.;Li,J.;Ye,R.;Ren,D.Distance-IoULoss:Fasterandbetterlearningforboundingboxregression.In\nProceedingsoftheAAAIConferenceonArtificialIntelligence(AAAI),NewYork,NY,USA,712February2020.\n59. Misra,D.Mish:Aselfregularizednonmonotonicneuralactivationfunction.arXiv2019,arXiv:1908.08681.\n60. Yao,Z.;Cao,Y.;Zheng,S.;Huang,G.;Lin,S.Cross-IterationBatchNormalization.arXiv2020,arXiv:2002.05712.\n61. Ultralytics.YOLOv52020.Availableonline:https://github.com/ultralytics/yolov5(accessedon1January2023).\n62. Jocher,G.;Stoken,A.;Borovec,J.;Christopher,S.T.A.N.;Laughing,L.C.Ultralytics/yolov5:v4.0-nn.SiLU()Activations,Weights\n&BiasesLogging,PyTorchHubIntegration.Zenodo2021.Availableonline:https://zenodo.org/record/4418161(accessedon\n5January2023).\n63. Lin,T.Y.;Maire,M.;Belongie,S.;Hays,J.;Perona,P.;Ramanan,D.;Zitnick,C.L.Microsoftcoco:Commonobjectsincontext.In\nProceedingsoftheEuropeanConferenceonComputerVision,Zurich,Switzerland,612September2014.\n64. Tan,M.;Pang,R.;Le,Q.V.EfficientDet:ScalableandEfficientObjectDetection.InProceedingsoftheIEEE/CVFConferenceon\nComputerVisionandPatternRecognition,Seattle,WA,USA,1319June2020.\n65. Li,C.;Li,L.;Jiang,H.;Weng,K.;Geng,Y.;Li,L.;Wei,X.YOLOv6:ASingle-StageObjectDetectionFrameworkforIndustrial\nApplications.arXiv2022,arXiv:2209.02976.\n66. Ding,X.;Zhang,X.;Ma,N.;Han,J.;Ding,G.;Sun,J.Repvgg: Makingvgg-styleconvnetsgreatagain. InProceedingsofthe\nIEEE/CVFConferenceonComputerVisionandPatternRecognition,Nashville,TN,USA,2025June2021;pp.1373313742.\n67. Zhang, H.; Wang, Y.; Dayoub, F.; Sunderhauf, N. Varifocalnet: An iou-aware dense object detector. In Proceedings of the\nIEEE/CVFConferenceonComputerVisionandPatternRecognition,Nashville,TN,USA,2025June2021;pp.85148523.\n68. Li,X.;Wang,W.;Wu,L.;Chen,S.;Hu,X.;Li,J.;Yang,J.Generalizedfocalloss: Learningqualifiedanddistributedbounding\nboxesfordenseobjectdetection.Adv.NeuralInf.Process.Syst.2020,33,2100221012.\n69. Gevorgyan,Z.Siouloss:Morepowerfullearningforboundingboxregression.arXiv2022,arXiv:2205.12740.\n70. Shu, C.; Liu, Y.; Gao, J.; Yan, Z.; Shen, C.Channel-wiseknowledgedistillationfordenseprediction. InProceedingsofthe\nIEEE/CVFInternationalConferenceonComputerVision,Montreal,BC,Canada,1117October20221;pp.53115320.\n71. Solawetz,J.;Nelson,J.WhatsNewinYOLOv6?4July2022.Availableonline:https://blog.roboflow.com/yolov6/(accessedon\n1January2023).\n72. Wang,C.Y.;Bochkovskiy,A.;LiaoHY,M.YOLOv7:Trainablebag-of-freebiessetsnewstate-of-the-artforreal-timeobjectdetectors.\narXiv2022,arXiv:2207.02696.\n73. Ge,Z.;Liu,S.;Wang,F.;Li,Z.;Sun,J.YOLOX:ExceedingYOLOseriesin2021.arXiv2021,arXiv:2107.08430.\n74. Wang, C.-Y.; Yeh, I.-H.; Liao, H.-Y.M. You only learn one representation: Unified network for multiple tasks. arXiv 2021,\narXiv:2105.04206.\n75. Wu,W.;Zhao,Y.;Xu,Y.;Tan,X.;He,D.;Zou,Z.;Ye,J.;Li,Y.;Yao,M.;Dong,Z.;etal.DSANet:DynamicSegmentAggrDSANet:\nDynamicSegmentAggregationNetworkforVideo-LevelRepresentationLearning.InProceedingsoftheMM2129thACM\nInternationalConferenceonMultimedia,Virtual,2024October2021.[CrossRef]\n76. Li,C.;Tang,T.;Wang,G.;Peng,J.;Wang,B.;Liang,X.;Chang,X.BossNAS:ExploringHybridCNN-transformerswithBlockwiselySelf-supervisedNeuralArchitectureSearch. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer\nVision,Online,1117October2021.[CrossRef]\n77. Dollar,P.;Singh,M.;Girshick,R.Fastandaccuratemodelscaling.InProceedingsoftheIEEE/CVFConferenceonComputer\nVisionandPatternRecognition(CVPR),Nashville,TN,USA,2025June2021;pp.924932.\n78. Guo,S.;Alvarez,J.M.;Salzmann,M.ExpandNets:Linearover-parameterizationtotraincompactconvolutionalnetworks.Adv.\nNeuralInf.Process.Syst.(NeurIPS)2020,33,12981310.\n79. Ding,X.;Zhang,X.;Zhou,Y.;Han,J.;Ding,G.;Sun,J.Scalingupyourkernelsto3131:RevisitinglargekerneldesigninCNNs.\nInProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),NewOrleans,LA,USA,1824\nJune2022.\n80. Jocher,G.;Chaurasia,A.;Qiu,J.YOLObyUltralytics.GitHub.1January2023.Availableonline:https://github.com/ultralytics/\nultralytics(accessedon12January2023).\nMachines2023,11,677 25of25\n81. Jin,R.;Niu,Q.AutomaticFabricDefectDetectionBasedonanImprovedYOLOv5.Math.Probl.Eng.2021,2021,113.[CrossRef]\n82. NVIDIAJetsonTX2:HighPerformanceAIattheEdge,NVIDIA.Availableonline:https://www.nvidia.com/en-gb/autonomousmachines/embedded-systems/jetson-tx2/(accessedon30January2023).\n83. NVIDIATensorRT.NVIDIADeveloper. 18July2019. Availableonline: https://developer.nvidia.com/tensorrt(accessedon\n5January2023).\n84. Dlamini,S.;Kao,C.-Y.;Su,S.-L.;Kuo,C.-F.J.Developmentofareal-timemachinevisionsystemforfunctionaltextilefabricdefect\ndetectionusingadeepYOLOv4model.Text.Res.J.2021,92,675690.[CrossRef]\n85. Lin,G.;Liu,K.;Xia,X.;Yan,R.AnEfficientandIntelligentDetectionMethodforFabricDefectsbasedonImprovedYOLOv5.\nSensors2022,23,97.[CrossRef][PubMed]\n86. Liu,Z.;Tan,Y.;He,Q.;Xiao,Y.SwinNet:SwinTransformerDrivesEdge-AwareRGB-DandRGB-TSalientObjectDetection.IEEE\nTrans.CircuitsSyst.VideoTechnol.2021,32,44864497.[CrossRef]\n87. Zhang,M.;Yin,L.SolarCellSurfaceDefectDetectionBasedonImprovedYOLOv5.IEEEAccess2022,10,8080480815.[CrossRef]\n88. Binomairah,A.;Abdullah,A.;Khoo,B.E.;Mahdavipour,Z.;Teo,T.W.;Noor,N.S.M.;Abdullah,M.Z.Detectionofmicrocracks\nanddarkspotsinmonocrystallinePERCcellsusingphotoluminesceneimagingandYOLO-basedCNNwithspatialpyramid\npooling.EPJPhotovolt.2022,13,27.[CrossRef]\n89. Sun,T.;Xing,H.;Cao,S.;Zhang,Y.;Fan,S.;Liu,P.Anoveldetectionmethodforhotspotsofphotovoltaic(PV)panelsusing\nimprovedanchorsandpredictionheadsofYOLOv5network.EnergyRep.2022,8,12191229.[CrossRef]\n90. Yang,D.;Cui,Y.;Yu,Z.;Yuan,H.DeepLearningBasedSteelPipeWeldDefectDetection.Appl.Artif.Intell.2021,35,12371249.\n[CrossRef]\n91. Ma,Z.;Li,Y.;Huang,M.;Huang,Q.;Cheng,J.;Tang,S.Alightweightdetectorbasedonattentionmechanismforaluminumstrip\nsurfacedefectdetection.Comput.Ind.2021,136,103585.[CrossRef]\n92. Shi,J.;Yang,J.;Zhang,Y.ResearchonSteelSurfaceDefectDetectionBasedonYOLOv5withAttentionMechanism.Electronics\n2022,11,3735.[CrossRef]\n93. CEP,F.A.5InsightfulStatisticsRelatedtoWarehouseSafety.Availableonline:www.damotech.com(accessedon11January2023).\n94. Armour,R.TheRackGroup.Availableonline:https://therackgroup.com/product/rack-armour/(accessedon12January2023).\n95. Hussain,M.;Chen,T.;Hill,R.MovingtowardSmartManufacturingwithanAutonomousPalletRackingInspectionSystem\nBasedonMobileNetV2.J.Manuf.Mater.Process.2022,6,75.[CrossRef]\n96. Hussain,M.;Al-Aqrabi,H.;Munawar,M.;Hill,R.;Alsboui,T.DomainFeatureMappingwithYOLOv7forAutomatedEdge-Based\nPalletRackingInspections.Sensors2022,22,6927.[CrossRef][PubMed]\n97. Farahnakian,F.;Koivunen,L.;Makila,T.;Heikkonen,J.TowardsAutonomousIndustrialWarehouseInspection.InProceedingsof\nthe202126thInternationalConferenceonAutomationandComputing(ICAC),Portsmouth,UK,24September2021.[CrossRef]\nDisclaimer/Publishers Note: The statements, opinions and data contained in all publications are solely those of the individual\nauthor(s)andcontributor(s)andnotofMDPIand/ortheeditor(s).MDPIand/ortheeditor(s)disclaimresponsibilityforanyinjuryto\npeopleorpropertyresultingfromanyideas,methods,instructionsorproductsreferredtointhecontent.",
  "extraction_quality": {
    "status": "success",
    "confidence": 1.0,
    "issues": [
      "High number of extraction artifacts"
    ],
    "text_length": 140807,
    "sections_found": 7
  },
  "llm_enhanced": {
    "key_contributions": [
      "In-depth review of YOLO evolution from YOLO-v1 to YOLO-v8",
      "Analysis of key architectural advancements in YOLO variants for industrial surface defect detection",
      "Exploration of YOLO's compatibility with industrial requirements for automated quality inspection"
    ],
    "main_findings": [
      "YOLO variants have rapidly grown with increasing intensity to address industrial surface defect detection requirements",
      "YOLO's principle of real-time and high-classification performance is underpinned by limited but efficient computational parameters",
      "YOLO-v8 is the latest release, providing improved performance for industrial defect detection"
    ],
    "technical_approach": "Review of YOLO variants' architectural advancements and their application in industrial surface defect detection",
    "research_gap": "Lack of comprehensive review of YOLO evolution and its application in industrial manufacturing, particularly in surface defect detection"
  }
}